"""
ìƒìš©í™”ê¸‰ ê³ ê¸‰ RAG ì‹œìŠ¤í…œ ì„œë¹„ìŠ¤ - DeepSeek + Qdrant ê¸°ë°˜
ë©€í‹°ëª¨ë‹¬, í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰, RAG Fusion, ì ì‘í˜• ì²­í‚¹, ì‹¤ì‹œê°„ í•™ìŠµ ë“± ìµœì‹  ê¸°ìˆ  í†µí•©
"""
import os
import json
import logging
import asyncio
from typing import List, Dict, Any, Optional, Tuple, Union
from datetime import datetime, timedelta
from pathlib import Path
import uuid
import numpy as np
from collections import defaultdict
import hashlib

# ì´ë¯¸ì§€ ì²˜ë¦¬ë¥¼ ìœ„í•œ ì¡°ê±´ë¶€ ì„í¬íŠ¸
try:
    from PIL import Image
    import pytesseract
    VISION_AVAILABLE = True
except ImportError:
    VISION_AVAILABLE = False

# í…ìŠ¤íŠ¸ ì²˜ë¦¬ ë¼ì´ë¸ŒëŸ¬ë¦¬
try:
    from sentence_transformers import SentenceTransformer
    import spacy
    ADVANCED_NLP_AVAILABLE = True
except ImportError:
    ADVANCED_NLP_AVAILABLE = False

from sqlalchemy.orm import Session
from sqlalchemy import text, func, and_, or_

from ..models.question import Question
from ..models.user import User
from ..services.deepseek_service import deepseek_service
from ..services.qdrant_service import qdrant_service
from ..core.config import settings

logger = logging.getLogger(__name__)

class AdvancedRAGService:
    """ìƒìš©í™”ê¸‰ ê³ ê¸‰ RAG ì‹œìŠ¤í…œ"""
    
    def __init__(self):
        self.deepseek = deepseek_service
        self.vector_db = qdrant_service
        
        # ê³ ê¸‰ ê¸°ëŠ¥ ì„¤ì •
        self.chunk_strategies = ["semantic", "hierarchical", "adaptive"]
        self.search_modes = ["hybrid", "dense", "sparse", "graph"]
        self.fusion_methods = ["rrf", "weighted", "neural"]
        
        # ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§
        self.performance_metrics = defaultdict(list)
        self.user_feedback = defaultdict(list)
        
        # ìºì‹œ ì‹œìŠ¤í…œ
        self.query_cache = {}
        self.embedding_cache = {}
        
        logger.info("ğŸš€ ê³ ê¸‰ RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ")
    
    # ============ 1. ë©€í‹°ëª¨ë‹¬ RAG ============
    
    async def process_multimodal_document(
        self,
        db: Session,
        file_path: str,
        document_title: str,
        user_id: int,
        extract_images: bool = True,
        extract_tables: bool = True
    ) -> Dict[str, Any]:
        """ë©€í‹°ëª¨ë‹¬ ë¬¸ì„œ ì²˜ë¦¬ (PDF + ì´ë¯¸ì§€ + í…ìŠ¤íŠ¸)"""
        try:
            logger.info(f"ğŸ¯ ë©€í‹°ëª¨ë‹¬ ë¬¸ì„œ ì²˜ë¦¬ ì‹œì‘: {document_title}")
            
            results = {
                "document_title": document_title,
                "processing_steps": {},
                "extracted_content": {
                    "text": [],
                    "images": [],
                    "tables": [],
                    "metadata": {}
                }
            }
            
            # 1. í…ìŠ¤íŠ¸ ì¶”ì¶œ (ê¸°ì¡´ ë°©ì‹ + ê°œì„ )
            text_content = await self._extract_enhanced_text(file_path)
            results["extracted_content"]["text"] = text_content
            results["processing_steps"]["text_extraction"] = {"success": True, "chunks": len(text_content)}
            
            # 2. ì´ë¯¸ì§€ ì¶”ì¶œ ë° OCR (ì„ íƒì )
            if extract_images and VISION_AVAILABLE:
                image_content = await self._extract_and_analyze_images(file_path)
                results["extracted_content"]["images"] = image_content
                results["processing_steps"]["image_extraction"] = {"success": True, "images": len(image_content)}
            
            # 3. í‘œ ì¶”ì¶œ ë° êµ¬ì¡°í™” (ì„ íƒì )
            if extract_tables:
                table_content = await self._extract_structured_tables(file_path)
                results["extracted_content"]["tables"] = table_content
                results["processing_steps"]["table_extraction"] = {"success": True, "tables": len(table_content)}
            
            # 4. í†µí•© ì„ë² ë”© ìƒì„± ë° ì €ì¥
            embedding_result = await self._create_multimodal_embeddings(
                db, results["extracted_content"], document_title, user_id
            )
            results["processing_steps"]["embedding_creation"] = embedding_result
            
            logger.info(f"âœ… ë©€í‹°ëª¨ë‹¬ ì²˜ë¦¬ ì™„ë£Œ: {document_title}")
            return {"success": True, "results": results}
            
        except Exception as e:
            logger.error(f"âŒ ë©€í‹°ëª¨ë‹¬ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            return {"success": False, "error": str(e)}
    
    async def _extract_enhanced_text(self, file_path: str) -> List[Dict[str, Any]]:
        """í–¥ìƒëœ í…ìŠ¤íŠ¸ ì¶”ì¶œ (êµ¬ì¡° ì¸ì‹)"""
        try:
            # PDFì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ (ê¸°ì¡´ ë¡œì§ + êµ¬ì¡° ì •ë³´)
            from PyPDF2 import PdfReader
            
            reader = PdfReader(file_path)
            text_chunks = []
            
            for page_num, page in enumerate(reader.pages):
                text = page.extract_text()
                if text.strip():
                    # ì ì‘í˜• ì²­í‚¹ ì ìš©
                    chunks = await self._adaptive_chunking(text, page_num)
                    text_chunks.extend(chunks)
            
            return text_chunks
            
        except Exception as e:
            logger.error(f"âŒ í–¥ìƒëœ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return []
    
    async def _extract_and_analyze_images(self, file_path: str) -> List[Dict[str, Any]]:
        """ì´ë¯¸ì§€ ì¶”ì¶œ ë° ë¶„ì„ (OCR + ì„¤ëª… ìƒì„±)"""
        if not VISION_AVAILABLE:
            return []
        
        try:
            # PDFì—ì„œ ì´ë¯¸ì§€ ì¶”ì¶œ (ì‹¤ì œ êµ¬í˜„ ì‹œ fitz ë“± ì‚¬ìš©)
            images = []
            
            # ê° ì´ë¯¸ì§€ì— ëŒ€í•´ OCR ë° ì„¤ëª… ìƒì„±
            for i, image_data in enumerate([]):  # ì‹¤ì œ ì´ë¯¸ì§€ ë°ì´í„°
                image_info = {
                    "image_id": f"img_{i}",
                    "ocr_text": "",
                    "description": "",
                    "metadata": {"page": i, "type": "figure"}
                }
                
                # OCR í…ìŠ¤íŠ¸ ì¶”ì¶œ
                try:
                    # image_info["ocr_text"] = pytesseract.image_to_string(image_data)
                    pass
                except:
                    pass
                
                # DeepSeekìœ¼ë¡œ ì´ë¯¸ì§€ ì„¤ëª… ìƒì„± (í…ìŠ¤íŠ¸ ê¸°ë°˜)
                if image_info["ocr_text"]:
                    description_prompt = f"ë‹¤ìŒ ì´ë¯¸ì§€ì˜ OCR í…ìŠ¤íŠ¸ë¥¼ ë¶„ì„í•˜ì—¬ êµìœ¡ì  ì„¤ëª…ì„ ì œê³µí•˜ì„¸ìš”: {image_info['ocr_text']}"
                    desc_result = await self.deepseek.chat_completion(
                        messages=[{"role": "user", "content": description_prompt}],
                        temperature=0.3
                    )
                    if desc_result["success"]:
                        image_info["description"] = desc_result["content"]
                
                images.append(image_info)
            
            return images
            
        except Exception as e:
            logger.error(f"âŒ ì´ë¯¸ì§€ ë¶„ì„ ì‹¤íŒ¨: {e}")
            return []
    
    async def _extract_structured_tables(self, file_path: str) -> List[Dict[str, Any]]:
        """êµ¬ì¡°í™”ëœ í‘œ ì¶”ì¶œ"""
        try:
            # í‘œ ì¶”ì¶œ ë¡œì§ (ì‹¤ì œ êµ¬í˜„ ì‹œ camelot, tabula ë“± ì‚¬ìš©)
            tables = []
            
            # ê° í‘œë¥¼ êµ¬ì¡°í™”í•˜ì—¬ ì €ì¥
            for i, table_data in enumerate([]):  # ì‹¤ì œ í‘œ ë°ì´í„°
                table_info = {
                    "table_id": f"table_{i}",
                    "headers": [],
                    "rows": [],
                    "summary": "",
                    "metadata": {"page": i, "type": "table"}
                }
                
                # DeepSeekìœ¼ë¡œ í‘œ ìš”ì•½ ìƒì„±
                table_text = str(table_data)
                summary_prompt = f"ë‹¤ìŒ í‘œì˜ ë‚´ìš©ì„ êµìœ¡ì  ê´€ì ì—ì„œ ìš”ì•½í•˜ì„¸ìš”: {table_text}"
                summary_result = await self.deepseek.chat_completion(
                    messages=[{"role": "user", "content": summary_prompt}],
                    temperature=0.3
                )
                if summary_result["success"]:
                    table_info["summary"] = summary_result["content"]
                
                tables.append(table_info)
            
            return tables
            
        except Exception as e:
            logger.error(f"âŒ í‘œ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return []
    
    # ============ 2. ì ì‘í˜• ì²­í‚¹ ============
    
    async def _adaptive_chunking(self, text: str, page_num: int) -> List[Dict[str, Any]]:
        """ì ì‘í˜• ì§€ëŠ¥ ì²­í‚¹ (ë¬¸ì„œ êµ¬ì¡° ì¸ì‹)"""
        try:
            chunks = []
            
            # 1. ê¸°ë³¸ êµ¬ì¡° ë¶„ì„ (ì œëª©, ë‹¨ë½, ë¦¬ìŠ¤íŠ¸ ë“±)
            lines = text.split('\n')
            current_chunk = ""
            chunk_type = "paragraph"
            
            for line in lines:
                line = line.strip()
                if not line:
                    continue
                
                # ì œëª© íŒ¨í„´ ê°ì§€
                if self._is_heading(line):
                    if current_chunk:
                        chunks.append(self._create_chunk(current_chunk, chunk_type, page_num, len(chunks)))
                        current_chunk = ""
                    chunk_type = "heading"
                    current_chunk = line
                
                # ë¦¬ìŠ¤íŠ¸ íŒ¨í„´ ê°ì§€
                elif self._is_list_item(line):
                    if chunk_type != "list" and current_chunk:
                        chunks.append(self._create_chunk(current_chunk, chunk_type, page_num, len(chunks)))
                        current_chunk = ""
                    chunk_type = "list"
                    current_chunk += f"{line}\n"
                
                # ì¼ë°˜ í…ìŠ¤íŠ¸
                else:
                    if chunk_type != "paragraph" and current_chunk:
                        chunks.append(self._create_chunk(current_chunk, chunk_type, page_num, len(chunks)))
                        current_chunk = ""
                    chunk_type = "paragraph"
                    current_chunk += f"{line}\n"
                
                # ì²­í¬ í¬ê¸° ì œí•œ
                if len(current_chunk) > 1000:
                    chunks.append(self._create_chunk(current_chunk, chunk_type, page_num, len(chunks)))
                    current_chunk = ""
            
            # ë§ˆì§€ë§‰ ì²­í¬ ì²˜ë¦¬
            if current_chunk:
                chunks.append(self._create_chunk(current_chunk, chunk_type, page_num, len(chunks)))
            
            return chunks
            
        except Exception as e:
            logger.error(f"âŒ ì ì‘í˜• ì²­í‚¹ ì‹¤íŒ¨: {e}")
            return [self._create_chunk(text, "paragraph", page_num, 0)]
    
    def _is_heading(self, line: str) -> bool:
        """ì œëª© íŒ¨í„´ ê°ì§€"""
        heading_patterns = [
            line.isupper() and len(line) < 100,
            line.startswith(('Chapter', 'ì¥', 'ì œ', '1.', '2.', '3.')),
            len(line.split()) < 10 and line.endswith((':'))
        ]
        return any(heading_patterns)
    
    def _is_list_item(self, line: str) -> bool:
        """ë¦¬ìŠ¤íŠ¸ í•­ëª© ê°ì§€"""
        list_patterns = [
            line.startswith(('â€¢', '-', '*', 'â–ª', 'â—‹')),
            line.startswith(tuple(f'{i}.' for i in range(1, 21))),
            line.startswith(tuple(f'({i})' for i in range(1, 21)))
        ]
        return any(list_patterns)
    
    def _create_chunk(self, content: str, chunk_type: str, page_num: int, chunk_index: int) -> Dict[str, Any]:
        """ì²­í¬ ê°ì²´ ìƒì„±"""
        return {
            "content": content.strip(),
            "type": chunk_type,
            "page": page_num,
            "index": chunk_index,
            "length": len(content),
            "created_at": datetime.now().isoformat()
        }
    
    # ============ 3. í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ============
    
    async def hybrid_search(
        self,
        db: Session,
        query: str,
        search_mode: str = "hybrid",
        limit: int = 10,
        filters: Optional[Dict] = None
    ) -> Dict[str, Any]:
        """í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ (í‚¤ì›Œë“œ + ì‹œë§¨í‹± + ê·¸ë˜í”„)"""
        try:
            logger.info(f"ğŸ” í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì‹œì‘: {query} (ëª¨ë“œ: {search_mode})")
            
            results = {
                "query": query,
                "mode": search_mode,
                "results": [],
                "search_breakdown": {}
            }
            
            if search_mode == "hybrid":
                # 1. ì‹œë§¨í‹± ê²€ìƒ‰ (Qdrant)
                semantic_results = await self._semantic_search(query, limit//2, filters)
                results["search_breakdown"]["semantic"] = len(semantic_results)
                
                # 2. í‚¤ì›Œë“œ ê²€ìƒ‰ (PostgreSQL)
                keyword_results = await self._keyword_search(db, query, limit//2, filters)
                results["search_breakdown"]["keyword"] = len(keyword_results)
                
                # 3. ê²°ê³¼ ìœµí•© (RRF - Reciprocal Rank Fusion)
                fused_results = self._reciprocal_rank_fusion([semantic_results, keyword_results])
                results["results"] = fused_results[:limit]
                
            elif search_mode == "dense":
                # ìˆœìˆ˜ ì‹œë§¨í‹± ê²€ìƒ‰
                results["results"] = await self._semantic_search(query, limit, filters)
                results["search_breakdown"]["semantic"] = len(results["results"])
                
            elif search_mode == "sparse":
                # ìˆœìˆ˜ í‚¤ì›Œë“œ ê²€ìƒ‰
                results["results"] = await self._keyword_search(db, query, limit, filters)
                results["search_breakdown"]["keyword"] = len(results["results"])
                
            elif search_mode == "graph":
                # ê·¸ë˜í”„ ê¸°ë°˜ ì—°ê´€ ê²€ìƒ‰
                results["results"] = await self._graph_search(db, query, limit, filters)
                results["search_breakdown"]["graph"] = len(results["results"])
            
            # ê²€ìƒ‰ ì„±ëŠ¥ ê¸°ë¡
            self._record_search_performance(query, search_mode, len(results["results"]))
            
            logger.info(f"âœ… í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì™„ë£Œ: {len(results['results'])}ê°œ ê²°ê³¼")
            return {"success": True, "data": results}
            
        except Exception as e:
            logger.error(f"âŒ í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            return {"success": False, "error": str(e)}
    
    async def _semantic_search(self, query: str, limit: int, filters: Optional[Dict]) -> List[Dict]:
        """ì‹œë§¨í‹± ê²€ìƒ‰ (ë²¡í„° ìœ ì‚¬ë„)"""
        search_result = await self.vector_db.search_vectors(
            query_text=query,
            limit=limit,
            score_threshold=0.6,
            filter_conditions=filters
        )
        
        if search_result["success"]:
            return [
                {
                    "content": item["text"],
                    "score": item["score"],
                    "source": "semantic",
                    "metadata": item["metadata"]
                }
                for item in search_result["results"]
            ]
        return []
    
    async def _keyword_search(self, db: Session, query: str, limit: int, filters: Optional[Dict]) -> List[Dict]:
        """í‚¤ì›Œë“œ ê²€ìƒ‰ (PostgreSQL FTS)"""
        try:
            # PostgreSQLì˜ ì „ë¬¸ ê²€ìƒ‰ ì‚¬ìš©
            search_query = text("""
                SELECT id, content, subject_name, difficulty, 
                       ts_rank(to_tsvector('korean', content), plainto_tsquery('korean', :query)) as rank
                FROM questions 
                WHERE to_tsvector('korean', content) @@ plainto_tsquery('korean', :query)
                ORDER BY rank DESC
                LIMIT :limit
            """)
            
            result = db.execute(search_query, {"query": query, "limit": limit})
            rows = result.fetchall()
            
            return [
                {
                    "content": row.content,
                    "score": float(row.rank),
                    "source": "keyword",
                    "metadata": {
                        "question_id": row.id,
                        "subject": row.subject_name,
                        "difficulty": row.difficulty
                    }
                }
                for row in rows
            ]
            
        except Exception as e:
            logger.error(f"âŒ í‚¤ì›Œë“œ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            return []
    
    async def _graph_search(self, db: Session, query: str, limit: int, filters: Optional[Dict]) -> List[Dict]:
        """ê·¸ë˜í”„ ê¸°ë°˜ ì—°ê´€ ê²€ìƒ‰"""
        try:
            # 1. ì´ˆê¸° ì‹œë§¨í‹± ê²€ìƒ‰ìœ¼ë¡œ ì‹œë“œ ë…¸ë“œ ì°¾ê¸°
            seed_results = await self._semantic_search(query, 3, filters)
            
            if not seed_results:
                return []
            
            # 2. ì‹œë“œ ë…¸ë“œì™€ ì—°ê´€ëœ ì½˜í…ì¸  ì°¾ê¸°
            related_content = []
            
            for seed in seed_results:
                if "question_id" in seed["metadata"]:
                    # ê°™ì€ ê³¼ëª©/ë‚œì´ë„ì˜ ë¬¸ì œë“¤ ì°¾ê¸°
                    related_query = text("""
                        SELECT content, subject_name, difficulty
                        FROM questions 
                        WHERE subject_name = :subject 
                        AND difficulty = :difficulty
                        AND id != :question_id
                        LIMIT 3
                    """)
                    
                    result = db.execute(related_query, {
                        "subject": seed["metadata"].get("subject", ""),
                        "difficulty": seed["metadata"].get("difficulty", ""),
                        "question_id": seed["metadata"]["question_id"]
                    })
                    
                    for row in result.fetchall():
                        related_content.append({
                            "content": row.content,
                            "score": seed["score"] * 0.8,  # ì—°ê´€ë„ ê°ì†Œ
                            "source": "graph",
                            "metadata": {
                                "subject": row.subject_name,
                                "difficulty": row.difficulty,
                                "relation": "same_category"
                            }
                        })
            
            # 3. ì›ë³¸ ê²°ê³¼ì™€ ì—°ê´€ ê²°ê³¼ ê²°í•©
            all_results = seed_results + related_content
            
            # ì¤‘ë³µ ì œê±° ë° ì ìˆ˜ìˆœ ì •ë ¬
            unique_results = {}
            for item in all_results:
                content_hash = hashlib.md5(item["content"].encode()).hexdigest()
                if content_hash not in unique_results or unique_results[content_hash]["score"] < item["score"]:
                    unique_results[content_hash] = item
            
            return sorted(unique_results.values(), key=lambda x: x["score"], reverse=True)[:limit]
            
        except Exception as e:
            logger.error(f"âŒ ê·¸ë˜í”„ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            return []
    
    def _reciprocal_rank_fusion(self, result_lists: List[List[Dict]], k: int = 60) -> List[Dict]:
        """Reciprocal Rank Fusionìœ¼ë¡œ ê²€ìƒ‰ ê²°ê³¼ ìœµí•©"""
        scores = defaultdict(float)
        all_items = {}
        
        for result_list in result_lists:
            for rank, item in enumerate(result_list):
                content_hash = hashlib.md5(item["content"].encode()).hexdigest()
                scores[content_hash] += 1 / (k + rank + 1)
                all_items[content_hash] = item
        
        # ì ìˆ˜ìˆœìœ¼ë¡œ ì •ë ¬í•˜ì—¬ ë°˜í™˜
        sorted_items = sorted(scores.items(), key=lambda x: x[1], reverse=True)
        return [all_items[content_hash] for content_hash, score in sorted_items]
    
    # ============ 4. RAG Fusion ============
    
    async def rag_fusion_search(
        self,
        db: Session,
        original_query: str,
        num_queries: int = 5,
        fusion_method: str = "rrf"
    ) -> Dict[str, Any]:
        """RAG Fusion - ë‹¤ì¤‘ ì¿¼ë¦¬ ìƒì„± ë° ê²°ê³¼ ìœµí•©"""
        try:
            logger.info(f"ğŸ”¥ RAG Fusion ê²€ìƒ‰ ì‹œì‘: {original_query}")
            
            # 1. ë‹¤ì–‘í•œ ê´€ì ì˜ ì¿¼ë¦¬ ìƒì„±
            generated_queries = await self._generate_multiple_queries(original_query, num_queries)
            
            # 2. ê° ì¿¼ë¦¬ë¡œ ê°œë³„ ê²€ìƒ‰ ìˆ˜í–‰
            all_results = []
            query_results = {}
            
            for i, query in enumerate([original_query] + generated_queries):
                search_result = await self.hybrid_search(db, query, "hybrid", 10)
                if search_result["success"]:
                    results = search_result["data"]["results"]
                    all_results.append(results)
                    query_results[f"query_{i}"] = {"query": query, "results_count": len(results)}
            
            # 3. ê²°ê³¼ ìœµí•©
            if fusion_method == "rrf":
                fused_results = self._reciprocal_rank_fusion(all_results)
            elif fusion_method == "weighted":
                fused_results = self._weighted_fusion(all_results, [1.0] + [0.8] * len(generated_queries))
            else:
                fused_results = self._neural_fusion(all_results)
            
            return {
                "success": True,
                "original_query": original_query,
                "generated_queries": generated_queries,
                "fusion_method": fusion_method,
                "query_breakdown": query_results,
                "final_results": fused_results[:15],
                "total_unique_results": len(fused_results)
            }
            
        except Exception as e:
            logger.error(f"âŒ RAG Fusion ì‹¤íŒ¨: {e}")
            return {"success": False, "error": str(e)}
    
    async def _generate_multiple_queries(self, original_query: str, num_queries: int) -> List[str]:
        """ë‹¤ì–‘í•œ ê´€ì ì˜ ì¿¼ë¦¬ ìƒì„±"""
        prompt = f"""
ì›ë³¸ ì§ˆë¬¸: "{original_query}"

ìœ„ ì§ˆë¬¸ì— ëŒ€í•´ ë‹¤ì–‘í•œ ê´€ì ì—ì„œ {num_queries}ê°œì˜ ìœ ì‚¬í•˜ì§€ë§Œ ë‹¤ë¥¸ ì§ˆë¬¸ë“¤ì„ ìƒì„±í•´ì£¼ì„¸ìš”.
ê° ì§ˆë¬¸ì€ ë‹¤ìŒê³¼ ê°™ì€ ë‹¤ë¥¸ ì ‘ê·¼ ë°©ì‹ì„ ì‚¬ìš©í•´ì•¼ í•©ë‹ˆë‹¤:
1. êµ¬ì²´ì  ì„¸ë¶€ì‚¬í•­ ì¤‘ì‹¬
2. ê´‘ë²”ìœ„í•œ ë§¥ë½ ì¤‘ì‹¬  
3. ì‹¤ë¬´ ì ìš© ì¤‘ì‹¬
4. ì´ë¡ ì  ë°°ê²½ ì¤‘ì‹¬
5. ë¬¸ì œ í•´ê²° ì¤‘ì‹¬

JSON í˜•ì‹ìœ¼ë¡œ ë°˜í™˜í•´ì£¼ì„¸ìš”:
{{"queries": ["ì§ˆë¬¸1", "ì§ˆë¬¸2", "ì§ˆë¬¸3", "ì§ˆë¬¸4", "ì§ˆë¬¸5"]}}
"""
        
        result = await self.deepseek.chat_completion(
            messages=[{"role": "user", "content": prompt}],
            temperature=0.8
        )
        
        if result["success"]:
            try:
                data = json.loads(result["content"])
                return data.get("queries", [])[:num_queries]
            except json.JSONDecodeError:
                pass
        
        # í´ë°±: ê°„ë‹¨í•œ ë³€í˜• ìƒì„±
        return [
            f"{original_query} ì‹¤ë¬´ ì‚¬ë¡€",
            f"{original_query} ì´ë¡ ì  ë°°ê²½",
            f"{original_query} ë¬¸ì œ í•´ê²°",
            f"{original_query} ì„¸ë¶€ ë‚´ìš©"
        ][:num_queries]
    
    def _weighted_fusion(self, result_lists: List[List[Dict]], weights: List[float]) -> List[Dict]:
        """ê°€ì¤‘ì¹˜ ê¸°ë°˜ ê²°ê³¼ ìœµí•©"""
        scores = defaultdict(float)
        all_items = {}
        
        for i, (result_list, weight) in enumerate(zip(result_lists, weights)):
            for rank, item in enumerate(result_list):
                content_hash = hashlib.md5(item["content"].encode()).hexdigest()
                scores[content_hash] += weight * item.get("score", 1.0) / (rank + 1)
                all_items[content_hash] = item
        
        sorted_items = sorted(scores.items(), key=lambda x: x[1], reverse=True)
        return [all_items[content_hash] for content_hash, score in sorted_items]
    
    def _neural_fusion(self, result_lists: List[List[Dict]]) -> List[Dict]:
        """ì‹ ê²½ë§ ê¸°ë°˜ ê²°ê³¼ ìœµí•© (ê°„ë‹¨í•œ ì•™ìƒë¸”)"""
        # í˜„ì¬ëŠ” RRFì™€ ë™ì¼í•˜ê²Œ êµ¬í˜„, ì¶”í›„ ML ëª¨ë¸ë¡œ í™•ì¥ ê°€ëŠ¥
        return self._reciprocal_rank_fusion(result_lists)
    
    # ============ 5. ì‹¤ì‹œê°„ í•™ìŠµ ë° ê°œì¸í™” ============
    
    async def update_from_feedback(
        self,
        db: Session,
        user_id: int,
        query: str,
        selected_result: Dict[str, Any],
        feedback_score: float,
        feedback_comment: Optional[str] = None
    ) -> Dict[str, Any]:
        """ì‚¬ìš©ì í”¼ë“œë°± ê¸°ë°˜ ì‹¤ì‹œê°„ í•™ìŠµ"""
        try:
            logger.info(f"ğŸ“š ì‹¤ì‹œê°„ í•™ìŠµ ì—…ë°ì´íŠ¸: ì‚¬ìš©ì {user_id}")
            
            # 1. í”¼ë“œë°± ê¸°ë¡
            feedback_data = {
                "user_id": user_id,
                "query": query,
                "selected_content": selected_result.get("content", ""),
                "score": feedback_score,
                "comment": feedback_comment,
                "timestamp": datetime.now().isoformat(),
                "metadata": selected_result.get("metadata", {})
            }
            
            self.user_feedback[user_id].append(feedback_data)
            
            # 2. ê¸ì •ì  í”¼ë“œë°±ì¸ ê²½ìš° ë²¡í„° ê°•í™”
            if feedback_score >= 4.0:  # 5ì  ë§Œì ì—ì„œ 4ì  ì´ìƒ
                await self._enhance_positive_vector(selected_result, user_id)
            
            # 3. ë¶€ì •ì  í”¼ë“œë°±ì¸ ê²½ìš° ë²¡í„° ì¡°ì •
            elif feedback_score <= 2.0:  # 2ì  ì´í•˜
                await self._adjust_negative_vector(selected_result, user_id)
            
            # 4. ê°œì¸í™” í”„ë¡œí•„ ì—…ë°ì´íŠ¸
            await self._update_user_preference_profile(user_id, query, selected_result, feedback_score)
            
            return {
                "success": True,
                "message": "í”¼ë“œë°±ì´ ë°˜ì˜ë˜ì—ˆìŠµë‹ˆë‹¤",
                "user_feedback_count": len(self.user_feedback[user_id]),
                "learning_status": "updated"
            }
            
        except Exception as e:
            logger.error(f"âŒ ì‹¤ì‹œê°„ í•™ìŠµ ì‹¤íŒ¨: {e}")
            return {"success": False, "error": str(e)}
    
    async def _enhance_positive_vector(self, result: Dict[str, Any], user_id: int):
        """ê¸ì •ì  í”¼ë“œë°± ë²¡í„° ê°•í™”"""
        try:
            content = result.get("content", "")
            if not content:
                return
            
            # ê¸°ì¡´ ë²¡í„°ì˜ ê°€ì¤‘ì¹˜ ì¦ê°€ (ë©”íƒ€ë°ì´í„°ì— ë°˜ì˜)
            enhanced_metadata = result.get("metadata", {})
            enhanced_metadata["positive_feedback_count"] = enhanced_metadata.get("positive_feedback_count", 0) + 1
            enhanced_metadata["user_preferences"] = enhanced_metadata.get("user_preferences", [])
            
            if user_id not in enhanced_metadata["user_preferences"]:
                enhanced_metadata["user_preferences"].append(user_id)
            
            # Qdrantì— ê°•í™”ëœ ë²¡í„° ì €ì¥ (ë³„ë„ ì»¬ë ‰ì…˜ ë˜ëŠ” ë©”íƒ€ë°ì´í„° ì—…ë°ì´íŠ¸)
            await self.vector_db.add_vectors(
                texts=[content],
                metadatas=[enhanced_metadata],
                ids=[f"enhanced_{user_id}_{uuid.uuid4()}"]
            )
            
        except Exception as e:
            logger.error(f"âŒ ê¸ì •ì  ë²¡í„° ê°•í™” ì‹¤íŒ¨: {e}")
    
    async def _update_user_preference_profile(
        self, 
        user_id: int, 
        query: str, 
        result: Dict[str, Any], 
        score: float
    ):
        """ì‚¬ìš©ì ì„ í˜¸ë„ í”„ë¡œí•„ ì—…ë°ì´íŠ¸"""
        try:
            # ì‚¬ìš©ìë³„ ì„ í˜¸ë„ ë¶„ì„
            if user_id not in self.user_feedback:
                self.user_feedback[user_id] = []
            
            # ì„ í˜¸í•˜ëŠ” ì½˜í…ì¸  ìœ í˜• ë¶„ì„
            metadata = result.get("metadata", {})
            subject = metadata.get("subject", "")
            difficulty = metadata.get("difficulty", "")
            
            # ê°„ë‹¨í•œ ì„ í˜¸ë„ ì ìˆ˜ ê³„ì‚°
            preferences = {
                "preferred_subjects": defaultdict(float),
                "preferred_difficulty": defaultdict(float),
                "query_patterns": []
            }
            
            for feedback in self.user_feedback[user_id]:
                if feedback["score"] >= 4.0:
                    meta = feedback["metadata"]
                    preferences["preferred_subjects"][meta.get("subject", "")] += 1
                    preferences["preferred_difficulty"][meta.get("difficulty", "")] += 1
            
            logger.info(f"ğŸ“Š ì‚¬ìš©ì {user_id} ì„ í˜¸ë„ í”„ë¡œí•„ ì—…ë°ì´íŠ¸ ì™„ë£Œ")
            
        except Exception as e:
            logger.error(f"âŒ ì„ í˜¸ë„ í”„ë¡œí•„ ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {e}")
    
    # ============ 6. ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ============
    
    def _record_search_performance(self, query: str, mode: str, result_count: int):
        """ê²€ìƒ‰ ì„±ëŠ¥ ê¸°ë¡"""
        performance_data = {
            "timestamp": datetime.now().isoformat(),
            "query": query,
            "mode": mode,
            "result_count": result_count,
            "response_time": 0  # ì‹¤ì œ êµ¬í˜„ ì‹œ ì¸¡ì •
        }
        
        self.performance_metrics[mode].append(performance_data)
        
        # ìµœê·¼ 1000ê°œ ê¸°ë¡ë§Œ ìœ ì§€
        if len(self.performance_metrics[mode]) > 1000:
            self.performance_metrics[mode] = self.performance_metrics[mode][-1000:]
    
    async def get_performance_analytics(self) -> Dict[str, Any]:
        """ì„±ëŠ¥ ë¶„ì„ ë¦¬í¬íŠ¸"""
        try:
            analytics = {
                "total_searches": sum(len(metrics) for metrics in self.performance_metrics.values()),
                "search_modes": {},
                "user_satisfaction": {},
                "query_patterns": {},
                "system_health": "excellent"
            }
            
            # ëª¨ë“œë³„ ì„±ëŠ¥ ë¶„ì„
            for mode, metrics in self.performance_metrics.items():
                if metrics:
                    avg_results = sum(m["result_count"] for m in metrics) / len(metrics)
                    analytics["search_modes"][mode] = {
                        "total_searches": len(metrics),
                        "avg_results": round(avg_results, 2),
                        "last_used": metrics[-1]["timestamp"] if metrics else None
                    }
            
            # ì‚¬ìš©ì ë§Œì¡±ë„ ë¶„ì„
            total_feedback = sum(len(feedback) for feedback in self.user_feedback.values())
            if total_feedback > 0:
                all_scores = []
                for user_feedback in self.user_feedback.values():
                    all_scores.extend([f["score"] for f in user_feedback])
                
                analytics["user_satisfaction"] = {
                    "total_feedback": total_feedback,
                    "avg_score": round(sum(all_scores) / len(all_scores), 2),
                    "satisfaction_rate": round(len([s for s in all_scores if s >= 4.0]) / len(all_scores) * 100, 1)
                }
            
            return analytics
            
        except Exception as e:
            logger.error(f"âŒ ì„±ëŠ¥ ë¶„ì„ ì‹¤íŒ¨: {e}")
            return {"error": str(e)}

# ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤
advanced_rag_service = AdvancedRAGService() 