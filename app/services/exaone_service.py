"""
ë¡œì»¬ Exaone ì„œë¹„ìŠ¤
Ollamaë¥¼ í†µí•œ ë¡œì»¬ Exaone ëª¨ë¸ ì‹¤í–‰
DeepSeek + OpenAI ê¸°ëŠ¥ì„ í†µí•© ëŒ€ì²´
"""

import json
import logging
import httpx
import asyncio
from typing import Dict, List, Any, Optional, Union
from datetime import datetime
import time
import uuid
import os
from pathlib import Path

logger = logging.getLogger(__name__)

class LocalExaoneService:
    """ë¡œì»¬ Exaone AI ì„œë¹„ìŠ¤ (Ollama ê¸°ë°˜)"""
    
    def __init__(self):
        # Ollama ì„¤ì •
        self.ollama_host = os.getenv("OLLAMA_HOST", "http://localhost:11434")
        self.model_name = "exaone-deep:7.8b"
        self.embedding_model = "mxbai-embed-large"
        
        # HTTP í´ë¼ì´ì–¸íŠ¸
        self.client = httpx.AsyncClient(timeout=300.0)
        
        # ìºì‹œ ë° í†µê³„
        self.conversation_cache = {}
        self.performance_stats = {
            "total_requests": 0,
            "successful_requests": 0,
            "failed_requests": 0,
            "average_response_time": 0.0,
            "cache_hits": 0
        }
        
        logger.info(f"âœ… ë¡œì»¬ Exaone ì„œë¹„ìŠ¤ ì´ˆê¸°í™” ì™„ë£Œ")

    async def check_model_availability(self) -> bool:
        """ëª¨ë¸ ì‚¬ìš© ê°€ëŠ¥ì„± í™•ì¸"""
        try:
            response = await self.client.get(f"{self.ollama_host}/api/tags")
            if response.status_code == 200:
                models = response.json().get("models", [])
                available_models = [model["name"] for model in models]
                
                if self.model_name in available_models:
                    logger.info(f"âœ… Exaone ëª¨ë¸ ì‚¬ìš© ê°€ëŠ¥: {self.model_name}")
                    return True
                else:
                    logger.warning(f"âŒ Exaone ëª¨ë¸ ì—†ìŒ: {self.model_name}")
                    logger.info(f"ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸: {available_models}")
                    return False
            return False
        except Exception as e:
            logger.error(f"ëª¨ë¸ í™•ì¸ ì‹¤íŒ¨: {e}")
            return False

    async def chat_completion(
        self,
        messages: List[Dict[str, str]],
        temperature: float = 0.7,
        max_tokens: Optional[int] = None,
        stream: bool = False
    ) -> Dict[str, Any]:
        """
        Exaone ì±„íŒ… ì™„ì„± (OpenAI ChatCompletionê³¼ ë™ì¼í•œ ì¸í„°í˜ì´ìŠ¤)
        """
        start_time = time.time()
        self.performance_stats["total_requests"] += 1
        
        try:
            # ë©”ì‹œì§€ë¥¼ ë‹¨ì¼ í”„ë¡¬í”„íŠ¸ë¡œ ë³€í™˜
            prompt = self._convert_messages_to_prompt(messages)
            
            # ìºì‹œ í™•ì¸
            cache_key = f"{hash(prompt)}_{temperature}_{max_tokens}"
            if cache_key in self.conversation_cache:
                self.performance_stats["cache_hits"] += 1
                logger.info("ğŸ’¾ ìºì‹œì—ì„œ ì‘ë‹µ ë°˜í™˜")
                return self.conversation_cache[cache_key]
            
            # Ollama API í˜¸ì¶œ
            payload = {
                "model": self.model_name,
                "prompt": prompt,
                "stream": False,
                "options": {
                    "temperature": temperature,
                    "num_predict": max_tokens or 2048,
                    "top_k": 40,
                    "top_p": 0.9
                }
            }
            
            response = await self.client.post(
                f"{self.ollama_host}/api/generate",
                json=payload,
                timeout=300.0
            )
            
            if response.status_code == 200:
                result_data = response.json()
                content = result_data.get("response", "").strip()
                
                # ì‘ë‹µ ì‹œê°„ ê³„ì‚°
                response_time = time.time() - start_time
                
                result = {
                    "success": True,
                    "content": content,
                    "model": self.model_name,
                    "response_time": response_time,
                    "tokens_used": len(content.split()),
                    "timestamp": datetime.now().isoformat()
                }
                
                # ìºì‹œì— ì €ì¥
                self.conversation_cache[cache_key] = result
                
                # í†µê³„ ì—…ë°ì´íŠ¸
                self.performance_stats["successful_requests"] += 1
                self._update_average_response_time(response_time)
                
                logger.info(f"âœ… Exaone ì±„íŒ… ì™„ì„± ì„±ê³µ ({response_time:.2f}ì´ˆ)")
                return result
            else:
                raise Exception(f"Ollama API ì˜¤ë¥˜: {response.status_code}")
                
        except Exception as e:
            self.performance_stats["failed_requests"] += 1
            logger.error(f"ë¡œì»¬ Exaone ì±„íŒ… ì™„ì„± ì˜¤ë¥˜: {e}")
            return {
                "success": False,
                "error": str(e),
                "model": self.model_name,
                "timestamp": datetime.now().isoformat()
            }

    def _convert_messages_to_prompt(self, messages: List[Dict[str, str]]) -> str:
        """OpenAI ë©”ì‹œì§€ í˜•ì‹ì„ ë‹¨ì¼ í”„ë¡¬í”„íŠ¸ë¡œ ë³€í™˜"""
        prompt_parts = []
        
        for message in messages:
            role = message.get("role", "user")
            content = message.get("content", "")
            
            if role == "system":
                prompt_parts.append(f"[ì‹œìŠ¤í…œ ì§€ì‹œì‚¬í•­]\n{content}\n\n")
            elif role == "user":
                prompt_parts.append(f"[ì‚¬ìš©ì ì§ˆë¬¸]\n{content}\n\n")
            elif role == "assistant":
                prompt_parts.append(f"[ì´ì „ ë‹µë³€]\n{content}\n\n")
        
        prompt_parts.append("[AI ë‹µë³€]")
        return "".join(prompt_parts)

    async def create_embeddings(
        self,
        texts: Union[str, List[str]],
        normalize: bool = True
    ) -> Dict[str, Any]:
        """
        í…ìŠ¤íŠ¸ ì„ë² ë”© ìƒì„± (OpenAI Embeddingê³¼ ë™ì¼í•œ ì¸í„°í˜ì´ìŠ¤)
        """
        try:
            if isinstance(texts, str):
                texts = [texts]
            
            embeddings = []
            
            for text in texts:
                payload = {
                    "model": self.embedding_model,
                    "prompt": text
                }
                
                response = await self.client.post(
                    f"{self.ollama_host}/api/embeddings",
                    json=payload
                )
                
                if response.status_code == 200:
                    result = response.json()
                    embedding = result.get("embedding", [])
                    
                    if normalize and embedding:
                        # ë²¡í„° ì •ê·œí™”
                        import numpy as np
                        embedding = np.array(embedding)
                        norm = np.linalg.norm(embedding)
                        if norm > 0:
                            embedding = embedding / norm
                        embedding = embedding.tolist()
                    
                    embeddings.append(embedding)
                else:
                    logger.error(f"ì„ë² ë”© ìƒì„± ì‹¤íŒ¨: {response.status_code}")
                    embeddings.append([])
            
            return {
                "success": True,
                "embeddings": embeddings,
                "model": self.embedding_model,
                "total_tokens": sum(len(text.split()) for text in texts)
            }
            
        except Exception as e:
            logger.error(f"ë¡œì»¬ Exaone ì„ë² ë”© ìƒì„± ì˜¤ë¥˜: {e}")
            return {
                "success": False,
                "error": str(e),
                "embeddings": []
            }

    # === AI ë¬¸ì œ ìƒì„± ê¸°ëŠ¥ ===
    
    async def generate_question(
        self,
        topic: str,
        difficulty: str = "medium",
        department: str = "ì¼ë°˜í•™ê³¼",
        question_type: str = "multiple_choice"
    ) -> Dict[str, Any]:
        """AI ë¬¸ì œ ìƒì„±"""
        prompt = f"""
ë‹¹ì‹ ì€ {department} ì „ë¬¸ êµìœ¡ìì…ë‹ˆë‹¤.

ë‹¤ìŒ ì¡°ê±´ì— ë§ëŠ” ë¬¸ì œë¥¼ ìƒì„±í•´ì£¼ì„¸ìš”:
- ì£¼ì œ: {topic}
- ë‚œì´ë„: {difficulty}
- ë¬¸ì œ ìœ í˜•: {question_type}

ì¶œë ¥ í˜•ì‹:
{{
    "question": "ë¬¸ì œ ë‚´ìš©",
    "options": {{
        "1": "ì„ íƒì§€ 1",
        "2": "ì„ íƒì§€ 2", 
        "3": "ì„ íƒì§€ 3",
        "4": "ì„ íƒì§€ 4"
    }},
    "correct_answer": "ì •ë‹µ ë²ˆí˜¸",
    "explanation": "í•´ì„¤",
    "difficulty": "{difficulty}",
    "subject": "{topic}",
    "department": "{department}"
}}

JSON í˜•ì‹ìœ¼ë¡œë§Œ ë‹µë³€í•´ì£¼ì„¸ìš”.
"""
        
        messages = [{"role": "user", "content": prompt}]
        result = await self.chat_completion(messages, temperature=0.7)
        
        if result["success"]:
            try:
                content = result["content"]
                # JSON íŒŒì‹± ì‹œë„
                import re
                json_match = re.search(r'\{.*\}', content, re.DOTALL)
                if json_match:
                    question_data = json.loads(json_match.group())
                    return {
                        "success": True,
                        "question": question_data,
                        "generated_by": "Exaone Deep 7.8B"
                    }
            except Exception as e:
                logger.warning(f"JSON íŒŒì‹± ì‹¤íŒ¨: {e}")
        
        return {
            "success": False,
            "error": "ë¬¸ì œ ìƒì„± ì‹¤íŒ¨",
            "raw_response": result.get("content", "")
        }

    # === ë‚œì´ë„ ë¶„ì„ ê¸°ëŠ¥ ===
    
    async def analyze_difficulty(
        self, 
        question_content: str, 
        department: str = "ì¼ë°˜í•™ê³¼"
    ) -> Dict[str, Any]:
        """ë¬¸ì œ ë‚œì´ë„ ë¶„ì„"""
        prompt = f"""
ë‹¹ì‹ ì€ {department} êµìœ¡ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.

ë‹¤ìŒ ë¬¸ì œì˜ ë‚œì´ë„ë¥¼ ë¶„ì„í•´ì£¼ì„¸ìš”:

{question_content}

ë‹¤ìŒ ê¸°ì¤€ìœ¼ë¡œ ë¶„ì„í•´ì£¼ì„¸ìš”:
1. ì§€ì‹ ìˆ˜ì¤€ (ê¸°ì´ˆ/ì‘ìš©/ì‹¬í™”)
2. ì‚¬ê³  ê³¼ì • ë³µì¡ë„
3. ì „ë¬¸ ìš©ì–´ ìˆ˜ì¤€
4. ë¬¸ì œ í•´ê²° ë‹¨ê³„

JSON í˜•ì‹ìœ¼ë¡œ ë‹µë³€:
{{
    "difficulty_level": "í•˜/ì¤‘/ìƒ",
    "difficulty_score": 1-10ì ,
    "analysis": {{
        "knowledge_level": "ë¶„ì„ ë‚´ìš©",
        "complexity": "ë¶„ì„ ë‚´ìš©",
        "terminology": "ë¶„ì„ ë‚´ìš©",
        "problem_solving": "ë¶„ì„ ë‚´ìš©"
    }},
    "recommendation": "êµìˆ˜ìë¥¼ ìœ„í•œ ì¶”ì²œ ì‚¬í•­"
}}
"""
        
        messages = [{"role": "user", "content": prompt}]
        result = await self.chat_completion(messages, temperature=0.3)
        
        if result["success"]:
            try:
                content = result["content"]
                import re
                json_match = re.search(r'\{.*\}', content, re.DOTALL)
                if json_match:
                    analysis_data = json.loads(json_match.group())
                    return {
                        "success": True,
                        "analysis": analysis_data,
                        "analyzed_by": "Exaone Deep 7.8B"
                    }
            except Exception as e:
                logger.warning(f"ë¶„ì„ ê²°ê³¼ íŒŒì‹± ì‹¤íŒ¨: {e}")
        
        return {
            "success": False,
            "error": "ë‚œì´ë„ ë¶„ì„ ì‹¤íŒ¨",
            "raw_response": result.get("content", "")
        }

    # === í…ìŠ¤íŠ¸ ê°œì„  ê¸°ëŠ¥ (OpenAI ëŒ€ì²´) ===
    
    async def improve_text_style(
        self,
        text: str,
        target_style: str = "educational",
        department: str = "ì¼ë°˜í•™ê³¼"
    ) -> Dict[str, Any]:
        """í…ìŠ¤íŠ¸ ë¬¸ì²´ ê°œì„ """
        prompt = f"""
ë‹¹ì‹ ì€ {department} ì „ë¬¸ êµìœ¡ ì½˜í…ì¸  í¸ì§‘ìì…ë‹ˆë‹¤.

ë‹¤ìŒ í…ìŠ¤íŠ¸ë¥¼ {target_style} ìŠ¤íƒ€ì¼ë¡œ ê°œì„ í•´ì£¼ì„¸ìš”:

ì›ë³¸ í…ìŠ¤íŠ¸:
{text}

ê°œì„  ìš”êµ¬ì‚¬í•­:
1. ìì—°ìŠ¤ëŸ¬ìš´ í•œêµ­ì–´ ë¬¸ì²´
2. êµìœ¡ì  í‘œí˜„ìœ¼ë¡œ ë‹¤ë“¬ê¸°
3. ì „ë¬¸ ìš©ì–´ ì ì ˆíˆ ì‚¬ìš©
4. ì´í•´í•˜ê¸° ì‰¬ìš´ êµ¬ì¡°

ê°œì„ ëœ í…ìŠ¤íŠ¸ë§Œ ì¶œë ¥í•´ì£¼ì„¸ìš”.
"""
        
        messages = [{"role": "user", "content": prompt}]
        result = await self.chat_completion(messages, temperature=0.5)
        
        if result["success"]:
            return {
                "success": True,
                "improved_content": result["content"],
                "original_content": text,
                "improved_by": "Exaone Deep 7.8B"
            }
        
        return {
            "success": False,
            "error": "í…ìŠ¤íŠ¸ ê°œì„  ì‹¤íŒ¨",
            "original_content": text
        }

    # === ì½˜í…ì¸  ë¶„ë¥˜ ê¸°ëŠ¥ ===
    
    async def classify_content(
        self,
        content: str,
        classification_type: str = "department"
    ) -> Dict[str, Any]:
        """ì½˜í…ì¸  ë¶„ë¥˜"""
        if classification_type == "department":
            categories = ["ê°„í˜¸í•™ê³¼", "ë¬¼ë¦¬ì¹˜ë£Œí•™ê³¼", "ì‘ì—…ì¹˜ë£Œí•™ê³¼", "ê¸°íƒ€"]
        else:
            categories = ["ì´ë¡ ", "ì‹¤ìŠµ", "ì‚¬ë¡€ì—°êµ¬", "í‰ê°€"]
        
        prompt = f"""
ë‹¤ìŒ ì½˜í…ì¸ ë¥¼ ì ì ˆí•œ ì¹´í…Œê³ ë¦¬ë¡œ ë¶„ë¥˜í•´ì£¼ì„¸ìš”.

ì½˜í…ì¸ :
{content[:1000]}...

ë¶„ë¥˜ ì¹´í…Œê³ ë¦¬: {', '.join(categories)}

JSON í˜•ì‹ìœ¼ë¡œ ë‹µë³€:
{{
    "category": "ë¶„ë¥˜ ê²°ê³¼",
    "confidence": 0.0-1.0,
    "reasoning": "ë¶„ë¥˜ ê·¼ê±°"
}}
"""
        
        messages = [{"role": "user", "content": prompt}]
        result = await self.chat_completion(messages, temperature=0.3)
        
        if result["success"]:
            try:
                content = result["content"]
                import re
                json_match = re.search(r'\{.*\}', content, re.DOTALL)
                if json_match:
                    classification_data = json.loads(json_match.group())
                    return {
                        "success": True,
                        "classification": classification_data,
                        "classified_by": "Exaone Deep 7.8B"
                    }
            except Exception as e:
                logger.warning(f"ë¶„ë¥˜ ê²°ê³¼ íŒŒì‹± ì‹¤íŒ¨: {e}")
        
        return {
            "success": False,
            "error": "ì½˜í…ì¸  ë¶„ë¥˜ ì‹¤íŒ¨"
        }

    # === í•™ìŠµ ë° ì§€ì‹ í…ŒìŠ¤íŠ¸ ê¸°ëŠ¥ ===
    
    async def learn_from_content(
        self,
        content: str,
        metadata: Dict[str, Any] = None
    ) -> Dict[str, Any]:
        """ì½˜í…ì¸  í•™ìŠµ"""
        # ì‹¤ì œë¡œëŠ” ë²¡í„° DBì— ì €ì¥í•˜ê±°ë‚˜ íŒŒì¸íŠœë‹ ë°ì´í„°ë¡œ í™œìš©
        try:
            # ì½˜í…ì¸  ìš”ì•½ ìƒì„±
            summary_prompt = f"""
ë‹¤ìŒ êµìœ¡ ì½˜í…ì¸ ì˜ í•µì‹¬ ë‚´ìš©ì„ ìš”ì•½í•´ì£¼ì„¸ìš”:

{content[:2000]}...

3-5ì¤„ë¡œ ìš”ì•½í•´ì£¼ì„¸ìš”.
"""
            
            messages = [{"role": "user", "content": summary_prompt}]
            result = await self.chat_completion(messages, temperature=0.3)
            
            if result["success"]:
                # í•™ìŠµ ê¸°ë¡ ì €ì¥ (ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” DBë‚˜ íŒŒì¼ë¡œ)
                learning_record = {
                    "content_id": str(uuid.uuid4()),
                    "summary": result["content"],
                    "original_length": len(content),
                    "learned_at": datetime.now().isoformat(),
                    "metadata": metadata or {}
                }
                
                return {
                    "success": True,
                    "learning_record": learning_record,
                    "message": "ì½˜í…ì¸  í•™ìŠµ ì™„ë£Œ"
                }
        
        except Exception as e:
            logger.error(f"ì½˜í…ì¸  í•™ìŠµ ì‹¤íŒ¨: {e}")
        
        return {
            "success": False,
            "error": "ì½˜í…ì¸  í•™ìŠµ ì‹¤íŒ¨"
        }

    async def test_knowledge(
        self,
        test_question: str,
        context: str = ""
    ) -> Dict[str, Any]:
        """í•™ìŠµëœ ì§€ì‹ í…ŒìŠ¤íŠ¸"""
        prompt = f"""
ë‹¹ì‹ ì€ êµìœ¡ ì „ë¬¸ AIì…ë‹ˆë‹¤.

{f"ì°¸ê³  ë§¥ë½: {context}" if context else ""}

ì§ˆë¬¸: {test_question}

í•™ìŠµí•œ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ì •í™•í•˜ê³  êµìœ¡ì ì¸ ë‹µë³€ì„ ì œê³µí•´ì£¼ì„¸ìš”.
"""
        
        messages = [{"role": "user", "content": prompt}]
        result = await self.chat_completion(messages, temperature=0.4)
        
        if result["success"]:
            return {
                "success": True,
                "answer": result["content"],
                "test_question": test_question,
                "answered_by": "Exaone Deep 7.8B"
            }
        
        return {
            "success": False,
            "error": "ì§€ì‹ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨"
        }

    # === ìœ í‹¸ë¦¬í‹° ë©”ì„œë“œ ===
    
    def _update_average_response_time(self, response_time: float):
        """í‰ê·  ì‘ë‹µ ì‹œê°„ ì—…ë°ì´íŠ¸"""
        total_requests = self.performance_stats["successful_requests"]
        current_avg = self.performance_stats["average_response_time"]
        
        # ì´ë™ í‰ê·  ê³„ì‚°
        new_avg = ((current_avg * (total_requests - 1)) + response_time) / total_requests
        self.performance_stats["average_response_time"] = new_avg

    async def get_performance_stats(self) -> Dict[str, Any]:
        """ì„±ëŠ¥ í†µê³„ ì¡°íšŒ"""
        return {
            **self.performance_stats,
            "cache_size": len(self.conversation_cache),
            "model_info": {
                "model_name": self.model_name,
                "embedding_model": self.embedding_model,
                "ollama_host": self.ollama_host
            },
            "timestamp": datetime.now().isoformat()
        }

    async def clear_cache(self):
        """ìºì‹œ ì •ë¦¬"""
        self.conversation_cache.clear()
        logger.info("ğŸ’¾ Exaone ì„œë¹„ìŠ¤ ìºì‹œ ì •ë¦¬ ì™„ë£Œ")

    async def health_check(self) -> Dict[str, Any]:
        """ì„œë¹„ìŠ¤ ìƒíƒœ í™•ì¸"""
        try:
            # ê°„ë‹¨í•œ í…ŒìŠ¤íŠ¸ ìš”ì²­
            test_messages = [{"role": "user", "content": "ì•ˆë…•í•˜ì„¸ìš”"}]
            result = await self.chat_completion(test_messages, temperature=0.1)
            
            return {
                "status": "healthy" if result["success"] else "unhealthy",
                "model_available": await self.check_model_availability(),
                "response_test": result["success"],
                "performance_stats": await self.get_performance_stats(),
                "timestamp": datetime.now().isoformat()
            }
        except Exception as e:
            return {
                "status": "error",
                "error": str(e),
                "timestamp": datetime.now().isoformat()
            }

# ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤
exaone_service = LocalExaoneService() 