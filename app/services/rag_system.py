"""
RAG (Retrieval-Augmented Generation) ì‹œìŠ¤í…œ ì„œë¹„ìŠ¤
DeepSeek + Qdrant ê¸°ë°˜ìœ¼ë¡œ ì™„ì „ ì „í™˜
"""
import os
import json
import logging
from typing import List, Dict, Any, Optional, Tuple
from datetime import datetime
from pathlib import Path
import uuid
import asyncio

# PDF ì²˜ë¦¬ë¥¼ ìœ„í•œ ì¡°ê±´ë¶€ ì„í¬íŠ¸
try:
    import PyPDF2
    PYPDF2_AVAILABLE = True
except ImportError:
    PYPDF2_AVAILABLE = False
from sqlalchemy.orm import Session
from sqlalchemy import text, func

from ..models.question import Question
from ..core.config import settings
from ..db.database import engine
from .deepseek_service import deepseek_service
from .qdrant_service import qdrant_service

logger = logging.getLogger(__name__)

class RAGService:
    """RAG ì‹œìŠ¤í…œ ì„œë¹„ìŠ¤ - DeepSeek + Qdrant ê¸°ë°˜"""
    
    def __init__(self):
        self.upload_dir = Path("uploads/rag_documents")
        self.upload_dir.mkdir(parents=True, exist_ok=True)
        
        # DeepSeekê³¼ Qdrant ì„œë¹„ìŠ¤ ì‚¬ìš©
        self.deepseek = deepseek_service
        self.vector_db = qdrant_service
        
        logger.info("âœ… RAG ì„œë¹„ìŠ¤ ì´ˆê¸°í™” ì™„ë£Œ (DeepSeek + Qdrant)")
    
    def extract_text_from_pdf(self, pdf_path: str) -> str:
        """PDFì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ"""
        try:
            if not PYPDF2_AVAILABLE:
                logger.warning("PyPDF2ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•„ PDF ì²˜ë¦¬ë¥¼ ê±´ë„ˆëœë‹ˆë‹¤")
                return "PDF íŒŒì¼ ì²˜ë¦¬ë¥¼ ìœ„í•´ PyPDF2 ì„¤ì¹˜ê°€ í•„ìš”í•©ë‹ˆë‹¤"
                
            text_content = ""
            with open(pdf_path, 'rb') as file:
                pdf_reader = PyPDF2.PdfReader(file)
                for page in pdf_reader.pages:
                    text_content += page.extract_text() + "\n"
            
            logger.info(f"ğŸ“„ PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ ì™„ë£Œ: {len(text_content)} ë¬¸ì")
            return text_content
            
        except Exception as e:
            logger.error(f"âŒ PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return ""
    
    def chunk_text(self, text: str, chunk_size: int = 1000, overlap: int = 200) -> List[str]:
        """í…ìŠ¤íŠ¸ë¥¼ ì²­í¬ë¡œ ë¶„í• """
        if not text:
            return []
        
        chunks = []
        start = 0
        text_length = len(text)
        
        while start < text_length:
            end = start + chunk_size
            if end > text_length:
                end = text_length
            
            chunk = text[start:end].strip()
            if chunk:
                chunks.append(chunk)
            
            start = end - overlap
            if start >= text_length:
                break
        
        logger.info(f"ğŸ“Š í…ìŠ¤íŠ¸ ì²­í‚¹ ì™„ë£Œ: {len(chunks)}ê°œ ì²­í¬")
        return chunks
    
    async def create_embeddings(self, texts: List[str]) -> List[List[float]]:
        """DeepSeekìœ¼ë¡œ ì„ë² ë”© ìƒì„±"""
        try:
            result = await self.deepseek.create_embeddings(texts)
            
            if result["success"]:
                logger.info(f"ğŸ§  DeepSeek ì„ë² ë”© ìƒì„± ì™„ë£Œ: {len(result['embeddings'])}ê°œ")
                return result["embeddings"]
            else:
                logger.error(f"âŒ ì„ë² ë”© ìƒì„± ì‹¤íŒ¨: {result.get('error', 'Unknown error')}")
                return []
                
        except Exception as e:
            logger.error(f"âŒ ì„ë² ë”© ìƒì„± ì¤‘ ì˜¤ë¥˜: {e}")
            return []
    
    async def store_document_embeddings(
        self, 
        db: Session, 
        document_title: str,
        document_path: str,
        text_chunks: List[str],
        user_id: int
    ) -> int:
        """ë¬¸ì„œ ì„ë² ë”©ì„ Qdrant + PostgreSQLì— ì €ì¥"""
        try:
            stored_count = 0
            
            # ë©”íƒ€ë°ì´í„° ì¤€ë¹„
            metadatas = []
            for i, chunk in enumerate(text_chunks):
                metadata = {
                    "document_title": document_title,
                    "document_path": document_path,
                    "chunk_index": i,
                    "user_id": user_id,
                    "type": "rag_document",
                    "subject": f"RAG-{document_title}",
                    "area_name": "RAG Knowledge Base"
                }
                metadatas.append(metadata)
            
            # Qdrantì— ë²¡í„° ì €ì¥
            vector_result = await self.vector_db.add_vectors(
                texts=text_chunks,
                metadatas=metadatas
            )
            
            if not vector_result["success"]:
                logger.error(f"âŒ Qdrant ë²¡í„° ì €ì¥ ì‹¤íŒ¨: {vector_result.get('error')}")
                return 0
            
            # PostgreSQLì— ë©”íƒ€ë°ì´í„° ì €ì¥
            for i, chunk in enumerate(text_chunks):
                question = Question(
                    question_number=i + 1,
                    question_type="rag_document",
                    content=chunk,
                    subject=f"RAG-{document_title}",
                    area_name="RAG Knowledge Base",
                    difficulty="ì¤‘",
                    approval_status="approved",
                    source_file_path=document_path,
                    file_title=document_title,
                    file_category="RAG_DOCUMENT",
                    is_active=True,
                    last_modified_by=user_id,
                    last_modified_at=datetime.now(),
                    approved_by=user_id,
                    approved_at=datetime.now()
                )
                
                db.add(question)
                stored_count += 1
            
            db.commit()
            logger.info(f"âœ… RAG ë¬¸ì„œ ì €ì¥ ì™„ë£Œ: {stored_count}ê°œ ì²­í¬ (Qdrant + PostgreSQL)")
            return stored_count
            
        except Exception as e:
            logger.error(f"âŒ RAG ë¬¸ì„œ ì €ì¥ ì‹¤íŒ¨: {e}")
            db.rollback()
            return 0
    
    async def upload_and_process_document(
        self, 
        db: Session,
        file_path: str,
        document_title: str,
        user_id: int,
        chunk_size: int = 1000,
        overlap: int = 200
    ) -> Dict[str, Any]:
        """ë¬¸ì„œ ì—…ë¡œë“œ ë° RAG ì²˜ë¦¬ - ì™„ì „ ë¹„ë™ê¸°"""
        try:
            logger.info(f"ğŸš€ RAG ë¬¸ì„œ ì²˜ë¦¬ ì‹œì‘: {document_title}")
            
            # 1. PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ
            text_content = self.extract_text_from_pdf(file_path)
            if not text_content:
                return {"success": False, "message": "PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹¤íŒ¨"}
            
            # 2. í…ìŠ¤íŠ¸ ì²­í‚¹
            text_chunks = self.chunk_text(text_content, chunk_size, overlap)
            if not text_chunks:
                return {"success": False, "message": "í…ìŠ¤íŠ¸ ì²­í‚¹ ì‹¤íŒ¨"}
            
            # 3. ì„ë² ë”© ìƒì„± ë° ì €ì¥
            stored_count = await self.store_document_embeddings(
                db, document_title, file_path, text_chunks, user_id
            )
            
            if stored_count == 0:
                return {"success": False, "message": "ë¬¸ì„œ ì €ì¥ ì‹¤íŒ¨"}
            
            return {
                "success": True,
                "message": f"ğŸ‰ RAG ë¬¸ì„œ ì²˜ë¦¬ ì™„ë£Œ",
                "document_title": document_title,
                "chunks_count": len(text_chunks),
                "stored_count": stored_count,
                "total_characters": len(text_content)
            }
            
        except Exception as e:
            logger.error(f"âŒ RAG ë¬¸ì„œ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            return {"success": False, "message": f"ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"}
    
    async def similarity_search(
        self, 
        db: Session,
        query_text: str,
        limit: int = 5,
        similarity_threshold: float = 0.7,
        document_title: Optional[str] = None
    ) -> List[Dict[str, Any]]:
        """Qdrant ê¸°ë°˜ ìœ ì‚¬ë„ ê²€ìƒ‰"""
        try:
            # í•„í„° ì¡°ê±´ ì„¤ì •
            filter_conditions = {"type": "rag_document"}
            if document_title:
                filter_conditions["document_title"] = document_title
            
            # Qdrantì—ì„œ ìœ ì‚¬ ë¬¸ì„œ ê²€ìƒ‰
            search_result = await self.vector_db.search_vectors(
                query_text=query_text,
                limit=limit,
                score_threshold=similarity_threshold,
                filter_conditions=filter_conditions
            )
            
            if not search_result["success"]:
                logger.error(f"âŒ ë²¡í„° ê²€ìƒ‰ ì‹¤íŒ¨: {search_result.get('error')}")
                return []
            
            results = []
            for item in search_result["results"]:
                result = {
                    "content": item["text"],
                    "similarity": item["score"],
                    "document_title": item["metadata"].get("document_title", ""),
                    "chunk_index": item["metadata"].get("chunk_index", 0),
                    "subject": item["metadata"].get("subject", ""),
                    "area_name": item["metadata"].get("area_name", "")
                }
                results.append(result)
            
            logger.info(f"ğŸ” ìœ ì‚¬ë„ ê²€ìƒ‰ ì™„ë£Œ: {len(results)}ê°œ ê²°ê³¼")
            return results
            
        except Exception as e:
            logger.error(f"âŒ ìœ ì‚¬ë„ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            return []
    
    async def generate_question_with_rag(
        self,
        db: Session,
        topic: str,
        difficulty: str = "ì¤‘",
        question_type: str = "multiple_choice",
        context_limit: int = 3,
        department: str = "ê°„í˜¸í•™ê³¼"
    ) -> Dict[str, Any]:
        """RAG ê¸°ë°˜ ë¬¸ì œ ìƒì„± - DeepSeek ì‚¬ìš©"""
        try:
            logger.info(f"ğŸ¯ RAG ë¬¸ì œ ìƒì„± ì‹œì‘: {topic} ({difficulty})")
            
            # 1. ê´€ë ¨ ì»¨í…ìŠ¤íŠ¸ ê²€ìƒ‰
            contexts = await self.similarity_search(
                db=db,
                query_text=topic,
                limit=context_limit,
                similarity_threshold=0.6
            )
            
            if not contexts:
                logger.warning("âš ï¸ ê´€ë ¨ ì»¨í…ìŠ¤íŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ")
                return {"success": False, "message": "ê´€ë ¨ í•™ìŠµ ìë£Œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."}
            
            # 2. ì»¨í…ìŠ¤íŠ¸ í†µí•©
            context_text = "\n\n".join([ctx["content"] for ctx in contexts])
            
            # 3. DeepSeekìœ¼ë¡œ ë¬¸ì œ ìƒì„±
            prompt = f"""
ë‹¤ìŒì€ {department} í•™ìŠµ ìë£Œì…ë‹ˆë‹¤:

{context_text}

ìœ„ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ë‹¤ìŒ ì¡°ê±´ì— ë§ëŠ” ë¬¸ì œë¥¼ ìƒì„±í•´ì£¼ì„¸ìš”:
- ì£¼ì œ: {topic}
- ë‚œì´ë„: {difficulty}
- ë¬¸ì œ ìœ í˜•: {question_type}
- ëŒ€ìƒ: {department} í•™ìƒ

ë¬¸ì œëŠ” ë‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œ ì‘ì„±í•´ì£¼ì„¸ìš”:
{{
    "question": "ë¬¸ì œ ë‚´ìš©",
    "options": ["ì„ íƒì§€1", "ì„ íƒì§€2", "ì„ íƒì§€3", "ì„ íƒì§€4"],
    "correct_answer": 1,
    "explanation": "ì •ë‹µ í•´ì„¤",
    "difficulty": "{difficulty}",
    "subject": "{topic}",
    "source_contexts": ["ì‚¬ìš©ëœ ì»¨í…ìŠ¤íŠ¸ ìš”ì•½"]
}}
"""
            
            generation_result = await self.deepseek.chat_completion(
                messages=[{"role": "user", "content": prompt}],
                temperature=0.7
            )
            
            if not generation_result["success"]:
                logger.error(f"âŒ ë¬¸ì œ ìƒì„± ì‹¤íŒ¨: {generation_result.get('error')}")
                return {"success": False, "message": "ë¬¸ì œ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ"}
            
            # 4. ê²°ê³¼ íŒŒì‹±
            try:
                question_data = json.loads(generation_result["content"])
                
                # ì»¨í…ìŠ¤íŠ¸ ì •ë³´ ì¶”ê°€
                question_data["rag_contexts"] = [
                    {
                        "content": ctx["content"][:200] + "...",
                        "similarity": ctx["similarity"],
                        "source": ctx["document_title"]
                    }
                    for ctx in contexts
                ]
                
                logger.info(f"âœ… RAG ë¬¸ì œ ìƒì„± ì™„ë£Œ: {question_data.get('subject', topic)}")
                
                return {
                    "success": True,
                    "question_data": question_data,
                    "contexts_used": len(contexts),
                    "generation_method": "DeepSeek + Qdrant RAG"
                }
                
            except json.JSONDecodeError as e:
                logger.error(f"âŒ JSON íŒŒì‹± ì‹¤íŒ¨: {e}")
                return {"success": False, "message": "ìƒì„±ëœ ë¬¸ì œ í˜•ì‹ ì˜¤ë¥˜"}
            
        except Exception as e:
            logger.error(f"âŒ RAG ë¬¸ì œ ìƒì„± ì‹¤íŒ¨: {e}")
            return {"success": False, "message": f"ë¬¸ì œ ìƒì„± ì¤‘ ì˜¤ë¥˜: {str(e)}"}
    
    async def get_rag_statistics(self, db: Session) -> Dict[str, Any]:
        """RAG ì‹œìŠ¤í…œ í†µê³„ ì¡°íšŒ"""
        try:
            # PostgreSQL í†µê³„
            total_docs = db.query(func.count(Question.id)).filter(
                Question.question_type == "rag_document"
            ).scalar()
            
            unique_docs = db.query(func.count(func.distinct(Question.file_title))).filter(
                Question.question_type == "rag_document"
            ).scalar()
            
            # Qdrant í†µê³„
            qdrant_info = self.vector_db.get_collection_info()
            
            stats = {
                "total_chunks": total_docs,
                "unique_documents": unique_docs,
                "vector_db_status": qdrant_info.get("success", False),
                "vector_count": qdrant_info.get("points_count", 0) if qdrant_info.get("success") else 0,
                "collection_name": qdrant_info.get("collection_name", ""),
                "last_updated": datetime.now().isoformat(),
                "system_type": "DeepSeek + Qdrant"
            }
            
            logger.info(f"ğŸ“Š RAG í†µê³„ ì¡°íšŒ ì™„ë£Œ: {stats}")
            return stats
            
        except Exception as e:
            logger.error(f"âŒ RAG í†µê³„ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return {
                "total_chunks": 0,
                "unique_documents": 0,
                "vector_db_status": False,
                "error": str(e)
            }
    
    async def delete_document(self, db: Session, document_title: str) -> Dict[str, Any]:
        """ë¬¸ì„œ ì‚­ì œ (PostgreSQL + Qdrant)"""
        try:
            # PostgreSQLì—ì„œ ì‚­ì œ
            deleted_count = db.query(Question).filter(
                Question.question_type == "rag_document",
                Question.file_title == document_title
            ).delete()
            
            db.commit()
            
            # Qdrantì—ì„œ ì‚­ì œ (ë¬¸ì„œë³„ ì‚­ì œëŠ” í•„í„° ê¸°ë°˜ìœ¼ë¡œ êµ¬í˜„ í•„ìš”)
            # í˜„ì¬ëŠ” ê°œë³„ ID ì‚­ì œë§Œ ì§€ì›í•˜ë¯€ë¡œ í–¥í›„ ê°œì„  í•„ìš”
            
            logger.info(f"ğŸ—‘ï¸ ë¬¸ì„œ ì‚­ì œ ì™„ë£Œ: {document_title} ({deleted_count}ê°œ ì²­í¬)")
            
            return {
                "success": True,
                "message": f"ë¬¸ì„œ ì‚­ì œ ì™„ë£Œ: {document_title}",
                "deleted_chunks": deleted_count
            }
            
        except Exception as e:
            logger.error(f"âŒ ë¬¸ì„œ ì‚­ì œ ì‹¤íŒ¨: {e}")
            db.rollback()
            return {"success": False, "message": f"ì‚­ì œ ì¤‘ ì˜¤ë¥˜: {str(e)}"}
    
    async def add_document(self, doc_id: str, content: str, metadata: Dict[str, Any] = None) -> Dict[str, Any]:
        """ë¬¸ì„œ ì¶”ê°€ (í…ŒìŠ¤íŠ¸ìš© ê°„ë‹¨ ë²„ì „)"""
        try:
            if metadata is None:
                metadata = {}
            
            # ë©”íƒ€ë°ì´í„° ì„¤ì •
            doc_metadata = {
                "document_id": doc_id,
                "type": "test_document",
                **metadata
            }
            
            # ë²¡í„° ì¶”ê°€
            result = await self.vector_db.add_vectors(
                texts=[content],
                metadatas=[doc_metadata]
            )
            
            if result["success"]:
                logger.info(f"âœ… í…ŒìŠ¤íŠ¸ ë¬¸ì„œ ì¶”ê°€ ì™„ë£Œ: {doc_id}")
                return {"success": True, "message": "ë¬¸ì„œ ì¶”ê°€ ì™„ë£Œ"}
            else:
                return {"success": False, "error": result.get("error", "Unknown")}
                
        except Exception as e:
            logger.error(f"âŒ ë¬¸ì„œ ì¶”ê°€ ì‹¤íŒ¨: {e}")
            return {"success": False, "error": str(e)}
    
    async def generate_answer(self, query: str, department: str = "ê°„í˜¸í•™ê³¼") -> str:
        """RAG ê¸°ë°˜ ë‹µë³€ ìƒì„± (í…ŒìŠ¤íŠ¸ìš© ê°„ë‹¨ ë²„ì „)"""
        try:
            # ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰
            search_result = await self.vector_db.search_vectors(
                query_text=query,
                limit=3,
                score_threshold=0.5
            )
            
            if not search_result["success"] or not search_result["results"]:
                return "ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
            
            # ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
            contexts = []
            for result in search_result["results"]:
                contexts.append(result["text"])
            
            context_text = "\n\n".join(contexts)
            
            # DeepSeekìœ¼ë¡œ ë‹µë³€ ìƒì„±
            prompt = f"""
ë‹¤ìŒ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”.

ì»¨í…ìŠ¤íŠ¸:
{context_text}

ì§ˆë¬¸: {query}

{department} í•™ìƒì—ê²Œ ì í•©í•œ ë‹µë³€ì„ ì œê³µí•´ì£¼ì„¸ìš”.
"""
            
            messages = [{"role": "user", "content": prompt}]
            result = await self.deepseek.chat_completion(messages, temperature=0.3)
            
            if result["success"]:
                return result["content"]
            else:
                return "ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."
                
        except Exception as e:
            logger.error(f"âŒ RAG ë‹µë³€ ìƒì„± ì‹¤íŒ¨: {e}")
            return "ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."

# ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤
rag_service = RAGService()
rag_system = rag_service  # í•˜ìœ„ í˜¸í™˜ì„±ì„ ìœ„í•œ ë³„ì¹­ 