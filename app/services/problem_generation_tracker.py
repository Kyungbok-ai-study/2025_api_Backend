"""
ë¬¸ì œ ìƒì„± ì´ë ¥ ì¶”ì  ë° ì¤‘ë³µ ë°©ì§€ ì„œë¹„ìŠ¤
- ìƒì„±ëœ ë¬¸ì œ ì´ë ¥ ê´€ë¦¬
- ì§€ì‹ë² ì´ìŠ¤ ì‚¬ìš© ì¶”ì 
- ë™ì  í‚¤ì›Œë“œ í™•ì¥
- ì¤‘ë³µ ë°©ì§€ ì•Œê³ ë¦¬ì¦˜
"""
import json
import logging
import hashlib
from typing import Dict, Any, List, Optional, Set, Tuple
from datetime import datetime, timedelta
from pathlib import Path
from dataclasses import dataclass
from collections import defaultdict

from sqlalchemy.orm import Session
from sqlalchemy import text, and_, func, desc

from ..models.question import Question
from ..models.user import User

logger = logging.getLogger(__name__)

@dataclass
class GenerationRecord:
    """ë¬¸ì œ ìƒì„± ê¸°ë¡"""
    user_id: int
    subject: str
    keywords: List[str]
    question_type: str
    difficulty: str
    used_knowledge_chunks: List[str]
    generated_concepts: List[str]
    timestamp: datetime
    session_id: str

@dataclass
class KnowledgeChunk:
    """ì§€ì‹ë² ì´ìŠ¤ ì²­í¬ ì •ë³´"""
    id: str
    content: str
    subject: str
    keywords: List[str]
    usage_count: int
    last_used: Optional[datetime]

class ProblemGenerationTracker:
    """ë¬¸ì œ ìƒì„± ì¶”ì  ë° ì¤‘ë³µ ë°©ì§€ ì„œë¹„ìŠ¤"""
    
    def __init__(self):
        self.generation_history_path = Path("data/generation_history")
        self.generation_history_path.mkdir(parents=True, exist_ok=True)
        
        # í‚¤ì›Œë“œ í™•ì¥ì„ ìœ„í•œ ê´€ë ¨ì–´ ë§µí•‘
        self.concept_relations = {
            "ê°„í˜¸í•™ê³¼": {
                "í™˜ìì•ˆì „": ["ë‚™ìƒë°©ì§€", "ê°ì—¼ê´€ë¦¬", "íˆ¬ì•½ì•ˆì „", "í™˜ìí™•ì¸", "ì˜ë£Œê¸°ê¸°ì•ˆì „"],
                "ê°ì—¼ê´€ë¦¬": ["ë¬´ê· ìˆ ", "ê²©ë¦¬", "ì†ìœ„ìƒ", "ê°œì¸ë³´í˜¸êµ¬", "í™˜ê²½ê´€ë¦¬"],
                "íˆ¬ì•½ê´€ë¦¬": ["5Rì›ì¹™", "ì•½ë¬¼ìƒí˜¸ì‘ìš©", "ë¶€ì‘ìš©ê´€ë¦¬", "íˆ¬ì•½ê²½ë¡œ", "ì•½ë¬¼ê³„ì‚°"],
                "í™œë ¥ì§•í›„": ["ì²´ì˜¨", "ë§¥ë°•", "í˜¸í¡", "í˜ˆì••", "ì‚°ì†Œí¬í™”ë„"],
                "ê°„í˜¸ì§„ë‹¨": ["NANDA", "ê°„í˜¸ê³¼ì •", "ì‚¬ì •", "ê³„íš", "í‰ê°€"],
            },
            "ë¬¼ë¦¬ì¹˜ë£Œí•™ê³¼": {
                "ê·¼ê³¨ê²©ê³„": ["ê´€ì ˆ", "ê·¼ìœ¡", "ì¸ëŒ€", "í˜ì¤„", "ë¼ˆ"],
                "ì‹ ê²½ê³„": ["ì¤‘ì¶”ì‹ ê²½", "ë§ì´ˆì‹ ê²½", "ë°˜ì‚¬", "ê°ê°", "ìš´ë™"],
                "ìš´ë™ì¹˜ë£Œ": ["ê´€ì ˆê°€ë™ë²”ìœ„", "ê·¼ë ¥ê°•í™”", "ì§€êµ¬ë ¥", "í˜‘ì‘ì„±", "ê· í˜•"],
                "ë„ìˆ˜ì¹˜ë£Œ": ["ê´€ì ˆê°€ë™ìˆ ", "ì—°ë¶€ì¡°ì§ê°€ë™ìˆ ", "ì‹ ê²½ê°€ë™ìˆ ", "ì²™ì¶”êµì •"],
                "ì „ê¸°ì¹˜ë£Œ": ["TENS", "FES", "ì´ˆìŒíŒŒ", "ì ì™¸ì„ ", "ë ˆì´ì €"],
            },
            "ì‘ì—…ì¹˜ë£Œí•™ê³¼": {
                "ì¼ìƒìƒí™œí™œë™": ["ADL", "IADL", "ìì¡°ê¸°ìˆ ", "ì´ë™", "ì˜ì‚¬ì†Œí†µ"],
                "ì¸ì§€ì¬í™œ": ["ì£¼ì˜ë ¥", "ê¸°ì–µë ¥", "ì‹¤í–‰ê¸°ëŠ¥", "ë¬¸ì œí•´ê²°", "í•™ìŠµ"],
                "ê°ê°í†µí•©": ["ì „ì •ê°ê°", "ê³ ìœ ê°ê°", "ì´‰ê°", "ì‹œê°", "ì²­ê°"],
                "ì§ì—…ì¬í™œ": ["ì§ë¬´ë¶„ì„", "ì‘ì—…ëŠ¥ë ¥í‰ê°€", "ì§ì—…ì ì‘", "ë³´ì¡°ê³µí•™"],
                "ë³´ì¡°ê¸°êµ¬": ["íœ ì²´ì–´", "ë³´í–‰ë³´ì¡°ê¸°êµ¬", "ì¼ìƒìƒí™œë³´ì¡°ê¸°êµ¬", "ì˜ì‚¬ì†Œí†µë³´ì¡°ê¸°êµ¬"],
            }
        }
    
    async def get_next_generation_strategy(
        self,
        db: Session,
        user_id: int,
        subject: str,
        difficulty: str,
        question_type: str,
        requested_keywords: Optional[str] = None,
        count: int = 5
    ) -> Dict[str, Any]:
        """ë‹¤ìŒ ë¬¸ì œ ìƒì„± ì „ëµ ê²°ì • (ì¤‘ë³µ ë°©ì§€)"""
        
        logger.info(f"ğŸ” ë¬¸ì œ ìƒì„± ì „ëµ ë¶„ì„ ì‹œì‘ - ì‚¬ìš©ì {user_id}")
        
        # 1. ì‚¬ìš©ìì˜ ì´ì „ ìƒì„± ì´ë ¥ ë¶„ì„
        generation_history = await self._get_user_generation_history(db, user_id)
        
        # 2. ì‚¬ìš©ëœ ì§€ì‹ë² ì´ìŠ¤ ì˜ì—­ ë¶„ì„
        used_knowledge_areas = await self._analyze_used_knowledge_areas(db, user_id)
        
        # 3. ë¯¸ì‚¬ìš© í‚¤ì›Œë“œ ë°œêµ´
        unused_keywords = await self._find_unused_keywords(
            user_id, subject, requested_keywords, generation_history
        )
        
        # 4. ì§€ì‹ë² ì´ìŠ¤ ì»¤ë²„ë¦¬ì§€ ë¶„ì„
        knowledge_coverage = await self._analyze_knowledge_coverage(db, user_id, subject)
        
        # 5. ìƒˆë¡œìš´ ìƒì„± ì „ëµ ìˆ˜ë¦½
        strategy = await self._create_generation_strategy(
            generation_history=generation_history,
            used_knowledge_areas=used_knowledge_areas,
            unused_keywords=unused_keywords,
            knowledge_coverage=knowledge_coverage,
            subject=subject,
            difficulty=difficulty,
            question_type=question_type,
            count=count
        )
        
        logger.info(f"âœ… ìƒì„± ì „ëµ ìˆ˜ë¦½ ì™„ë£Œ: {strategy['diversification_level']}% ë‹¤ì–‘ì„±")
        
        return strategy
    
    async def _get_user_generation_history(
        self, db: Session, user_id: int, days: int = 30
    ) -> List[GenerationRecord]:
        """ì‚¬ìš©ìì˜ ìµœê·¼ ìƒì„± ì´ë ¥ ì¡°íšŒ"""
        
        try:
            # ìµœê·¼ 30ì¼ê°„ ìƒì„±ëœ ë¬¸ì œë“¤ ì¡°íšŒ
            since_date = datetime.now() - timedelta(days=days)
            
            generated_questions = db.query(Question).filter(
                and_(
                    Question.last_modified_by == user_id,
                    Question.file_category == "ENHANCED_GENERATED",
                    Question.created_at >= since_date
                )
            ).all()
            
            # GenerationRecordë¡œ ë³€í™˜
            history = []
            for q in generated_questions:
                # ë©”íƒ€ë°ì´í„°ì—ì„œ ìƒì„± ì •ë³´ ì¶”ì¶œ
                source_path = q.source_file_path or ""
                keywords = [q.subject] if q.subject else []
                
                record = GenerationRecord(
                    user_id=user_id,
                    subject=q.subject or "",
                    keywords=keywords,
                    question_type=q.question_type or "multiple_choice",
                    difficulty=q.difficulty or "medium",
                    used_knowledge_chunks=[],  # ì¶”í›„ í™•ì¥
                    generated_concepts=keywords,
                    timestamp=q.created_at or datetime.now(),
                    session_id=f"session_{q.id}"
                )
                history.append(record)
            
            logger.info(f"ğŸ“Š ì‚¬ìš©ì {user_id}ì˜ ìµœê·¼ {days}ì¼ ìƒì„± ì´ë ¥: {len(history)}ê°œ")
            return history
            
        except Exception as e:
            logger.error(f"ìƒì„± ì´ë ¥ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return []
    
    async def _analyze_used_knowledge_areas(
        self, db: Session, user_id: int
    ) -> Dict[str, int]:
        """ì‚¬ìš©ëœ ì§€ì‹ë² ì´ìŠ¤ ì˜ì—­ ë¶„ì„"""
        
        try:
            # ì‚¬ìš©ìê°€ ìƒì„±í•œ ë¬¸ì œë“¤ì˜ ê³¼ëª©/ì˜ì—­ ë¶„ì„
            result = db.execute(text("""
                SELECT subject, area_name, COUNT(*) as usage_count
                FROM questions 
                WHERE last_modified_by = :user_id 
                    AND file_category = 'ENHANCED_GENERATED'
                    AND created_at >= :since_date
                GROUP BY subject, area_name
                ORDER BY usage_count DESC
            """), {
                "user_id": user_id,
                "since_date": datetime.now() - timedelta(days=30)
            }).fetchall()
            
            used_areas = {}
            for row in result:
                area_key = f"{row[0]}_{row[1]}" if row[1] else row[0]
                used_areas[area_key] = row[2]
            
            logger.info(f"ğŸ“ˆ ì‚¬ìš©ëœ ì§€ì‹ ì˜ì—­: {len(used_areas)}ê°œ")
            return used_areas
            
        except Exception as e:
            logger.error(f"ì§€ì‹ ì˜ì—­ ë¶„ì„ ì‹¤íŒ¨: {e}")
            return {}
    
    async def _find_unused_keywords(
        self,
        user_id: int,
        subject: str,
        requested_keywords: Optional[str],
        generation_history: List[GenerationRecord]
    ) -> List[str]:
        """ë¯¸ì‚¬ìš© í‚¤ì›Œë“œ ë°œêµ´"""
        
        # ì´ì „ì— ì‚¬ìš©ëœ í‚¤ì›Œë“œë“¤ ìˆ˜ì§‘
        used_keywords = set()
        for record in generation_history:
            used_keywords.update(record.keywords)
            used_keywords.update(record.generated_concepts)
        
        # ì‚¬ìš©ì ë¶€ì„œ ì •ë³´ë¡œ ê´€ë ¨ ê°œë… í™•ì¥
        user_dept = "ê°„í˜¸í•™ê³¼"  # ê¸°ë³¸ê°’ (ì‹¤ì œë¡œëŠ” DBì—ì„œ ì¡°íšŒ)
        available_concepts = self.concept_relations.get(user_dept, {})
        
        # ë¯¸ì‚¬ìš© í‚¤ì›Œë“œ ì°¾ê¸°
        unused_keywords = []
        
        if requested_keywords:
            # ìš”ì²­ëœ í‚¤ì›Œë“œì™€ ê´€ë ¨ëœ ë¯¸ì‚¬ìš© ê°œë… ì°¾ê¸°
            for concept, related in available_concepts.items():
                if requested_keywords.lower() in concept.lower():
                    for related_keyword in related:
                        if related_keyword not in used_keywords:
                            unused_keywords.append(related_keyword)
        
        # ì „ì²´ ê°œë…ì—ì„œ ë¯¸ì‚¬ìš© í‚¤ì›Œë“œ ì¶”ê°€
        for concept, related in available_concepts.items():
            if concept not in used_keywords:
                unused_keywords.append(concept)
            for related_keyword in related:
                if related_keyword not in used_keywords:
                    unused_keywords.append(related_keyword)
        
        # ì¤‘ë³µ ì œê±° ë° ìš°ì„ ìˆœìœ„ ì ìš©
        unused_keywords = list(set(unused_keywords))
        
        # ìš”ì²­ëœ í‚¤ì›Œë“œì™€ ê´€ë ¨ì„±ì´ ë†’ì€ ìˆœìœ¼ë¡œ ì •ë ¬
        if requested_keywords:
            unused_keywords.sort(key=lambda x: self._calculate_keyword_relevance(x, requested_keywords))
        
        logger.info(f"ğŸ” ë¯¸ì‚¬ìš© í‚¤ì›Œë“œ {len(unused_keywords)}ê°œ ë°œêµ´")
        return unused_keywords[:10]  # ìƒìœ„ 10ê°œë§Œ ë°˜í™˜
    
    async def _analyze_knowledge_coverage(
        self, db: Session, user_id: int, subject: str
    ) -> Dict[str, Any]:
        """ì§€ì‹ë² ì´ìŠ¤ ì»¤ë²„ë¦¬ì§€ ë¶„ì„"""
        
        try:
            # ì „ì²´ ì§€ì‹ë² ì´ìŠ¤ ë¬¸ì„œ ìˆ˜
            total_docs = db.execute(text("""
                SELECT COUNT(DISTINCT file_title) as total_count
                FROM questions 
                WHERE file_category = 'RAG_DOCUMENT' 
                    AND is_active = true
                    AND (subject LIKE :subject OR subject IS NULL)
            """), {"subject": f"%{subject}%"}).fetchone()
            
            # ì‚¬ìš©ëœ ì§€ì‹ë² ì´ìŠ¤ ë¬¸ì„œ ìˆ˜ (ê°„ì ‘ì ìœ¼ë¡œ ì¶”ì •)
            used_docs = db.execute(text("""
                SELECT COUNT(DISTINCT subject) as used_count
                FROM questions 
                WHERE last_modified_by = :user_id 
                    AND file_category = 'ENHANCED_GENERATED'
                    AND created_at >= :since_date
            """), {
                "user_id": user_id,
                "since_date": datetime.now() - timedelta(days=30)
            }).fetchone()
            
            total_count = total_docs[0] if total_docs else 0
            used_count = used_docs[0] if used_docs else 0
            
            coverage_rate = (used_count / total_count * 100) if total_count > 0 else 0
            
            coverage = {
                "total_documents": total_count,
                "used_documents": used_count,
                "coverage_rate": coverage_rate,
                "unused_rate": 100 - coverage_rate,
                "recommendation": "high_diversity" if coverage_rate < 30 else "moderate_diversity" if coverage_rate < 70 else "focus_depth"
            }
            
            logger.info(f"ğŸ“Š ì§€ì‹ë² ì´ìŠ¤ ì»¤ë²„ë¦¬ì§€: {coverage_rate:.1f}%")
            return coverage
            
        except Exception as e:
            logger.error(f"ì»¤ë²„ë¦¬ì§€ ë¶„ì„ ì‹¤íŒ¨: {e}")
            return {
                "total_documents": 0,
                "used_documents": 0,
                "coverage_rate": 0,
                "unused_rate": 100,
                "recommendation": "high_diversity"
            }
    
    async def _create_generation_strategy(
        self,
        generation_history: List[GenerationRecord],
        used_knowledge_areas: Dict[str, int],
        unused_keywords: List[str],
        knowledge_coverage: Dict[str, Any],
        subject: str,
        difficulty: str,
        question_type: str,
        count: int
    ) -> Dict[str, Any]:
        """ìƒˆë¡œìš´ ìƒì„± ì „ëµ ìˆ˜ë¦½"""
        
        # ë‹¤ì–‘ì„± ë ˆë²¨ ê²°ì •
        diversification_level = self._calculate_diversification_level(
            generation_history, knowledge_coverage
        )
        
        # í‚¤ì›Œë“œ ì „ëµ ìˆ˜ë¦½
        keyword_strategy = await self._create_keyword_strategy(
            unused_keywords, used_knowledge_areas, diversification_level
        )
        
        # ì§€ì‹ë² ì´ìŠ¤ í™œìš© ì „ëµ
        kb_strategy = self._create_knowledge_base_strategy(
            knowledge_coverage, diversification_level
        )
        
        # ë¬¸ì œ ìœ í˜• ë‹¤ì–‘í™” ì „ëµ
        type_strategy = self._create_type_diversification_strategy(
            generation_history, question_type, count
        )
        
        strategy = {
            "diversification_level": diversification_level,
            "target_keywords": keyword_strategy["primary_keywords"],
            "alternative_keywords": keyword_strategy["alternative_keywords"],
            "knowledge_base_focus": {
                "focus_areas": kb_strategy["focus_areas"],
                "kb_ratio_adjustment": kb_strategy["kb_ratio_adjustment"],
                "exploration_mode": kb_strategy["exploration_mode"]
            },
            "avoid_patterns": self._extract_avoid_patterns(generation_history),
            "type_distribution": type_strategy,
            "generation_guidance": {
                "prioritize_unused_knowledge": diversification_level > 70,
                "expand_keyword_scope": diversification_level > 50,
                "vary_question_approaches": diversification_level > 60,
                "explore_new_concepts": len(unused_keywords) > 5
            },
            "session_id": f"session_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        }
        
        return strategy
    
    def _calculate_diversification_level(
        self, generation_history: List[GenerationRecord], knowledge_coverage: Dict[str, Any]
    ) -> int:
        """ë‹¤ì–‘ì„± í•„ìš” ë ˆë²¨ ê³„ì‚° (0-100)"""
        
        base_level = 50  # ê¸°ë³¸ ë‹¤ì–‘ì„± ë ˆë²¨
        
        # ìµœê·¼ ìƒì„± ë¹ˆë„ì— ë”°ë¥¸ ì¡°ì •
        recent_generations = len([r for r in generation_history if 
                                (datetime.now() - r.timestamp).days <= 7])
        
        if recent_generations > 10:
            base_level += 30  # ë§ì´ ìƒì„±í–ˆìœ¼ë©´ ë” ë‹¤ì–‘í•˜ê²Œ
        elif recent_generations > 5:
            base_level += 15
        
        # ì§€ì‹ë² ì´ìŠ¤ ì»¤ë²„ë¦¬ì§€ì— ë”°ë¥¸ ì¡°ì •
        if knowledge_coverage["coverage_rate"] > 70:
            base_level += 20  # ë§ì´ ì‚¬ìš©í–ˆìœ¼ë©´ ë” ë‹¤ì–‘í•˜ê²Œ
        elif knowledge_coverage["coverage_rate"] < 30:
            base_level -= 10  # ì•„ì§ ì—¬ìœ  ìˆìŒ
        
        # í‚¤ì›Œë“œ ë°˜ë³µ ì‚¬ìš© íŒ¨í„´ ë¶„ì„
        keyword_usage = defaultdict(int)
        for record in generation_history:
            for keyword in record.keywords:
                keyword_usage[keyword] += 1
        
        if keyword_usage and max(keyword_usage.values()) > 3:
            base_level += 25  # ê°™ì€ í‚¤ì›Œë“œ ë°˜ë³µ ì‚¬ìš© ì‹œ ë‹¤ì–‘ì„± ì¦ê°€
        
        return min(100, max(0, base_level))
    
    async def _create_keyword_strategy(
        self, unused_keywords: List[str], used_knowledge_areas: Dict[str, int], 
        diversification_level: int
    ) -> Dict[str, Any]:
        """í‚¤ì›Œë“œ ì „ëµ ìˆ˜ë¦½"""
        
        if diversification_level > 70:
            # ë†’ì€ ë‹¤ì–‘ì„±: ì™„ì „íˆ ìƒˆë¡œìš´ í‚¤ì›Œë“œ ìš°ì„ 
            primary_keywords = unused_keywords[:3]
            alternative_keywords = unused_keywords[3:6]
        elif diversification_level > 40:
            # ì¤‘ê°„ ë‹¤ì–‘ì„±: ì‚¬ìš©ë¹ˆë„ ë‚®ì€ í‚¤ì›Œë“œ + ìƒˆ í‚¤ì›Œë“œ ì¡°í•©
            low_usage_areas = [area for area, count in used_knowledge_areas.items() if count <= 2]
            primary_keywords = unused_keywords[:2] + low_usage_areas[:1]
            alternative_keywords = unused_keywords[2:5]
        else:
            # ë‚®ì€ ë‹¤ì–‘ì„±: ê¸°ì¡´ í‚¤ì›Œë“œ ì¤‘ì‹¬ìœ¼ë¡œ ì•½ê°„ì˜ ë³€í™”
            primary_keywords = unused_keywords[:1]
            alternative_keywords = unused_keywords[1:4]
        
        return {
            "primary_keywords": primary_keywords,
            "alternative_keywords": alternative_keywords,
            "strategy": "high_diversity" if diversification_level > 70 else "moderate_diversity"
        }
    
    def _create_knowledge_base_strategy(
        self, knowledge_coverage: Dict[str, Any], diversification_level: int
    ) -> Dict[str, Any]:
        """ì§€ì‹ë² ì´ìŠ¤ í™œìš© ì „ëµ"""
        
        if knowledge_coverage["recommendation"] == "high_diversity":
            focus_areas = ["unexplored_documents", "low_usage_chunks", "cross_domain_knowledge"]
        elif knowledge_coverage["recommendation"] == "moderate_diversity":
            focus_areas = ["balanced_coverage", "related_concepts", "depth_expansion"]
        else:
            focus_areas = ["depth_focus", "advanced_concepts", "specialized_knowledge"]
        
        return {
            "focus_areas": focus_areas,
            "kb_ratio_adjustment": 0.8 if diversification_level > 70 else 0.7,  # ë‹¤ì–‘ì„± ë†’ì„ ë•Œ ì§€ì‹ë² ì´ìŠ¤ ë¹„ì¤‘ ì¦ê°€
            "exploration_mode": diversification_level > 60
        }
    
    def _create_type_diversification_strategy(
        self, generation_history: List[GenerationRecord], 
        requested_type: str, count: int
    ) -> Dict[str, int]:
        """ë¬¸ì œ ìœ í˜• ë‹¤ì–‘í™” ì „ëµ"""
        
        # ìµœê·¼ ì‚¬ìš©ëœ ë¬¸ì œ ìœ í˜• ë¶„ì„
        recent_types = defaultdict(int)
        for record in generation_history[-10:]:  # ìµœê·¼ 10ê°œë§Œ
            recent_types[record.question_type] += 1
        
        # ìš”ì²­ëœ ìœ í˜• ê¸°ë°˜ìœ¼ë¡œ ë¶„ë°° ì¡°ì •
        distribution = {requested_type: count}
        
        # ë‹¤ì–‘ì„±ì´ í•„ìš”í•œ ê²½ìš° ìœ í˜• ë¶„ì‚°
        if len(recent_types) > 0 and recent_types[requested_type] > 3:
            # ê°™ì€ ìœ í˜•ì„ ë§ì´ ì‚¬ìš©í–ˆìœ¼ë©´ ë¶„ì‚°
            alternative_types = ["multiple_choice", "short_answer", "essay", "true_false"]
            alternative_types.remove(requested_type)
            
            main_count = max(1, count // 2)
            alt_count = count - main_count
            
            distribution = {
                requested_type: main_count,
                alternative_types[0]: alt_count
            }
        
        return distribution
    
    def _extract_avoid_patterns(self, generation_history: List[GenerationRecord]) -> List[str]:
        """í”¼í•´ì•¼ í•  íŒ¨í„´ ì¶”ì¶œ"""
        
        patterns = []
        
        # ìì£¼ ë°˜ë³µë˜ëŠ” í‚¤ì›Œë“œ íŒ¨í„´
        keyword_frequency = defaultdict(int)
        for record in generation_history:
            for keyword in record.keywords:
                keyword_frequency[keyword] += 1
        
        # 3íšŒ ì´ìƒ ì‚¬ìš©ëœ í‚¤ì›Œë“œëŠ” í”¼í•˜ê¸° ëª©ë¡ì— ì¶”ê°€
        overused_keywords = [keyword for keyword, count in keyword_frequency.items() if count >= 3]
        patterns.extend([f"overused_keyword:{keyword}" for keyword in overused_keywords])
        
        # ì—°ì†ìœ¼ë¡œ ê°™ì€ ë‚œì´ë„ ì‚¬ìš© íŒ¨í„´
        recent_difficulties = [r.difficulty for r in generation_history[-5:]]
        if len(set(recent_difficulties)) == 1 and len(recent_difficulties) >= 3:
            patterns.append(f"repeated_difficulty:{recent_difficulties[0]}")
        
        # ê°™ì€ ì£¼ì œ ë°˜ë³µ íŒ¨í„´
        recent_subjects = [r.subject for r in generation_history[-5:]]
        if len(set(recent_subjects)) == 1 and len(recent_subjects) >= 3:
            patterns.append(f"repeated_subject:{recent_subjects[0]}")
        
        return patterns
    
    def _calculate_keyword_relevance(self, keyword: str, target: str) -> float:
        """í‚¤ì›Œë“œ ê´€ë ¨ì„± ê³„ì‚° (ë†’ì„ìˆ˜ë¡ ê´€ë ¨ì„± ë†’ìŒ)"""
        
        # ë‹¨ìˆœí•œ ë¬¸ìì—´ ìœ ì‚¬ë„ ê¸°ë°˜ (ì‹¤ì œë¡œëŠ” ë” ì •êµí•œ ì•Œê³ ë¦¬ì¦˜ ì‚¬ìš© ê°€ëŠ¥)
        target_lower = target.lower()
        keyword_lower = keyword.lower()
        
        if target_lower in keyword_lower or keyword_lower in target_lower:
            return 1.0
        
        # ê³µí†µ ë¬¸ì ë¹„ìœ¨
        common_chars = set(target_lower) & set(keyword_lower)
        relevance = len(common_chars) / max(len(target_lower), len(keyword_lower))
        
        return relevance
    
    async def record_generation_session(
        self,
        user_id: int,
        session_id: str,
        generated_problems: List[Dict[str, Any]],
        strategy_used: Dict[str, Any]
    ) -> None:
        """ìƒì„± ì„¸ì…˜ ê¸°ë¡ ì €ì¥"""
        
        try:
            session_record = {
                "user_id": user_id,
                "session_id": session_id,
                "timestamp": datetime.now().isoformat(),
                "strategy": strategy_used,
                "problems_generated": len(generated_problems),
                "keywords_used": strategy_used.get("target_keywords", []),
                "diversification_achieved": True  # ì‹¤ì œë¡œëŠ” ìƒì„± ê²°ê³¼ ë¶„ì„ í•„ìš”
            }
            
            # íŒŒì¼ì— ì €ì¥
            session_file = self.generation_history_path / f"{session_id}.json"
            with open(session_file, 'w', encoding='utf-8') as f:
                json.dump(session_record, f, ensure_ascii=False, indent=2)
            
            logger.info(f"ğŸ“ ìƒì„± ì„¸ì…˜ ê¸°ë¡ ì €ì¥: {session_id}")
            
        except Exception as e:
            logger.error(f"ìƒì„± ì„¸ì…˜ ê¸°ë¡ ì‹¤íŒ¨: {e}")


# ì „ì—­ ì„œë¹„ìŠ¤ ì¸ìŠ¤í„´ìŠ¤
generation_tracker = ProblemGenerationTracker() 