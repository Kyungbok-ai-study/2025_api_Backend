"""
êµ­ê°€ê³ ì‹œ ë¬¸ì œ ì¤‘ë³µ ë°©ì§€ ì„œë¹„ìŠ¤
- ê¸°ì¡´ êµ­ê°€ê³ ì‹œ ë¬¸ì œì™€ì˜ ìœ ì‚¬ë„ ê²€ì‚¬
- AI ê¸°ë°˜ ë‚´ìš© ë¶„ì„ ë° ì¤‘ë³µ íƒì§€
- ìƒˆë¡œìš´ ë¬¸ì œ ìƒì„±ì‹œ ì‹¤ì‹œê°„ ê²€ì¦
- í•™ìŠµëœ íŒ¨í„´ í™œìš©í•œ ë‹¤ì–‘ì„± ë³´ì¥
"""
import json
import logging
import hashlib
import re
from typing import Dict, Any, List, Optional, Tuple
from datetime import datetime
from pathlib import Path
from difflib import SequenceMatcher
from dataclasses import dataclass

from sqlalchemy.orm import Session
from sqlalchemy import text, and_, func

from ..models.question import Question

logger = logging.getLogger(__name__)

@dataclass
class SimilarityResult:
    """ìœ ì‚¬ë„ ê²€ì‚¬ ê²°ê³¼"""
    is_duplicate: bool
    similarity_score: float
    similar_question_id: Optional[int]
    similar_content: Optional[str]
    reason: str
    
@dataclass
class QuestionPattern:
    """ë¬¸ì œ íŒ¨í„´ ë¶„ì„ ê²°ê³¼"""
    content_hash: str
    keywords: List[str]
    structure_pattern: str
    difficulty_level: str
    subject_area: str

class DuplicatePreventionService:
    """êµ­ê°€ê³ ì‹œ ë¬¸ì œ ì¤‘ë³µ ë°©ì§€ ì„œë¹„ìŠ¤"""
    
    def __init__(self):
        self.national_exam_cache = {}  # êµ­ê°€ê³ ì‹œ ë¬¸ì œ ìºì‹œ
        self.similarity_threshold = 0.8  # ì¤‘ë³µ íŒì • ì„ê³„ê°’
        self.pattern_cache = {}  # íŒ¨í„´ ìºì‹œ
        
        # í•™ìŠµëœ ë¶„ì„ ë°ì´í„° ë¡œë“œ
        self.evaluator_data = self._load_evaluator_analysis()
        
    def _load_evaluator_analysis(self) -> Dict[str, Any]:
        """í‰ê°€ìœ„ì› ë¶„ì„ ë°ì´í„° ë¡œë“œ"""
        try:
            data_path = Path("data")
            analysis_data = {}
            
            # ë¬¼ë¦¬ì¹˜ë£Œí•™ê³¼ ë°ì´í„°
            pt_detailed = data_path / "detailed_evaluator_analysis.json"
            if pt_detailed.exists():
                with open(pt_detailed, 'r', encoding='utf-8') as f:
                    analysis_data["ë¬¼ë¦¬ì¹˜ë£Œí•™ê³¼"] = json.load(f)
            
            # ì‘ì—…ì¹˜ë£Œí•™ê³¼ ë°ì´í„°  
            ot_detailed = data_path / "detailed_evaluator_analysis_ot.json"
            if ot_detailed.exists():
                with open(ot_detailed, 'r', encoding='utf-8') as f:
                    analysis_data["ì‘ì—…ì¹˜ë£Œí•™ê³¼"] = json.load(f)
            
            logger.info(f"ğŸ“Š í‰ê°€ìœ„ì› ë¶„ì„ ë°ì´í„° ë¡œë“œ: {len(analysis_data)}ê°œ í•™ê³¼")
            return analysis_data
            
        except Exception as e:
            logger.error(f"í‰ê°€ìœ„ì› ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}")
            return {}
    
    async def check_duplicate_against_national_exams(
        self, 
        db: Session,
        question_content: str,
        department: str,
        options: Optional[Dict[str, str]] = None
    ) -> SimilarityResult:
        """êµ­ê°€ê³ ì‹œ ë¬¸ì œì™€ì˜ ì¤‘ë³µ ê²€ì‚¬"""
        
        try:
            # 1ë‹¨ê³„: ë°ì´í„°ë² ì´ìŠ¤ì˜ ê¸°ì¡´ ë¬¸ì œë“¤ê³¼ ë¹„êµ
            db_similarity = await self._check_db_similarity(db, question_content, department)
            
            if db_similarity.is_duplicate:
                return db_similarity
            
            # 2ë‹¨ê³„: ë¶„ì„ ë°ì´í„°ì˜ íŒ¨í„´ê³¼ ë¹„êµ  
            pattern_similarity = await self._check_pattern_similarity(question_content, department)
            
            if pattern_similarity.is_duplicate:
                return pattern_similarity
            
            # 3ë‹¨ê³„: í‚¤ì›Œë“œ ë° êµ¬ì¡° ìœ ì‚¬ë„ ê²€ì‚¬
            structure_similarity = await self._check_structure_similarity(question_content, options)
            
            if structure_similarity.is_duplicate:
                return structure_similarity
            
            # ëª¨ë“  ê²€ì‚¬ í†µê³¼
            return SimilarityResult(
                is_duplicate=False,
                similarity_score=max(db_similarity.similarity_score, 
                                   pattern_similarity.similarity_score,
                                   structure_similarity.similarity_score),
                similar_question_id=None,
                similar_content=None,
                reason="ì¤‘ë³µ ì—†ìŒ - ìƒˆë¡œìš´ ë¬¸ì œ ìƒì„± ê°€ëŠ¥"
            )
            
        except Exception as e:
            logger.error(f"ì¤‘ë³µ ê²€ì‚¬ ì‹¤íŒ¨: {e}")
            return SimilarityResult(
                is_duplicate=False,
                similarity_score=0.0,
                similar_question_id=None,
                similar_content=None,
                reason=f"ê²€ì‚¬ ì˜¤ë¥˜: {str(e)}"
            )
    
    async def _check_db_similarity(
        self, db: Session, question_content: str, department: str
    ) -> SimilarityResult:
        """ë°ì´í„°ë² ì´ìŠ¤ ê¸°ì¡´ ë¬¸ì œì™€ì˜ ìœ ì‚¬ë„ ê²€ì‚¬"""
        
        try:
            # ê°™ì€ í•™ê³¼ì˜ ê¸°ì¡´ ë¬¸ì œë“¤ ì¡°íšŒ
            existing_questions = db.query(Question).filter(
                and_(
                    Question.is_active == True,
                    Question.subject.like(f"%{department}%")
                )
            ).all()
            
            max_similarity = 0.0
            most_similar_question = None
            
            for q in existing_questions:
                if q.content:
                    similarity = self._calculate_text_similarity(question_content, q.content)
                    
                    if similarity > max_similarity:
                        max_similarity = similarity
                        most_similar_question = q
                    
                    # ë†’ì€ ìœ ì‚¬ë„ ë°œê²¬ì‹œ ì¦‰ì‹œ ì¤‘ë³µ íŒì •
                    if similarity >= self.similarity_threshold:
                        return SimilarityResult(
                            is_duplicate=True,
                            similarity_score=similarity,
                            similar_question_id=q.id,
                            similar_content=q.content,
                            reason=f"ê¸°ì¡´ ë¬¸ì œì™€ {similarity:.1%} ìœ ì‚¬ (ID: {q.id})"
                        )
            
            return SimilarityResult(
                is_duplicate=False,
                similarity_score=max_similarity,
                similar_question_id=most_similar_question.id if most_similar_question else None,
                similar_content=None,
                reason=f"DB ê²€ì‚¬ í†µê³¼ (ìµœëŒ€ ìœ ì‚¬ë„: {max_similarity:.1%})"
            )
            
        except Exception as e:
            logger.error(f"DB ìœ ì‚¬ë„ ê²€ì‚¬ ì‹¤íŒ¨: {e}")
            return SimilarityResult(False, 0.0, None, None, f"DB ê²€ì‚¬ ì˜¤ë¥˜: {str(e)}")
    
    async def _check_pattern_similarity(
        self, question_content: str, department: str
    ) -> SimilarityResult:
        """í•™ìŠµëœ íŒ¨í„´ê³¼ì˜ ìœ ì‚¬ë„ ê²€ì‚¬"""
        
        try:
            dept_data = self.evaluator_data.get(department, {})
            if not dept_data:
                return SimilarityResult(False, 0.0, None, None, "íŒ¨í„´ ë°ì´í„° ì—†ìŒ")
            
            # ë¬¸ì œ ë‚´ìš©ì—ì„œ í•µì‹¬ í‚¤ì›Œë“œ ì¶”ì¶œ
            question_keywords = self._extract_keywords(question_content)
            
            # í‰ê°€ìœ„ì› ë°ì´í„°ì˜ íŒ¨í„´ê³¼ ë¹„êµ
            evaluators = dept_data.get("departments", {}).get(department.replace("í•™ê³¼", ""), {}).get("evaluators", {})
            
            max_pattern_similarity = 0.0
            
            for evaluator_name, evaluator_data in evaluators.items():
                subjects = evaluator_data.get("subject_distribution", {})
                
                for subject, count in subjects.items():
                    subject_similarity = self._calculate_keyword_similarity(question_keywords, [subject])
                    
                    if subject_similarity > max_pattern_similarity:
                        max_pattern_similarity = subject_similarity
                    
                    # íŒ¨í„´ ì¤‘ë³µ ì„ê³„ê°’ (ë” ë‚®ê²Œ ì„¤ì •)
                    if subject_similarity >= 0.7:
                        return SimilarityResult(
                            is_duplicate=True,
                            similarity_score=subject_similarity,
                            similar_question_id=None,
                            similar_content=f"í‰ê°€ìœ„ì› {evaluator_name}ì˜ {subject} íŒ¨í„´",
                            reason=f"ê¸°ì¡´ ì¶œì œ íŒ¨í„´ê³¼ {subject_similarity:.1%} ìœ ì‚¬"
                        )
            
            return SimilarityResult(
                is_duplicate=False,
                similarity_score=max_pattern_similarity,
                similar_question_id=None,
                similar_content=None,
                reason=f"íŒ¨í„´ ê²€ì‚¬ í†µê³¼ (ìµœëŒ€ ìœ ì‚¬ë„: {max_pattern_similarity:.1%})"
            )
            
        except Exception as e:
            logger.error(f"íŒ¨í„´ ìœ ì‚¬ë„ ê²€ì‚¬ ì‹¤íŒ¨: {e}")
            return SimilarityResult(False, 0.0, None, None, f"íŒ¨í„´ ê²€ì‚¬ ì˜¤ë¥˜: {str(e)}")
    
    async def _check_structure_similarity(
        self, question_content: str, options: Optional[Dict[str, str]]
    ) -> SimilarityResult:
        """ë¬¸ì œ êµ¬ì¡° ìœ ì‚¬ë„ ê²€ì‚¬"""
        
        try:
            # ë¬¸ì œ êµ¬ì¡° íŒ¨í„´ ë¶„ì„
            structure_pattern = self._analyze_question_structure(question_content, options)
            
            # ìºì‹œëœ êµ¬ì¡° íŒ¨í„´ê³¼ ë¹„êµ
            for cached_pattern, cached_info in self.pattern_cache.items():
                similarity = self._calculate_structure_similarity(structure_pattern, cached_pattern)
                
                if similarity >= 0.9:  # êµ¬ì¡°ê°€ ê±°ì˜ ë™ì¼
                    return SimilarityResult(
                        is_duplicate=True,
                        similarity_score=similarity,
                        similar_question_id=cached_info.get("question_id"),
                        similar_content=None,
                        reason=f"ë¬¸ì œ êµ¬ì¡°ê°€ {similarity:.1%} ë™ì¼"
                    )
            
            # ìƒˆë¡œìš´ íŒ¨í„´ì„ ìºì‹œì— ì¶”ê°€
            pattern_hash = hashlib.md5(structure_pattern.encode()).hexdigest()
            self.pattern_cache[pattern_hash] = {
                "pattern": structure_pattern,
                "created_at": datetime.now(),
                "question_content": question_content[:100]
            }
            
            return SimilarityResult(
                is_duplicate=False,
                similarity_score=0.0,
                similar_question_id=None,
                similar_content=None,
                reason="êµ¬ì¡° ê²€ì‚¬ í†µê³¼ - ìƒˆë¡œìš´ íŒ¨í„´"
            )
            
        except Exception as e:
            logger.error(f"êµ¬ì¡° ìœ ì‚¬ë„ ê²€ì‚¬ ì‹¤íŒ¨: {e}")
            return SimilarityResult(False, 0.0, None, None, f"êµ¬ì¡° ê²€ì‚¬ ì˜¤ë¥˜: {str(e)}")
    
    def _calculate_text_similarity(self, text1: str, text2: str) -> float:
        """í…ìŠ¤íŠ¸ ìœ ì‚¬ë„ ê³„ì‚°"""
        # ì •ê·œí™”
        clean_text1 = re.sub(r'[^\w\s]', '', text1.lower())
        clean_text2 = re.sub(r'[^\w\s]', '', text2.lower())
        
        # SequenceMatcherë¥¼ ì‚¬ìš©í•œ ìœ ì‚¬ë„ ê³„ì‚°
        similarity = SequenceMatcher(None, clean_text1, clean_text2).ratio()
        
        return similarity
    
    def _extract_keywords(self, text: str) -> List[str]:
        """í…ìŠ¤íŠ¸ì—ì„œ í•µì‹¬ í‚¤ì›Œë“œ ì¶”ì¶œ"""
        # ì˜ë£Œ ì „ë¬¸ ìš©ì–´ ìš°ì„  ì¶”ì¶œ
        medical_terms = [
            'ê·¼ìœ¡', 'ê´€ì ˆ', 'ì‹ ê²½', 'í˜ˆê´€', 'í˜¸í¡', 'ìˆœí™˜', 'ì†Œí™”', 'ë‚´ë¶„ë¹„',
            'ë©´ì—­', 'ê°ê°', 'ìš´ë™', 'ì¸ì§€', 'ì¬í™œ', 'ì¹˜ë£Œ', 'ì§„ë‹¨', 'í‰ê°€',
            'í•´ë¶€', 'ìƒë¦¬', 'ë³‘ë¦¬', 'ì•½ë¦¬', 'ì˜ìƒ', 'ê²€ì‚¬'
        ]
        
        keywords = []
        text_lower = text.lower()
        
        for term in medical_terms:
            if term in text_lower:
                keywords.append(term)
        
        # í•œê¸€ ëª…ì‚¬ ì¶”ì¶œ (ê°„ë‹¨í•œ íŒ¨í„´)
        korean_nouns = re.findall(r'[ê°€-í£]{2,}', text)
        keywords.extend(korean_nouns[:5])  # ìƒìœ„ 5ê°œë§Œ
        
        return list(set(keywords))
    
    def _calculate_keyword_similarity(self, keywords1: List[str], keywords2: List[str]) -> float:
        """í‚¤ì›Œë“œ ìœ ì‚¬ë„ ê³„ì‚°"""
        if not keywords1 or not keywords2:
            return 0.0
        
        set1 = set(keywords1)
        set2 = set(keywords2)
        
        intersection = len(set1 & set2)
        union = len(set1 | set2)
        
        return intersection / union if union > 0 else 0.0
    
    def _analyze_question_structure(self, content: str, options: Optional[Dict[str, str]]) -> str:
        """ë¬¸ì œ êµ¬ì¡° ë¶„ì„"""
        structure_elements = []
        
        # ë¬¸ì œ ê¸¸ì´ íŒ¨í„´
        if len(content) < 50:
            structure_elements.append("SHORT")
        elif len(content) < 150:
            structure_elements.append("MEDIUM")
        else:
            structure_elements.append("LONG")
        
        # ì§ˆë¬¸ ìœ í˜• íŒ¨í„´
        if "?" in content:
            structure_elements.append("QUESTION")
        if "ë‹¤ìŒ" in content:
            structure_elements.append("MULTIPLE_CHOICE")
        if "ê°€ì¥" in content:
            structure_elements.append("BEST_ANSWER")
        
        # ì„ íƒì§€ íŒ¨í„´
        if options:
            structure_elements.append(f"OPTIONS_{len(options)}")
        
        return "_".join(structure_elements)
    
    def _calculate_structure_similarity(self, pattern1: str, pattern2: str) -> float:
        """êµ¬ì¡° íŒ¨í„´ ìœ ì‚¬ë„ ê³„ì‚°"""
        elements1 = set(pattern1.split("_"))
        elements2 = set(pattern2.split("_"))
        
        if not elements1 or not elements2:
            return 0.0
        
        intersection = len(elements1 & elements2)
        union = len(elements1 | elements2)
        
        return intersection / union
    
    async def generate_unique_question_guidance(
        self, 
        db: Session,
        subject: str,
        difficulty: str,
        department: str,
        keywords: List[str]
    ) -> Dict[str, Any]:
        """ì¤‘ë³µ ì—†ëŠ” ë¬¸ì œ ìƒì„± ê°€ì´ë“œ"""
        
        # ì‚¬ìš© ë¹ˆë„ê°€ ë‚®ì€ í‚¤ì›Œë“œ ì°¾ê¸°
        unused_keywords = await self._find_unused_concepts(db, department, keywords)
        
        # ìƒˆë¡œìš´ ë¬¸ì œ ì ‘ê·¼ë²• ì œì•ˆ
        alternative_approaches = await self._suggest_alternative_approaches(department, subject)
        
        # ë‚œì´ë„ë³„ ë‹¤ì–‘ì„± ì „ëµ
        diversity_strategy = self._create_diversity_strategy(difficulty, department)
        
        return {
            "recommended_keywords": unused_keywords[:5],
            "alternative_approaches": alternative_approaches,
            "diversity_strategy": diversity_strategy,
            "avoid_patterns": await self._get_overused_patterns(db, department),
            "uniqueness_tips": [
                "ê¸°ì¡´ ë¬¸ì œì™€ ë‹¤ë¥¸ ê´€ì ìœ¼ë¡œ ì ‘ê·¼í•˜ì„¸ìš”",
                "ì‹¤ì œ ì„ìƒ ìƒí™©ì„ ë°˜ì˜í•œ ìƒˆë¡œìš´ ì‹œë‚˜ë¦¬ì˜¤ë¥¼ ë§Œë“œì„¸ìš”", 
                "ìµœì‹  ì—°êµ¬ë‚˜ ê¸°ìˆ ì„ ë°˜ì˜í•˜ì„¸ìš”",
                "ë‹¤í•™ì œì  ì ‘ê·¼ì„ ì‹œë„í•˜ì„¸ìš”"
            ]
        }
    
    async def _find_unused_concepts(
        self, db: Session, department: str, current_keywords: List[str]
    ) -> List[str]:
        """ì‚¬ìš©ë˜ì§€ ì•Šì€ ê°œë… ì°¾ê¸°"""
        
        # í‰ê°€ìœ„ì› ë°ì´í„°ì—ì„œ ëª¨ë“  ê°œë… ì¶”ì¶œ
        dept_data = self.evaluator_data.get(department, {})
        all_concepts = set()
        
        evaluators = dept_data.get("departments", {}).get(department.replace("í•™ê³¼", ""), {}).get("evaluators", {})
        for evaluator_data in evaluators.values():
            subjects = evaluator_data.get("subject_distribution", {})
            all_concepts.update(subjects.keys())
        
        # í˜„ì¬ í‚¤ì›Œë“œì™€ ì¤‘ë³µë˜ì§€ ì•ŠëŠ” ê°œë…ë“¤
        unused_concepts = list(all_concepts - set(current_keywords))
        
        return unused_concepts[:10]
    
    async def _suggest_alternative_approaches(self, department: str, subject: str) -> List[str]:
        """ëŒ€ì•ˆì  ì ‘ê·¼ë²• ì œì•ˆ"""
        
        approaches = {
            "ë¬¼ë¦¬ì¹˜ë£Œí•™ê³¼": [
                "í™˜ì ì¼€ì´ìŠ¤ ìŠ¤í„°ë”” ê¸°ë°˜ ë¬¸ì œ",
                "ìš´ë™í•™ì  ë¶„ì„ ë¬¸ì œ", 
                "ì„ìƒ ì˜ì‚¬ê²°ì • ì‹œë‚˜ë¦¬ì˜¤",
                "ê·¼ê±°ê¸°ë°˜ ì¹˜ë£Œ ì„ íƒ ë¬¸ì œ"
            ],
            "ì‘ì—…ì¹˜ë£Œí•™ê³¼": [
                "ì¼ìƒìƒí™œ ì ì‘ ì‹œë‚˜ë¦¬ì˜¤",
                "ì¸ì§€ì¬í™œ í”„ë¡œê·¸ë¨ ì„¤ê³„",
                "ë³´ì¡°ê¸°êµ¬ ì„ íƒ ë° ì ìš©",
                "í™˜ê²½ ìˆ˜ì • ë° ì ì‘ ì „ëµ"
            ],
            "ê°„í˜¸í•™ê³¼": [
                "ê°„í˜¸ê³¼ì • ì ìš© ì‹œë‚˜ë¦¬ì˜¤",
                "í™˜ì ì•ˆì „ ìƒí™© íŒë‹¨",
                "ì˜ë£Œì§„ í˜‘ë ¥ ìƒí™©",
                "ìœ¤ë¦¬ì  ë”œë ˆë§ˆ í•´ê²°"
            ]
        }
        
        return approaches.get(department, [])
    
    def _create_diversity_strategy(self, difficulty: str, department: str) -> Dict[str, Any]:
        """ë‹¤ì–‘ì„± ì „ëµ ìƒì„±"""
        
        strategies = {
            "í•˜": {
                "focus": "ê¸°ë³¸ ê°œë…ì˜ ìƒˆë¡œìš´ í‘œí˜„",
                "methods": ["ë„ì‹í™”", "ë¹„êµë¶„ì„", "ì‹¤ì˜ˆ ì ìš©"]
            },
            "ì¤‘": {
                "focus": "ì‘ìš© ë° ì—°ê²° ê°œë…",
                "methods": ["í†µí•©ì  ì‚¬ê³ ", "ì›ì¸-ê²°ê³¼ ë¶„ì„", "ìƒí™© ì ìš©"]
            },
            "ìƒ": {
                "focus": "ë³µí•©ì  ë¬¸ì œ í•´ê²°",
                "methods": ["ë‹¤ë‹¨ê³„ ë¶„ì„", "ê°€ì„¤ ê²€ì¦", "ì°½ì˜ì  í•´ê²°ì±…"]
            }
        }
        
        return strategies.get(difficulty, strategies["ì¤‘"])
    
    async def _get_overused_patterns(self, db: Session, department: str) -> List[str]:
        """ê³¼ë‹¤ ì‚¬ìš©ëœ íŒ¨í„´ ì¶”ì¶œ"""
        
        try:
            # ìµœê·¼ ìƒì„±ëœ ë¬¸ì œë“¤ì˜ íŒ¨í„´ ë¶„ì„
            recent_questions = db.query(Question).filter(
                and_(
                    Question.subject.like(f"%{department}%"),
                    Question.created_at >= datetime.now().replace(month=datetime.now().month-1)
                )
            ).all()
            
            pattern_count = {}
            for q in recent_questions:
                if q.content:
                    pattern = self._analyze_question_structure(q.content, q.options)
                    pattern_count[pattern] = pattern_count.get(pattern, 0) + 1
            
            # 3íšŒ ì´ìƒ ì‚¬ìš©ëœ íŒ¨í„´ë“¤
            overused = [pattern for pattern, count in pattern_count.items() if count >= 3]
            
            return overused
            
        except Exception as e:
            logger.error(f"ê³¼ë‹¤ ì‚¬ìš© íŒ¨í„´ ë¶„ì„ ì‹¤íŒ¨: {e}")
            return []

# ì „ì—­ ì„œë¹„ìŠ¤ ì¸ìŠ¤í„´ìŠ¤
duplicate_prevention_service = DuplicatePreventionService() 