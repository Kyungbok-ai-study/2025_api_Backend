#!/usr/bin/env python3
"""
ë¬¸ì œ ìœ í˜• ìë™ ë°°ì • ì‹œìŠ¤í…œ
ì—‘ì…€ íŒŒì¼ì„ í†µí•´ ë¬¸ì œ ìœ í˜•ì„ ìë™ìœ¼ë¡œ ë§¤í•‘í•˜ëŠ” ì„œë¹„ìŠ¤
"""

import json
import os
import logging
import pandas as pd
from typing import Dict, List, Any, Optional
from datetime import datetime
from pathlib import Path
import re

logger = logging.getLogger(__name__)

class QuestionTypeMapper:
    """ë¬¸ì œ ìœ í˜• ìë™ ë°°ì • ì‹œìŠ¤í…œ"""
    
    def __init__(self):
        # ë°ì´í„° ë””ë ‰í† ë¦¬ ì„¤ì •
        self.data_dir = Path("app/data")
        self.data_dir.mkdir(exist_ok=True)
        
        # ë¬¸ì œ ìœ í˜• ë§¤í•‘ ë°ì´í„° íŒŒì¼
        self.type_mapping_file = self.data_dir / "question_type_mapping.json"
        
        # ê¸°ë³¸ ë¬¸ì œ ìœ í˜• ì •ì˜
        self.question_types = {
            "multiple_choice": {
                "name": "ê°ê´€ì‹",
                "description": "5ì§€ì„ ë‹¤í˜• ë¬¸ì œ",
                "keywords": ["ì„ íƒí•˜ì‹œì˜¤", "ë§ëŠ” ê²ƒì€", "í‹€ë¦° ê²ƒì€", "ì˜¬ë°”ë¥¸ ê²ƒì€", "â‘ ", "â‘¡", "â‘¢", "â‘£", "â‘¤"],
                "patterns": [r"â‘ .*â‘¡.*â‘¢.*â‘£.*â‘¤", r"\d+\.\s+.*\n\d+\.\s+.*", r"ê°€\).*ë‚˜\).*ë‹¤\)"]
            },
            "short_answer": {
                "name": "ë‹¨ë‹µí˜•",
                "description": "ê°„ë‹¨í•œ ë‹µì•ˆ ì„œìˆ ",
                "keywords": ["ì„œìˆ í•˜ì‹œì˜¤", "ì ìœ¼ì‹œì˜¤", "ì“°ì‹œì˜¤", "ê¸°ìˆ í•˜ì‹œì˜¤", "ë¬´ì—‡ì¸ê°€"],
                "patterns": [r".*\?\s*$", r".*ì€\?\s*$", r".*ì¸ê°€\?\s*$"]
            },
            "essay": {
                "name": "ë…¼ìˆ í˜•",
                "description": "ì¥ë¬¸ ì„œìˆ í˜• ë¬¸ì œ",
                "keywords": ["ë…¼ìˆ í•˜ì‹œì˜¤", "ì„¤ëª…í•˜ì‹œì˜¤", "ë¶„ì„í•˜ì‹œì˜¤", "ë¹„êµí•˜ì‹œì˜¤", "í‰ê°€í•˜ì‹œì˜¤"],
                "patterns": [r".*ì„¤ëª…í•˜ì‹œì˜¤", r".*ë…¼ìˆ í•˜ì‹œì˜¤", r".*ë¶„ì„í•˜ì‹œì˜¤"]
            },
            "true_false": {
                "name": "ì°¸/ê±°ì§“",
                "description": "O/Xí˜• ë¬¸ì œ",
                "keywords": ["ì°¸ì¸ì§€ ê±°ì§“ì¸ì§€", "O ë˜ëŠ” X", "ë§ìœ¼ë©´ O", "í‹€ë¦¬ë©´ X", "ì˜³ê³  ê·¸ë¦„"],
                "patterns": [r".*\(O\).*\(X\)", r".*ì°¸.*ê±°ì§“", r".*O.*X"]
            },
            "fill_blank": {
                "name": "ë¹ˆì¹¸ì±„ìš°ê¸°",
                "description": "ë¹ˆì¹¸ì„ ì±„ìš°ëŠ” ë¬¸ì œ",
                "keywords": ["ë¹ˆì¹¸ì— ë“¤ì–´ê°ˆ", "ê´„í˜¸ ì•ˆì—", "_____", "â–¡", "â—‹â—‹â—‹"],
                "patterns": [r"_{2,}", r"â–¡+", r"\(\s*\)", r"â—‹{2,}"]
            },
            "matching": {
                "name": "ì—°ê²°í˜•",
                "description": "í•­ëª©ì„ ì—°ê²°í•˜ëŠ” ë¬¸ì œ",
                "keywords": ["ì—°ê²°í•˜ì‹œì˜¤", "ì§ì§€ìœ¼ì‹œì˜¤", "ë§¤ì¹­", "ê°€-ë‚˜ ì—°ê²°", "ì™¼ìª½ê³¼ ì˜¤ë¥¸ìª½"],
                "patterns": [r"ê°€.*ë‚˜.*ë‹¤.*ë¼", r"A.*B.*C.*D", r"ì™¼ìª½.*ì˜¤ë¥¸ìª½"]
            }
        }
        
        # í•™ê³¼ë³„ ë¬¸ì œ ìœ í˜• íŠ¹ì„±
        self.department_preferences = {
            "ê°„í˜¸í•™ê³¼": {
                "multiple_choice": 0.6,  # 60% ê°ê´€ì‹
                "short_answer": 0.2,     # 20% ë‹¨ë‹µí˜•
                "essay": 0.15,           # 15% ë…¼ìˆ í˜•
                "true_false": 0.05       # 5% ì°¸/ê±°ì§“
            },
            "ë¬¼ë¦¬ì¹˜ë£Œí•™ê³¼": {
                "multiple_choice": 0.65,
                "short_answer": 0.25,
                "essay": 0.1,
                "true_false": 0.0
            },
            "ì‘ì—…ì¹˜ë£Œí•™ê³¼": {
                "multiple_choice": 0.55,
                "short_answer": 0.25,
                "essay": 0.2,
                "true_false": 0.0
            }
        }
        
        # ê¸°ì¡´ ë§¤í•‘ ë°ì´í„° ë¡œë“œ
        self.load_type_mapping_data()
        
        logger.info("âœ… ë¬¸ì œ ìœ í˜• ìë™ ë°°ì • ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ")
    
    def load_type_mapping_data(self):
        """ê¸°ì¡´ ë¬¸ì œ ìœ í˜• ë§¤í•‘ ë°ì´í„° ë¡œë“œ"""
        try:
            if self.type_mapping_file.exists():
                with open(self.type_mapping_file, 'r', encoding='utf-8') as f:
                    self.type_mapping_data = json.load(f)
                logger.info(f"âœ… ê¸°ì¡´ ë¬¸ì œ ìœ í˜• ë§¤í•‘ ë°ì´í„° ë¡œë“œ: {len(self.type_mapping_data)}ê°œ íŒŒì¼")
            else:
                self.type_mapping_data = {}
                logger.info("ğŸ“ ìƒˆë¡œìš´ ë¬¸ì œ ìœ í˜• ë§¤í•‘ ë°ì´í„° ìƒì„±")
        except Exception as e:
            logger.error(f"âŒ ë¬¸ì œ ìœ í˜• ë§¤í•‘ ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}")
            self.type_mapping_data = {}
    
    def save_type_mapping_data(self):
        """ë¬¸ì œ ìœ í˜• ë§¤í•‘ ë°ì´í„° ì €ì¥"""
        try:
            with open(self.type_mapping_file, 'w', encoding='utf-8') as f:
                json.dump(self.type_mapping_data, f, ensure_ascii=False, indent=2)
            logger.info("âœ… ë¬¸ì œ ìœ í˜• ë§¤í•‘ ë°ì´í„° ì €ì¥ ì™„ë£Œ")
        except Exception as e:
            logger.error(f"âŒ ë¬¸ì œ ìœ í˜• ë§¤í•‘ ë°ì´í„° ì €ì¥ ì‹¤íŒ¨: {e}")
    
    async def process_excel_for_question_types(
        self, 
        excel_file_path: str, 
        professor_name: str,
        department: str = "ì¼ë°˜"
    ) -> Dict[str, Any]:
        """
        ì—‘ì…€ íŒŒì¼ì—ì„œ ë¬¸ì œ ìœ í˜• ìë™ ë°°ì •
        """
        try:
            logger.info(f"ğŸ“Š ë¬¸ì œ ìœ í˜• ì—‘ì…€ ì²˜ë¦¬ ì‹œì‘: {excel_file_path}")
            
            # ì—‘ì…€ íŒŒì¼ ì½ê¸°
            df = pd.read_excel(excel_file_path, engine='openpyxl')
            logger.info(f"   ğŸ“‹ ì—‘ì…€ ë°ì´í„°: {len(df)}í–‰, ì»¬ëŸ¼: {list(df.columns)}")
            
            # ë¬¸ì œ ìœ í˜• ë¶„ì„ ìˆ˜í–‰
            type_analysis = await self._analyze_question_types(df, professor_name, department)
            
            # ê²°ê³¼ ì €ì¥
            file_key = f"{professor_name}_{Path(excel_file_path).stem}_{datetime.now().strftime('%Y%m%d')}"
            self.type_mapping_data[file_key] = {
                "file_path": excel_file_path,
                "professor": professor_name,
                "department": department,
                "processed_at": datetime.now().isoformat(),
                "total_questions": len(df),
                "type_analysis": type_analysis,
                "type_distribution": self._calculate_type_distribution(type_analysis)
            }
            
            # ë§¤í•‘ ë°ì´í„° ì €ì¥
            self.save_type_mapping_data()
            
            logger.info(f"âœ… ë¬¸ì œ ìœ í˜• ì—‘ì…€ ì²˜ë¦¬ ì™„ë£Œ: {len(type_analysis['questions'])}ê°œ ë¬¸ì œ")
            
            return {
                "success": True,
                "file_key": file_key,
                "total_questions": len(df),
                "type_analysis": type_analysis,
                "message": "ë¬¸ì œ ìœ í˜• ìë™ ë°°ì • ì™„ë£Œ"
            }
            
        except Exception as e:
            logger.error(f"âŒ ë¬¸ì œ ìœ í˜• ì—‘ì…€ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            return {
                "success": False,
                "error": str(e),
                "message": "ë¬¸ì œ ìœ í˜• ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ"
            }
    
    async def _analyze_question_types(
        self, 
        df: pd.DataFrame, 
        professor_name: str,
        department: str
    ) -> Dict[str, Any]:
        """ë¬¸ì œ ìœ í˜• ë¶„ì„ ìˆ˜í–‰"""
        
        type_analysis = {
            "questions": [],
            "type_stats": {},
            "confidence_scores": {},
            "auto_assigned": 0,
            "manual_required": 0
        }
        
        try:
            # ì—‘ì…€ ì»¬ëŸ¼ ë§¤í•‘
            column_mapping = self._map_excel_columns(df)
            logger.info(f"   ğŸ“Š ì»¬ëŸ¼ ë§¤í•‘: {column_mapping}")
            
            # ê° ë¬¸ì œë³„ ìœ í˜• ë¶„ì„
            for idx, row in df.iterrows():
                try:
                    question_data = self._extract_question_data(row, column_mapping, idx)
                    
                    if question_data["content"]:
                        # ë¬¸ì œ ìœ í˜• ìë™ íŒë‹¨
                        type_result = self._determine_question_type(
                            question_data["content"], 
                            question_data.get("options", ""),
                            department
                        )
                        
                        question_analysis = {
                            "question_number": idx + 1,
                            "content": question_data["content"][:100] + "..." if len(question_data["content"]) > 100 else question_data["content"],
                            "detected_type": type_result["type"],
                            "confidence": type_result["confidence"],
                            "reasoning": type_result["reasoning"],
                            "alternative_types": type_result["alternatives"],
                            "manual_review_needed": type_result["confidence"] < 0.7
                        }
                        
                        type_analysis["questions"].append(question_analysis)
                        
                        # í†µê³„ ì—…ë°ì´íŠ¸
                        question_type = type_result["type"]
                        if question_type not in type_analysis["type_stats"]:
                            type_analysis["type_stats"][question_type] = 0
                        type_analysis["type_stats"][question_type] += 1
                        
                        # ì‹ ë¢°ë„ ì ìˆ˜ ì¶”ê°€
                        if question_type not in type_analysis["confidence_scores"]:
                            type_analysis["confidence_scores"][question_type] = []
                        type_analysis["confidence_scores"][question_type].append(type_result["confidence"])
                        
                        # ìë™/ìˆ˜ë™ ë¶„ë¥˜ ì¹´ìš´íŠ¸
                        if type_result["confidence"] >= 0.7:
                            type_analysis["auto_assigned"] += 1
                        else:
                            type_analysis["manual_required"] += 1
                
                except Exception as e:
                    logger.warning(f"âš ï¸ ë¬¸ì œ {idx + 1} ë¶„ì„ ì‹¤íŒ¨: {e}")
                    continue
            
            # í‰ê·  ì‹ ë¢°ë„ ê³„ì‚°
            for qtype, scores in type_analysis["confidence_scores"].items():
                type_analysis["confidence_scores"][qtype] = {
                    "average": sum(scores) / len(scores),
                    "count": len(scores),
                    "min": min(scores),
                    "max": max(scores)
                }
            
        except Exception as e:
            logger.error(f"âŒ ë¬¸ì œ ìœ í˜• ë¶„ì„ ì‹¤íŒ¨: {e}")
        
        return type_analysis
    
    def _map_excel_columns(self, df: pd.DataFrame) -> Dict[str, str]:
        """ì—‘ì…€ ì»¬ëŸ¼ ìë™ ë§¤í•‘"""
        
        possible_columns = {
            "question": ["ë¬¸ì œ", "ë¬¸í•­", "question", "ë¬¸ì œë‚´ìš©", "ë‚´ìš©", "ì§€ë¬¸"],
            "options": ["ì„ íƒì§€", "ë³´ê¸°", "options", "choices", "ë‹µì•ˆ", "í•­ëª©"],
            "answer": ["ì •ë‹µ", "ë‹µ", "answer", "correct_answer", "ê°€ë‹µì•ˆ"],
            "type": ["ìœ í˜•", "í˜•íƒœ", "type", "ë¶„ë¥˜", "ì¢…ë¥˜"]
        }
        
        column_mapping = {}
        for key, candidates in possible_columns.items():
            for col in df.columns:
                if any(candidate in str(col).lower() for candidate in candidates):
                    column_mapping[key] = col
                    break
        
        return column_mapping
    
    def _extract_question_data(self, row: pd.Series, column_mapping: Dict, idx: int) -> Dict:
        """í–‰ì—ì„œ ë¬¸ì œ ë°ì´í„° ì¶”ì¶œ"""
        
        return {
            "content": str(row.get(column_mapping.get("question", ""), "")).strip(),
            "options": str(row.get(column_mapping.get("options", ""), "")).strip(), 
            "answer": str(row.get(column_mapping.get("answer", ""), "")).strip(),
            "manual_type": str(row.get(column_mapping.get("type", ""), "")).strip(),
            "row_index": idx
        }
    
    def _determine_question_type(
        self, 
        question_content: str, 
        options: str = "",
        department: str = "ì¼ë°˜"
    ) -> Dict[str, Any]:
        """ë¬¸ì œ ë‚´ìš©ì„ ë¶„ì„í•˜ì—¬ ìœ í˜• ìë™ íŒë‹¨"""
        
        type_scores = {}
        reasoning_details = []
        
        # ê° ë¬¸ì œ ìœ í˜•ë³„ ì ìˆ˜ ê³„ì‚°
        for qtype, config in self.question_types.items():
            score = 0
            matched_keywords = []
            matched_patterns = []
            
            # í‚¤ì›Œë“œ ë§¤ì¹­
            for keyword in config["keywords"]:
                if keyword in question_content or keyword in options:
                    score += 2
                    matched_keywords.append(keyword)
            
            # íŒ¨í„´ ë§¤ì¹­
            for pattern in config["patterns"]:
                if re.search(pattern, question_content + " " + options):
                    score += 3
                    matched_patterns.append(pattern)
            
            # í•™ê³¼ë³„ ì„ í˜¸ë„ ë°˜ì˜
            dept_prefs = self.department_preferences.get(department, {})
            if qtype in dept_prefs:
                score += dept_prefs[qtype] * 1  # ì„ í˜¸ë„ ë³´ë„ˆìŠ¤
            
            type_scores[qtype] = score
            
            if matched_keywords or matched_patterns:
                reasoning_details.append({
                    "type": qtype,
                    "score": score,
                    "keywords": matched_keywords,
                    "patterns": matched_patterns
                })
        
        # ìµœê³  ì ìˆ˜ ìœ í˜• ê²°ì •
        if not type_scores or max(type_scores.values()) == 0:
            # ê¸°ë³¸ê°’: í•™ê³¼ë³„ ê°€ì¥ ì¼ë°˜ì ì¸ ìœ í˜•
            dept_prefs = self.department_preferences.get(department, {})
            if dept_prefs:
                best_type = max(dept_prefs.items(), key=lambda x: x[1])[0]
                confidence = 0.3  # ë‚®ì€ ì‹ ë¢°ë„
            else:
                best_type = "multiple_choice"
                confidence = 0.2
            reasoning = "í‚¤ì›Œë“œ/íŒ¨í„´ ë§¤ì¹­ ì‹¤íŒ¨, ê¸°ë³¸ê°’ ì‚¬ìš©"
        else:
            best_type = max(type_scores.items(), key=lambda x: x[1])[0]
            max_score = type_scores[best_type]
            
            # ì‹ ë¢°ë„ ê³„ì‚° (0-1 ìŠ¤ì¼€ì¼)
            total_possible = 10  # ìµœëŒ€ ê°€ëŠ¥ ì ìˆ˜ (í‚¤ì›Œë“œ 5ê°œ * 2 + íŒ¨í„´ ë§¤ì¹­ ë³´ë„ˆìŠ¤)
            confidence = min(max_score / total_possible, 1.0)
            
            reasoning = f"ì ìˆ˜: {max_score}, ë§¤ì¹­ëœ ìš”ì†Œë“¤ ê¸°ë°˜ íŒë‹¨"
        
        # ëŒ€ì•ˆ ìœ í˜•ë“¤ (ìƒìœ„ 3ê°œ)
        sorted_types = sorted(type_scores.items(), key=lambda x: x[1], reverse=True)
        alternatives = [
            {"type": qtype, "score": score, "name": self.question_types[qtype]["name"]} 
            for qtype, score in sorted_types[1:4] if score > 0
        ]
        
        return {
            "type": best_type,
            "confidence": confidence,
            "reasoning": reasoning,
            "alternatives": alternatives,
            "type_scores": type_scores,
            "reasoning_details": reasoning_details
        }
    
    def _calculate_type_distribution(self, type_analysis: Dict) -> Dict[str, Any]:
        """ë¬¸ì œ ìœ í˜• ë¶„í¬ ê³„ì‚°"""
        
        total = len(type_analysis["questions"])
        if total == 0:
            return {}
        
        distribution = {}
        for qtype, count in type_analysis["type_stats"].items():
            distribution[qtype] = {
                "count": count,
                "percentage": round((count / total) * 100, 1),
                "name": self.question_types.get(qtype, {}).get("name", qtype)
            }
        
        return distribution
    
    def get_question_type_for_question(
        self, 
        question_content: str, 
        file_key: str = None,
        question_number: int = None
    ) -> str:
        """
        íŠ¹ì • ë¬¸ì œì˜ ìœ í˜• ì¡°íšŒ (íŒŒì„œì—ì„œ ì‚¬ìš©)
        """
        try:
            # íŒŒì¼ë³„ ë§¤í•‘ ë°ì´í„°ì—ì„œ ì¡°íšŒ
            if file_key and file_key in self.type_mapping_data:
                mapping_data = self.type_mapping_data[file_key]
                questions = mapping_data.get("type_analysis", {}).get("questions", [])
                
                # ë¬¸ì œ ë²ˆí˜¸ë¡œ ì¡°íšŒ
                if question_number:
                    for q in questions:
                        if q.get("question_number") == question_number:
                            return q.get("detected_type", "multiple_choice")
                
                # ë‚´ìš© ìœ ì‚¬ë„ë¡œ ì¡°íšŒ
                for q in questions:
                    if question_content and len(question_content) > 20:
                        # ê°„ë‹¨í•œ ìœ ì‚¬ë„ ì²´í¬ (ì²« 50ì ë¹„êµ)
                        if question_content[:50] in q.get("content", ""):
                            return q.get("detected_type", "multiple_choice")
            
            # ì‹¤ì‹œê°„ ë¶„ì„
            type_result = self._determine_question_type(question_content)
            return type_result["type"]
            
        except Exception as e:
            logger.warning(f"âš ï¸ ë¬¸ì œ ìœ í˜• ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return "multiple_choice"  # ê¸°ë³¸ê°’
    
    def get_type_mapping_summary(self) -> Dict[str, Any]:
        """ë¬¸ì œ ìœ í˜• ë§¤í•‘ í˜„í™© ìš”ì•½"""
        
        summary = {
            "total_files": len(self.type_mapping_data),
            "total_questions": 0,
            "type_distribution": {},
            "department_stats": {},
            "confidence_analysis": {
                "high_confidence": 0,    # >= 0.8
                "medium_confidence": 0,  # 0.5-0.8
                "low_confidence": 0      # < 0.5
            },
            "recent_files": []
        }
        
        try:
            for file_key, data in self.type_mapping_data.items():
                questions = data.get("type_analysis", {}).get("questions", [])
                summary["total_questions"] += len(questions)
                
                # í•™ê³¼ë³„ í†µê³„
                dept = data.get("department", "ì¼ë°˜")
                if dept not in summary["department_stats"]:
                    summary["department_stats"][dept] = {"files": 0, "questions": 0}
                summary["department_stats"][dept]["files"] += 1
                summary["department_stats"][dept]["questions"] += len(questions)
                
                # ìœ í˜•ë³„ ë¶„í¬
                for q in questions:
                    qtype = q.get("detected_type", "unknown")
                    if qtype not in summary["type_distribution"]:
                        summary["type_distribution"][qtype] = 0
                    summary["type_distribution"][qtype] += 1
                    
                    # ì‹ ë¢°ë„ ë¶„ì„
                    confidence = q.get("confidence", 0)
                    if confidence >= 0.8:
                        summary["confidence_analysis"]["high_confidence"] += 1
                    elif confidence >= 0.5:
                        summary["confidence_analysis"]["medium_confidence"] += 1
                    else:
                        summary["confidence_analysis"]["low_confidence"] += 1
                
                # ìµœê·¼ íŒŒì¼ ëª©ë¡ (ìƒìœ„ 5ê°œ)
                summary["recent_files"].append({
                    "file_key": file_key,
                    "professor": data.get("professor"),
                    "department": data.get("department"),
                    "processed_at": data.get("processed_at"),
                    "question_count": len(questions)
                })
            
            # ìµœê·¼ íŒŒì¼ ì •ë ¬
            summary["recent_files"] = sorted(
                summary["recent_files"], 
                key=lambda x: x["processed_at"], 
                reverse=True
            )[:5]
            
        except Exception as e:
            logger.error(f"âŒ ë¬¸ì œ ìœ í˜• ë§¤í•‘ ìš”ì•½ ìƒì„± ì‹¤íŒ¨: {e}")
        
        return summary

# ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤
question_type_mapper = QuestionTypeMapper() 