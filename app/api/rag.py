"""
RAG ì‹œìŠ¤í…œ API ì—”ë“œí¬ì¸íŠ¸ - DeepSeek + Gemini í†µí•©
"""
import os
import shutil
from typing import List, Dict, Any, Optional
from datetime import datetime
from pathlib import Path

from fastapi import APIRouter, Depends, HTTPException, File, UploadFile, Form, status
from fastapi.responses import JSONResponse
from sqlalchemy.orm import Session
from pydantic import BaseModel, Field

from ..db.database import get_db
from ..auth.dependencies import get_current_user
from ..models.user import User
from ..services.rag_system import RAGService
from ..services.deepseek_service import deepseek_service
from ..services.qdrant_service import qdrant_service
from ..services.rag_integration_service import rag_integration_service

router = APIRouter(prefix="/rag", tags=["RAG ë¬¸ì„œ ê´€ë¦¬"])

# === DeepSeek + Gemini í†µí•© Pydantic ëª¨ë¸ë“¤ ===

class DeepSeekRAGUploadRequest(BaseModel):
    """DeepSeek RAG ì—…ë°ì´íŠ¸ ìš”ì²­ ëª¨ë¸"""
    document_title: str = Field(..., min_length=1, max_length=200)
    department: str = Field(..., description="í•™ê³¼ (ê°„í˜¸í•™ê³¼, ë¬¼ë¦¬ì¹˜ë£Œí•™ê³¼, ì‘ì—…ì¹˜ë£Œí•™ê³¼)")
    subject: Optional[str] = Field(None, description="ê³¼ëª©ëª…")
    auto_classify: bool = Field(True, description="ìë™ ë¶„ë¥˜ ì‚¬ìš© ì—¬ë¶€")
    chunk_size: int = Field(1000, ge=100, le=3000)
    overlap: int = Field(200, ge=0, le=500)
    use_deepseek_labeling: bool = Field(True, description="DeepSeek ë¼ë²¨ë§ ì‚¬ìš©")

class DeepSeekRAGUploadResponse(BaseModel):
    """DeepSeek RAG ì—…ë°ì´íŠ¸ ì‘ë‹µ ëª¨ë¸"""
    success: bool
    message: str
    processing_id: str
    document_info: Dict[str, Any]
    processing_steps: Dict[str, Any]
    statistics: Dict[str, Any]

class RAGProcessingStatus(BaseModel):
    """RAG ì²˜ë¦¬ ìƒíƒœ ëª¨ë¸"""
    processing_id: str
    status: str  # "processing", "completed", "failed"
    progress_percentage: int
    current_step: str
    steps_completed: List[str]
    results: Optional[Dict[str, Any]] = None
    error_message: Optional[str] = None

class DeepSeekKnowledgeBaseStats(BaseModel):
    """DeepSeek ì§€ì‹ë² ì´ìŠ¤ í†µê³„"""
    total_documents: int
    total_chunks: int
    total_vectors: int
    departments: Dict[str, int]
    subjects: Dict[str, int]
    difficulty_distribution: Dict[str, int]
    last_updated: str
    embedding_model: str
    vector_dimension: int

# === ê¸°ì¡´ ëª¨ë¸ë“¤ ===
class DocumentUploadResponse(BaseModel):
    success: bool
    message: str
    document_title: Optional[str] = None
    chunks_count: Optional[int] = None
    stored_count: Optional[int] = None
    file_path: Optional[str] = None

class QuestionGenerationRequest(BaseModel):
    topic: str
    difficulty: str = "ì¤‘"
    question_type: str = "multiple_choice"
    context_limit: int = 3

class QuestionGenerationResponse(BaseModel):
    success: bool
    message: str
    question: Optional[Dict[str, Any]] = None
    contexts_used: Optional[List[Dict[str, Any]]] = None
    sources: Optional[List[str]] = None

class RAGStatistics(BaseModel):
    document_count: int
    chunk_count: int
    avg_chunk_length: int
    recent_documents: List[Dict[str, Any]]
    vector_enabled: bool
    embedding_model: Optional[str] = None

class SimilaritySearchRequest(BaseModel):
    query_text: str
    limit: int = 5
    similarity_threshold: float = 0.7

# ì„œë¹„ìŠ¤ ì¸ìŠ¤í„´ìŠ¤ë“¤
rag_service = RAGService()

# === DeepSeek + Gemini í†µí•© ì—”ë“œí¬ì¸íŠ¸ë“¤ ===

@router.post("/deepseek-upload", response_model=DeepSeekRAGUploadResponse)
async def upload_document_with_deepseek(
    file: UploadFile = File(...),
    request_data: str = Form(...),  # JSON ë¬¸ìì—´ë¡œ ë°›ì•„ì„œ íŒŒì‹±
    db: Session = Depends(get_db),
    current_user: User = Depends(get_current_user)
):
    """
    DeepSeek + Gemini í†µí•© ë¬¸ì„œ ì—…ë¡œë“œ ë° ì²˜ë¦¬
    
    ì›Œí¬í”Œë¡œìš°:
    1. Geminië¡œ PDF íŒŒì‹±
    2. DeepSeekìœ¼ë¡œ ë‚œì´ë„/ìœ í˜• ë¶„ë¥˜  
    3. Qdrant ë²¡í„° DBì— ì €ì¥
    4. DeepSeek í•™ìŠµ ë°ì´í„° ì—…ë°ì´íŠ¸
    """
    import json
    import uuid
    
    try:
        # ìš”ì²­ ë°ì´í„° íŒŒì‹±
        try:
            request = DeepSeekRAGUploadRequest(**json.loads(request_data))
        except (json.JSONDecodeError, ValueError) as e:
            raise HTTPException(
                status_code=400,
                detail=f"ì˜ëª»ëœ ìš”ì²­ ë°ì´í„°: {str(e)}"
            )
        
        # íŒŒì¼ ê²€ì¦
        if not file.filename.endswith('.pdf'):
            raise HTTPException(
                status_code=400,
                detail="PDF íŒŒì¼ë§Œ ì—…ë¡œë“œ ê°€ëŠ¥í•©ë‹ˆë‹¤."
            )
        
        # ì²˜ë¦¬ ID ìƒì„±
        processing_id = str(uuid.uuid4())
        
        # íŒŒì¼ ì €ì¥
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        safe_filename = f"deepseek_{timestamp}_{current_user.id}_{file.filename}"
        upload_dir = Path("uploads/rag_documents")
        upload_dir.mkdir(parents=True, exist_ok=True)
        file_path = upload_dir / safe_filename
        
        with open(file_path, "wb") as buffer:
            shutil.copyfileobj(file.file, buffer)
        
        # DeepSeek + Gemini í†µí•© ì²˜ë¦¬ ì‹œì‘
        processing_result = await _process_document_with_deepseek_gemini(
            file_path=file_path,
            request=request,
            processing_id=processing_id,
            user_id=current_user.id,
            db=db
        )
        
        return DeepSeekRAGUploadResponse(
            success=processing_result["success"],
            message=processing_result["message"],
            processing_id=processing_id,
            document_info=processing_result["document_info"],
            processing_steps=processing_result["processing_steps"],
            statistics=processing_result["statistics"]
        )
        
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(
            status_code=500,
            detail=f"DeepSeek RAG ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"
        )

@router.get("/deepseek-status/{processing_id}", response_model=RAGProcessingStatus)
async def get_deepseek_processing_status(
    processing_id: str,
    current_user: User = Depends(get_current_user)
):
    """DeepSeek RAG ì²˜ë¦¬ ìƒíƒœ ì¡°íšŒ"""
    try:
        # ì²˜ë¦¬ ìƒíƒœ ì¡°íšŒ (ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” Redisë‚˜ DBì—ì„œ ìƒíƒœ ê´€ë¦¬)
        status_file = Path(f"temp/processing_status_{processing_id}.json")
        
        if not status_file.exists():
            raise HTTPException(
                status_code=404,
                detail="ì²˜ë¦¬ ìƒíƒœë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
            )
        
        with open(status_file, 'r', encoding='utf-8') as f:
            status_data = json.load(f)
        
        return RAGProcessingStatus(**status_data)
        
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(
            status_code=500,
            detail=f"ì²˜ë¦¬ ìƒíƒœ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"
        )

@router.get("/deepseek-knowledge-base-stats", response_model=DeepSeekKnowledgeBaseStats)
async def get_deepseek_knowledge_base_stats(
    current_user: User = Depends(get_current_user),
    db: Session = Depends(get_db)
):
    """DeepSeek ì§€ì‹ë² ì´ìŠ¤ í†µê³„ ì¡°íšŒ"""
    try:
        # Qdrant í†µê³„ ì¡°íšŒ
        qdrant_stats = qdrant_service.get_collection_info()
        
        # ë°ì´í„°ë² ì´ìŠ¤ í†µê³„ ì¡°íšŒ
        from sqlalchemy import text, func
        
        # ë¶€ì„œë³„ í†µê³„
        dept_stats = db.execute(text("""
            SELECT department, COUNT(*) as count
            FROM questions 
            WHERE file_category = 'RAG_DEEPSEEK'
            GROUP BY department
        """)).fetchall()
        
        departments = {row.department or "ë¯¸ë¶„ë¥˜": row.count for row in dept_stats}
        
        # ê³¼ëª©ë³„ í†µê³„  
        subject_stats = db.execute(text("""
            SELECT subject_name, COUNT(*) as count
            FROM questions 
            WHERE file_category = 'RAG_DEEPSEEK'
            GROUP BY subject_name
        """)).fetchall()
        
        subjects = {row.subject_name or "ë¯¸ë¶„ë¥˜": row.count for row in subject_stats}
        
        # ë‚œì´ë„ë³„ í†µê³„
        difficulty_stats = db.execute(text("""
            SELECT difficulty, COUNT(*) as count
            FROM questions 
            WHERE file_category = 'RAG_DEEPSEEK'
            GROUP BY difficulty
        """)).fetchall()
        
        difficulty_distribution = {str(row.difficulty or "ë¯¸ë¶„ë¥˜"): row.count for row in difficulty_stats}
        
        return DeepSeekKnowledgeBaseStats(
            total_documents=len(list(Path("uploads/rag_documents").glob("deepseek_*.pdf"))),
            total_chunks=db.query(func.count()).filter_by(file_category='RAG_DEEPSEEK').scalar() or 0,
            total_vectors=qdrant_stats.get("vectors_count", 0),
            departments=departments,
            subjects=subjects,
            difficulty_distribution=difficulty_distribution,
            last_updated=datetime.now().isoformat(),
            embedding_model="DeepSeek Embedding",
            vector_dimension=768
        )
        
    except Exception as e:
        raise HTTPException(
            status_code=500,
            detail=f"ì§€ì‹ë² ì´ìŠ¤ í†µê³„ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"
        )

@router.post("/deepseek-reindex")
async def reindex_deepseek_knowledge_base(
    current_user: User = Depends(get_current_user),
    db: Session = Depends(get_db)
):
    """DeepSeek ì§€ì‹ë² ì´ìŠ¤ ì „ì²´ ì¬ì¸ë±ì‹±"""
    try:
        import logging
        logger = logging.getLogger(__name__)
        
        logger.info("ğŸ”„ DeepSeek ì§€ì‹ë² ì´ìŠ¤ ì¬ì¸ë±ì‹± ì‹œì‘")
        
        # Qdrant ì»¬ë ‰ì…˜ ì¬ìƒì„±
        reindex_result = await qdrant_service.recreate_collection()
        
        if not reindex_result["success"]:
            raise HTTPException(
                status_code=500,
                detail=f"ë²¡í„° DB ì¬ì¸ë±ì‹± ì‹¤íŒ¨: {reindex_result.get('error')}"
            )
        
        # RAG ë¬¸ì„œë“¤ ì¬ì²˜ë¦¬
        rag_docs_dir = Path("uploads/rag_documents")
        processed_count = 0
        
        if rag_docs_dir.exists():
            for pdf_file in rag_docs_dir.glob("deepseek_*.pdf"):
                # íŒŒì¼ë³„ ì¬ì²˜ë¦¬ ë¡œì§ (ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” ë°±ê·¸ë¼ìš´ë“œ ì‘ì—…ìœ¼ë¡œ)
                processed_count += 1
        
        logger.info(f"âœ… DeepSeek ì§€ì‹ë² ì´ìŠ¤ ì¬ì¸ë±ì‹± ì™„ë£Œ: {processed_count}ê°œ ë¬¸ì„œ")
        
        return {
            "success": True,
            "message": f"DeepSeek ì§€ì‹ë² ì´ìŠ¤ ì¬ì¸ë±ì‹±ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.",
            "processed_documents": processed_count,
            "vector_count": reindex_result.get("vector_count", 0),
            "reindex_time": datetime.now().isoformat()
        }
        
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(
            status_code=500,
            detail=f"ì¬ì¸ë±ì‹± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"
        )

# === DeepSeek + Gemini í†µí•© ì²˜ë¦¬ í•¨ìˆ˜ ===

async def _process_document_with_deepseek_gemini(
    file_path: Path,
    request: DeepSeekRAGUploadRequest,
    processing_id: str,
    user_id: int,
    db: Session
) -> Dict[str, Any]:
    """
    DeepSeek + Gemini í†µí•© ë¬¸ì„œ ì²˜ë¦¬
    """
    import logging
    import json
    
    logger = logging.getLogger(__name__)
    
    # ì²˜ë¦¬ ìƒíƒœ ì´ˆê¸°í™”
    status_data = {
        "processing_id": processing_id,
        "status": "processing",
        "progress_percentage": 0,
        "current_step": "ë¬¸ì„œ íŒŒì‹± ì¤€ë¹„",
        "steps_completed": [],
        "results": None,
        "error_message": None
    }
    
    # ìƒíƒœ íŒŒì¼ ì €ì¥ìš© ë””ë ‰í† ë¦¬
    temp_dir = Path("temp")
    temp_dir.mkdir(exist_ok=True)
    status_file = temp_dir / f"processing_status_{processing_id}.json"
    
    def update_status(step: str, progress: int, completed_step: str = None):
        status_data["current_step"] = step
        status_data["progress_percentage"] = progress
        if completed_step:
            status_data["steps_completed"].append(completed_step)
        
        with open(status_file, 'w', encoding='utf-8') as f:
            json.dump(status_data, f, ensure_ascii=False, indent=2)
    
    try:
        logger.info(f"ğŸš€ DeepSeek + Gemini í†µí•© ì²˜ë¦¬ ì‹œì‘: {file_path.name}")
        
        # 1. Gemini PDF íŒŒì‹±
        update_status("Gemini PDF íŒŒì‹± ì¤‘...", 10)
        
        # ì‹¤ì œ Gemini íŒŒì‹± í˜¸ì¶œ (gemini_service ê°€ ìˆë‹¤ê³  ê°€ì •)
        from ..services.gemini_service import gemini_service
        parsing_result = await gemini_service.parse_pdf_document(
            file_path=str(file_path),
            department=request.department
        )
        
        if not parsing_result["success"]:
            raise Exception(f"Gemini íŒŒì‹± ì‹¤íŒ¨: {parsing_result.get('error')}")
        
        update_status("PDF íŒŒì‹± ì™„ë£Œ", 25, "Gemini PDF íŒŒì‹±")
        
        # 2. í…ìŠ¤íŠ¸ ì²­í‚¹
        update_status("í…ìŠ¤íŠ¸ ì²­í‚¹ ì¤‘...", 35)
        
        content = parsing_result["content"]
        chunks = _create_text_chunks(content, request.chunk_size, request.overlap)
        
        update_status("í…ìŠ¤íŠ¸ ì²­í‚¹ ì™„ë£Œ", 45, "í…ìŠ¤íŠ¸ ì²­í‚¹")
        
        # 3. DeepSeek ë¶„ë¥˜ ë° ë¼ë²¨ë§
        update_status("DeepSeek ë¶„ë¥˜ ë° ë¼ë²¨ë§ ì¤‘...", 55)
        
        classified_chunks = []
        for i, chunk in enumerate(chunks):
            if request.use_deepseek_labeling:
                # DeepSeekìœ¼ë¡œ ë‚œì´ë„ ë° ìœ í˜• ë¶„ë¥˜
                classification_result = await deepseek_service.classify_content(
                    content=chunk,
                    department=request.department,
                    subject=request.subject
                )
                
                chunk_data = {
                    "content": chunk,
                    "difficulty": classification_result.get("difficulty", "ì¤‘"),
                    "content_type": classification_result.get("content_type", "ì´ë¡ "),
                    "keywords": classification_result.get("keywords", []),
                    "chunk_index": i
                }
            else:
                chunk_data = {
                    "content": chunk,
                    "difficulty": "ì¤‘",
                    "content_type": "ì´ë¡ ",
                    "keywords": [],
                    "chunk_index": i
                }
            
            classified_chunks.append(chunk_data)
        
        update_status("DeepSeek ë¶„ë¥˜ ì™„ë£Œ", 70, "DeepSeek ë¶„ë¥˜ ë° ë¼ë²¨ë§")
        
        # 4. Qdrant ë²¡í„° DB ì €ì¥
        update_status("ë²¡í„° DB ì €ì¥ ì¤‘...", 80)
        
        vector_storage_results = []
        for chunk_data in classified_chunks:
            # ë©”íƒ€ë°ì´í„° ì¤€ë¹„
            metadata = {
                "document_title": request.document_title,
                "department": request.department,
                "subject": request.subject or "ì¼ë°˜",
                "difficulty": chunk_data["difficulty"],
                "content_type": chunk_data["content_type"],
                "keywords": chunk_data["keywords"],
                "chunk_index": chunk_data["chunk_index"],
                "file_category": "RAG_DEEPSEEK",
                "user_id": user_id,
                "created_at": datetime.now().isoformat()
            }
            
            # Qdrantì— ì €ì¥
            vector_result = await qdrant_service.add_vectors(
                texts=[chunk_data["content"]],
                metadatas=[metadata],
                ids=[f"deepseek_{processing_id}_{chunk_data['chunk_index']}"]
            )
            
            vector_storage_results.append(vector_result)
        
        update_status("ë²¡í„° DB ì €ì¥ ì™„ë£Œ", 90, "Qdrant ë²¡í„° ì €ì¥")
        
        # 5. í•™ìŠµ ë°ì´í„° ì—…ë°ì´íŠ¸
        update_status("í•™ìŠµ ë°ì´í„° ì—…ë°ì´íŠ¸ ì¤‘...", 95)
        
        # DeepSeek í•™ìŠµ ë°ì´í„°ë¡œ ì €ì¥
        training_data = {
            "document_info": {
                "title": request.document_title,
                "department": request.department,
                "subject": request.subject,
                "file_path": str(file_path),
                "processing_id": processing_id
            },
            "chunks": classified_chunks,
            "statistics": {
                "total_chunks": len(classified_chunks),
                "successful_vectors": sum(1 for r in vector_storage_results if r.get("success")),
                "failed_vectors": sum(1 for r in vector_storage_results if not r.get("success"))
            }
        }
        
        # í•™ìŠµ ë°ì´í„° íŒŒì¼ ì €ì¥
        training_dir = Path("data/deepseek_training")
        training_dir.mkdir(parents=True, exist_ok=True)
        training_file = training_dir / f"training_{processing_id}.json"
        
        with open(training_file, 'w', encoding='utf-8') as f:
            json.dump(training_data, f, ensure_ascii=False, indent=2)
        
        update_status("ì²˜ë¦¬ ì™„ë£Œ", 100, "í•™ìŠµ ë°ì´í„° ì—…ë°ì´íŠ¸")
        
        # ìµœì¢… ê²°ê³¼
        final_result = {
            "success": True,
            "message": f"DeepSeek + Gemini í†µí•© ì²˜ë¦¬ê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.",
            "document_info": training_data["document_info"],
            "processing_steps": {
                "gemini_parsing": {"success": True, "content_length": len(content)},
                "text_chunking": {"success": True, "chunk_count": len(classified_chunks)},
                "deepseek_classification": {"success": True, "classified_count": len(classified_chunks)},
                "vector_storage": {"success": True, "stored_count": training_data["statistics"]["successful_vectors"]},
                "training_update": {"success": True, "training_file": str(training_file)}
            },
            "statistics": training_data["statistics"]
        }
        
        # ìµœì¢… ìƒíƒœ ì—…ë°ì´íŠ¸
        status_data["status"] = "completed"
        status_data["results"] = final_result
        
        with open(status_file, 'w', encoding='utf-8') as f:
            json.dump(status_data, f, ensure_ascii=False, indent=2)
        
        logger.info(f"âœ… DeepSeek + Gemini í†µí•© ì²˜ë¦¬ ì™„ë£Œ: {processing_id}")
        
        return final_result
        
    except Exception as e:
        logger.error(f"âŒ DeepSeek + Gemini í†µí•© ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
        
        # ì˜¤ë¥˜ ìƒíƒœ ì—…ë°ì´íŠ¸
        status_data["status"] = "failed"
        status_data["error_message"] = str(e)
        
        with open(status_file, 'w', encoding='utf-8') as f:
            json.dump(status_data, f, ensure_ascii=False, indent=2)
        
        return {
            "success": False,
            "message": f"ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}",
            "document_info": {},
            "processing_steps": {},
            "statistics": {}
        }

def _create_text_chunks(text: str, chunk_size: int, overlap: int) -> List[str]:
    """í…ìŠ¤íŠ¸ë¥¼ ì²­í¬ë¡œ ë¶„í• """
    chunks = []
    start = 0
    
    while start < len(text):
        end = start + chunk_size
        
        # ì²­í¬ ê²½ê³„ì—ì„œ ë‹¨ì–´ê°€ ì˜ë¦¬ì§€ ì•Šë„ë¡ ì¡°ì •
        if end < len(text):
            # ê³µë°±ì´ë‚˜ ë¬¸ì¥ ëì—ì„œ ìë¥´ê¸°
            while end > start and text[end] not in [' ', '\n', '.', '!', '?']:
                end -= 1
        
        chunk = text[start:end].strip()
        if chunk:
            chunks.append(chunk)
        
        start = end - overlap
        
        if start >= len(text):
            break
    
    return chunks

# === ê¸°ì¡´ ì—”ë“œí¬ì¸íŠ¸ë“¤ ===

@router.post("/upload-document", response_model=DocumentUploadResponse)
async def upload_document(
    file: UploadFile = File(...),
    document_title: str = Form(...),
    chunk_size: int = Form(1000),
    overlap: int = Form(200),
    db: Session = Depends(get_db),
    current_user: User = Depends(get_current_user)
):
    """
    PDF ë¬¸ì„œ ì—…ë¡œë“œ ë° RAG ì²˜ë¦¬
    """
    try:
        # íŒŒì¼ ê²€ì¦
        if not file.filename.endswith('.pdf'):
            raise HTTPException(
                status_code=400,
                detail="PDF íŒŒì¼ë§Œ ì—…ë¡œë“œ ê°€ëŠ¥í•©ë‹ˆë‹¤."
            )
        
        # íŒŒì¼ ì €ì¥
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        safe_filename = f"{timestamp}_{current_user.id}_{file.filename}"
        file_path = rag_service.upload_dir / safe_filename
        
        with open(file_path, "wb") as buffer:
            shutil.copyfileobj(file.file, buffer)
        
        # RAG ì²˜ë¦¬
        result = rag_service.upload_and_process_document(
            db=db,
            file_path=str(file_path),
            document_title=document_title,
            user_id=current_user.id,
            chunk_size=chunk_size,
            overlap=overlap
        )
        
        return DocumentUploadResponse(**result)
        
    except Exception as e:
        raise HTTPException(
            status_code=500,
            detail=f"ë¬¸ì„œ ì—…ë¡œë“œ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"
        )

@router.post("/generate-question", response_model=QuestionGenerationResponse)
async def generate_question_with_rag(
    request: QuestionGenerationRequest,
    db: Session = Depends(get_db),
    current_user: User = Depends(get_current_user)
):
    """
    RAG ê¸°ë°˜ ë¬¸ì œ ìƒì„±
    """
    try:
        result = rag_service.generate_question_with_rag(
            db=db,
            topic=request.topic,
            difficulty=request.difficulty,
            question_type=request.question_type,
            context_limit=request.context_limit
        )
        
        return QuestionGenerationResponse(**result)
        
    except Exception as e:
        raise HTTPException(
            status_code=500,
            detail=f"ë¬¸ì œ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"
        )

@router.post("/similarity-search")
async def similarity_search(
    request: SimilaritySearchRequest,
    db: Session = Depends(get_db),
    current_user: User = Depends(get_current_user)
):
    """
    ë²¡í„° ìœ ì‚¬ë„ ê²€ìƒ‰
    """
    try:
        results = rag_service.similarity_search(
            db=db,
            query_text=request.query_text,
            limit=request.limit,
            similarity_threshold=request.similarity_threshold
        )
        
        return {
            "success": True,
            "results": results,
            "total_count": len(results)
        }
        
    except Exception as e:
        raise HTTPException(
            status_code=500,
            detail=f"ìœ ì‚¬ë„ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"
        )

@router.get("/statistics", response_model=RAGStatistics)
async def get_rag_statistics(
    db: Session = Depends(get_db),
    current_user: User = Depends(get_current_user)
):
    """
    RAG ì‹œìŠ¤í…œ í†µê³„ ì •ë³´ ì¡°íšŒ
    """
    try:
        stats = rag_service.get_rag_statistics(db)
        return RAGStatistics(**stats)
        
    except Exception as e:
        raise HTTPException(
            status_code=500,
            detail=f"í†µê³„ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"
        )

@router.get("/documents")
async def get_rag_documents(
    limit: int = 20,
    offset: int = 0,
    db: Session = Depends(get_db),
    current_user: User = Depends(get_current_user)
):
    """
    RAG ë¬¸ì„œ ëª©ë¡ ì¡°íšŒ
    """
    try:
        from sqlalchemy import text
        
        with db.begin():
            result = db.execute(text("""
                SELECT DISTINCT file_title, created_at, COUNT(*) as chunk_count
                FROM questions 
                WHERE file_category = 'RAG_DOCUMENT'
                GROUP BY file_title, created_at
                ORDER BY created_at DESC
                LIMIT :limit OFFSET :offset
            """), {"limit": limit, "offset": offset})
            
            documents = []
            for row in result:
                documents.append({
                    "title": row[0],
                    "uploaded_at": row[1],
                    "chunk_count": row[2]
                })
            
            return {
                "success": True,
                "documents": documents,
                "total_count": len(documents)
            }
            
    except Exception as e:
        raise HTTPException(
            status_code=500,
            detail=f"ë¬¸ì„œ ëª©ë¡ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"
        )

@router.delete("/document/{document_title}")
async def delete_rag_document(
    document_title: str,
    db: Session = Depends(get_db),
    current_user: User = Depends(get_current_user)
):
    """
    RAG ë¬¸ì„œ ì‚­ì œ
    """
    try:
        from sqlalchemy import text
        
        # í•´ë‹¹ ë¬¸ì„œì˜ ëª¨ë“  ì²­í¬ ì‚­ì œ
        result = db.execute(text("""
            DELETE FROM questions 
            WHERE file_category = 'RAG_DOCUMENT' AND file_title = :title
        """), {"title": document_title})
        
        deleted_count = result.rowcount
        db.commit()
        
        return {
            "success": True,
            "message": f"ë¬¸ì„œ '{document_title}' ì‚­ì œ ì™„ë£Œ",
            "deleted_chunks": deleted_count
        }
        
    except Exception as e:
        db.rollback()
        raise HTTPException(
            status_code=500,
            detail=f"ë¬¸ì„œ ì‚­ì œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"
        )

@router.post("/reindex")
async def reindex_vectors(
    db: Session = Depends(get_db),
    current_user: User = Depends(get_current_user)
):
    """
    ë²¡í„° ì¸ë±ìŠ¤ ì¬êµ¬ì„±
    """
    try:
        from sqlalchemy import text
        
        # ë²¡í„° ì¸ë±ìŠ¤ ì¬êµ¬ì„± (PostgreSQL)
        with db.begin():
            db.execute(text("REINDEX INDEX CONCURRENTLY IF EXISTS questions_embedding_idx"))
        
        return {
            "success": True,
            "message": "ë²¡í„° ì¸ë±ìŠ¤ ì¬êµ¬ì„± ì™„ë£Œ"
        }
        
    except Exception as e:
        raise HTTPException(
            status_code=500,
            detail=f"ë²¡í„° ì¸ë±ìŠ¤ ì¬êµ¬ì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"
        ) 