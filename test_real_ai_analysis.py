#!/usr/bin/env python3
"""
ì‹¤ì œ ë°ì´í„° ê¸°ë°˜ AI ë¶„ì„ í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸
ìƒˆë¡œìš´ ì§„ë‹¨í…ŒìŠ¤íŠ¸ ì„¸ì…˜ì„ ìƒì„±í•˜ê³  ì™„ë£Œí•˜ì—¬ ì‹¤ì œ ë°ì´í„° AI ë¶„ì„ì´ ì œëŒ€ë¡œ ì‘ë™í•˜ëŠ”ì§€ í™•ì¸
"""

import os
import sys
import asyncio
from datetime import datetime
import json

# í˜„ì¬ ë””ë ‰í† ë¦¬ë¥¼ Python pathì— ì¶”ê°€
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from app.db.database import get_db
from app.models.diagnosis import DiagnosticSession, SessionStatus
from app.api.diagnosis import (
    real_data_ai_analysis, DetailedResult, 
    SessionStartRequest, SessionCompleteRequest
)

async def test_real_ai_analysis():
    """ì‹¤ì œ ë°ì´í„° ê¸°ë°˜ AI ë¶„ì„ í…ŒìŠ¤íŠ¸"""
    print("ğŸ§ª ì‹¤ì œ ë°ì´í„° ê¸°ë°˜ AI ë¶„ì„ í…ŒìŠ¤íŠ¸ ì‹œì‘")
    print("=" * 60)
    
    db = next(get_db())
    
    # í…ŒìŠ¤íŠ¸ìš© ìƒì„¸ ê²°ê³¼ ë°ì´í„° (ì‹¤ì œ ë¬¸ì œ 30ê°œ ì‹œë®¬ë ˆì´ì…˜)
    test_detailed_results = []
    
    # ë‹¤ì–‘í•œ ì„±ê³¼ íŒ¨í„´ìœ¼ë¡œ 30ê°œ ë¬¸ì œ ì‹œë®¬ë ˆì´ì…˜
    question_types = ["ê¸°ë³¸ê°œë…", "ì¢…í•©íŒë‹¨", "ì‘ìš©ë¬¸ì œ"]
    domains = ["ì‹ ê²½ê³„", "ê·¼ê³¨ê²©ê³„", "ì‹¬íìˆœí™˜ê³„", "ê¸°íƒ€"]
    difficulties = ["ì‰¬ì›€", "ë³´í†µ", "ì–´ë ¤ì›€"]
    
    correct_answers = 0
    total_time = 0
    
    for i in range(1, 31):
        # ëœë¤í•˜ê²Œ ì •ë‹µ/ì˜¤ë‹µ ê²°ì • (ì•½ 70% ì •ë‹µë¥ )
        is_correct = i % 10 != 3 and i % 10 != 7 and i % 10 != 9  # 70% ì •ë‹µë¥ 
        if is_correct:
            correct_answers += 1
        
        # ë¬¸ì œë³„ ì‹œê°„ (1-5ì´ˆ ë²”ìœ„)
        time_spent = (i % 5 + 1) * 1000  # ë°€ë¦¬ì´ˆ
        total_time += time_spent
        
        result = DetailedResult(
            question_id=f"DIAG_{i:03d}",
            question_number=i,
            selected_answer=str((i % 5) + 1),
            correct_answer=str((i % 5) + 1) if is_correct else str(((i + 1) % 5) + 1),
            is_correct=is_correct,
            time_spent_ms=time_spent,
            difficulty_level=difficulties[i % 3],
            domain=domains[i % 4],
            question_type=question_types[i % 3]
        )
        test_detailed_results.append(result)
    
    total_score = round((correct_answers / 30) * 100, 1)
    
    print(f"ğŸ“Š í…ŒìŠ¤íŠ¸ ì‹œë‚˜ë¦¬ì˜¤:")
    print(f"  ì •ë‹µ/ì˜¤ë‹µ: {correct_answers}/30")
    print(f"  ì ìˆ˜: {total_score}ì ")
    print(f"  ì´ ì‹œê°„: {total_time/1000:.1f}ì´ˆ")
    
    # ì‹¤ì œ ë°ì´í„° ê¸°ë°˜ AI ë¶„ì„ ì‹¤í–‰
    print(f"\nğŸ¤– ì‹¤ì œ ë°ì´í„° ê¸°ë°˜ AI ë¶„ì„ ì‹¤í–‰...")
    
    try:
        ai_result = await real_data_ai_analysis(
            session_id="test_session_123",
            user_id=32,  # ê¸°ì¡´ ì‚¬ìš©ì ID
            detailed_results=test_detailed_results,
            total_score=total_score,
            total_time_ms=total_time,
            test_type="physical_therapy_1st",
            department="ë¬¼ë¦¬ì¹˜ë£Œí•™ê³¼",
            db=db
        )
        
        print(f"âœ… AI ë¶„ì„ ì„±ê³µ!")
        print(f"\nğŸ“ˆ ë¶„ì„ ê²°ê³¼:")
        print(f"  ì‹ ë¢°ë„: {ai_result.confidence_score}")
        print(f"  ìœ í˜•ë³„ ì •ë‹µë¥ : {ai_result.type_analysis}")
        print(f"  ë‚œì´ë„ë³„ ì •ë‹µë¥ : {ai_result.difficulty_analysis}")
        print(f"  ì•½í•œ ì˜ì—­: {ai_result.weak_areas}")
        
        print(f"\nâ±ï¸ ì‹œê°„ ë¶„ì„:")
        time_analysis = ai_result.time_analysis
        print(f"  ì´ ì‹œê°„: {time_analysis['total_time_seconds']}ì´ˆ")
        print(f"  ë¬¸ì œë‹¹ í‰ê· : {time_analysis['avg_time_per_question']}ì´ˆ")
        print(f"  ì‹œê°„ íš¨ìœ¨ì„±: {time_analysis['time_efficiency']}")
        if 'time_percentile' in time_analysis:
            print(f"  ì‹œê°„ ë°±ë¶„ìœ„: {time_analysis['time_percentile']}%")
        
        print(f"\nğŸ‘¥ ë™ë£Œ ë¹„êµ ë¶„ì„:")
        peer_comparison = ai_result.peer_comparison
        print(f"  í•™ê³¼ í‰ê· : {peer_comparison['department_average']}ì ")
        print(f"  ë°±ë¶„ìœ„: {peer_comparison['percentile']}%")
        print(f"  ìˆœìœ„: {peer_comparison['ranking']}")
        print(f"  ë¹„êµ ëŒ€ìƒ: {peer_comparison['total_peers']}ëª…")
        if 'score_vs_avg' in peer_comparison:
            print(f"  í‰ê·  ëŒ€ë¹„: {peer_comparison['score_vs_avg']:+.1f}ì ")
        
        print(f"\nğŸ’¡ AI ê¶Œì¥ì‚¬í•­:")
        for i, recommendation in enumerate(ai_result.recommendations, 1):
            print(f"  {i}. {recommendation}")
        
        print(f"\nğŸ” ì‹¤ì œ ë°ì´í„° í™œìš© ì¦ëª…:")
        print(f"  âœ… ë™ë£Œ {peer_comparison['total_peers']}ëª… ë°ì´í„° ë¶„ì„")
        print(f"  âœ… ì‹¤ì œ ë¬¸ì œë³„ í†µê³„ í™œìš©")
        print(f"  âœ… ì‹œê°„ íš¨ìœ¨ì„± ë¹„êµ ë¶„ì„")
        print(f"  âœ… ê°œì¸í™”ëœ ê¶Œì¥ì‚¬í•­ ìƒì„±")
        
    except Exception as e:
        print(f"âŒ AI ë¶„ì„ ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()
    
    finally:
        db.close()
    
    print(f"\n" + "=" * 60)
    print(f"ğŸ§ª ì‹¤ì œ ë°ì´í„° ê¸°ë°˜ AI ë¶„ì„ í…ŒìŠ¤íŠ¸ ì™„ë£Œ")

if __name__ == "__main__":
    asyncio.run(test_real_ai_analysis()) 