"""
ë¬¼ë¦¬ì¹˜ë£Œí•™ê³¼ ì§„ë‹¨í…ŒìŠ¤íŠ¸ ë¬¸ì œ ìƒì„±ê¸°
88ê°œ í•™ìŠµëœ ë¬¸ì œì—ì„œ AIê°€ ë¶„ì„í•˜ì—¬ í•™ìƒ ìˆ˜ì¤€ ì§„ë‹¨ì— ìµœì í™”ëœ 30ë¬¸ì œ ì„ ë³„
"""
import json
import asyncio
from pathlib import Path
from typing import List, Dict, Any
from datetime import datetime
import random

from app.services.deepseek_service import LocalDeepSeekService

class DiagnosticTestCreator:
    """ì§„ë‹¨í…ŒìŠ¤íŠ¸ ìƒì„±ê¸°"""
    
    def __init__(self):
        self.json_dir = Path("data/save_parser")
        self.deepseek = LocalDeepSeekService()
        self.target_count = 30
        
    async def load_all_questions(self) -> List[Dict[str, Any]]:
        """88ê°œ í•™ìŠµëœ ë¬¸ì œ ëª¨ë‘ ë¡œë“œ"""
        json_files = list(self.json_dir.glob("*.json"))
        all_questions = []
        
        print(f"ğŸ“‚ JSON íŒŒì¼ ë¡œë”©: {len(json_files)}ê°œ")
        
        for json_file in json_files:
            print(f"ğŸ“„ ë¡œë”© ì¤‘: {json_file.name}")
            
            with open(json_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            questions = data.get("questions", [])
            
            # ë¬¸ì œì— ì¶”ê°€ ì •ë³´ ì‚½ì…
            for i, q in enumerate(questions):
                q["source_file"] = json_file.name
                q["original_index"] = i
                q["unique_id"] = f"{q.get('year', 2024)}_{q.get('question_number', i+1)}"
                
            all_questions.extend(questions)
        
        print(f"âœ… ì´ ë¡œë“œëœ ë¬¸ì œ: {len(all_questions)}ê°œ")
        return all_questions
    
    async def analyze_question_difficulty(self, question: Dict[str, Any]) -> Dict[str, Any]:
        """AIë¥¼ í†µí•œ ë¬¸ì œ ë‚œì´ë„ ë° íŠ¹ì„± ë¶„ì„"""
        
        question_text = question.get("content", "")
        options = question.get("options", {})
        subject = question.get("subject", "ë¬¼ë¦¬ì¹˜ë£Œí•™")
        
        if options:
            options_text = "\n".join([f"{k}. {v}" for k, v in options.items()])
            full_question = f"{question_text}\n\nì„ íƒì§€:\n{options_text}"
        else:
            full_question = question_text
        
        # AIì—ê²Œ ë¬¸ì œ ë¶„ì„ ìš”ì²­
        analysis_prompt = f"""
ë‹¹ì‹ ì€ ë¬¼ë¦¬ì¹˜ë£Œí•™ê³¼ êµìœ¡ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ë‹¤ìŒ êµ­ê°€ê³ ì‹œ ë¬¸ì œë¥¼ ë¶„ì„í•´ì£¼ì„¸ìš”.

ë¬¸ì œ:
{full_question}

ê³¼ëª©: {subject}
ì—°ë„: {question.get('year', 'ë¯¸ìƒ')}

ë‹¤ìŒ ê¸°ì¤€ìœ¼ë¡œ ë¶„ì„í•´ì£¼ì„¸ìš”:

1. ë‚œì´ë„ (1-10ì , 5ì ì´ ë³´í†µ)
2. ë¬¸ì œ ìœ í˜• (ê¸°ë³¸ê°œë…/ì‘ìš©/ì‹¤ë¬´/ì¢…í•©íŒë‹¨)
3. ì£¼ìš” ë¶„ì•¼ (ì‹ ê²½ê³„/ê·¼ê³¨ê²©ê³„/ì‹¬í/ì†Œì•„/ë…¸ì¸/ìŠ¤í¬ì¸ /ê¸°íƒ€)
4. ì§„ë‹¨í…ŒìŠ¤íŠ¸ ì í•©ì„± (1-10ì )
5. í•™ìƒ ìˆ˜ì¤€ ë³€ë³„ë ¥ (1-10ì )

ì‘ë‹µ í˜•ì‹ (JSON):
{{
  "difficulty": ìˆ«ì,
  "question_type": "ë¬¸ì œìœ í˜•",
  "domain": "ì£¼ìš”ë¶„ì•¼", 
  "diagnostic_suitability": ìˆ«ì,
  "discrimination_power": ìˆ«ì,
  "reasoning": "ì„ íƒ ì´ìœ  ê°„ë‹¨ ì„¤ëª…"
}}
"""
        
        try:
            messages = [
                {"role": "system", "content": "ë‹¹ì‹ ì€ ë¬¼ë¦¬ì¹˜ë£Œí•™ê³¼ êµìœ¡ í‰ê°€ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì •í™•í•˜ê³  ê°ê´€ì ì¸ ë¶„ì„ì„ ì œê³µí•´ì£¼ì„¸ìš”."},
                {"role": "user", "content": analysis_prompt}
            ]
            
            result = await self.deepseek.chat_completion(messages, temperature=0.3)
            
            if result.get('success'):
                # JSON ì‘ë‹µ íŒŒì‹± ì‹œë„
                response_text = result.get('content', '')
                
                # JSON ë¶€ë¶„ë§Œ ì¶”ì¶œ
                try:
                    json_start = response_text.find('{')
                    json_end = response_text.rfind('}') + 1
                    json_text = response_text[json_start:json_end]
                    
                    analysis = json.loads(json_text)
                    
                    # ê¸°ë³¸ê°’ ë³´ì •
                    analysis['difficulty'] = max(1, min(10, analysis.get('difficulty', 5)))
                    analysis['diagnostic_suitability'] = max(1, min(10, analysis.get('diagnostic_suitability', 5)))
                    analysis['discrimination_power'] = max(1, min(10, analysis.get('discrimination_power', 5)))
                    
                    return analysis
                    
                except json.JSONDecodeError:
                    print(f"âš ï¸ JSON íŒŒì‹± ì‹¤íŒ¨, ê¸°ë³¸ê°’ ì‚¬ìš©: {question.get('unique_id')}")
                    
            # ì‹¤íŒ¨ì‹œ ê¸°ë³¸ ë¶„ì„ê°’
            return self._get_default_analysis(question)
            
        except Exception as e:
            print(f"âŒ ë¶„ì„ ì˜¤ë¥˜: {e}")
            return self._get_default_analysis(question)
    
    def _get_default_analysis(self, question: Dict[str, Any]) -> Dict[str, Any]:
        """ê¸°ë³¸ ë¶„ì„ê°’ (AI ë¶„ì„ ì‹¤íŒ¨ì‹œ)"""
        subject = question.get("subject", "").lower()
        
        # ê³¼ëª©ë³„ ê¸°ë³¸ ë¶„ì•¼ ë§¤í•‘
        domain_mapping = {
            "ì‹ ê²½": "ì‹ ê²½ê³„",
            "ê·¼ê³¨ê²©": "ê·¼ê³¨ê²©ê³„", 
            "ì •í˜•": "ê·¼ê³¨ê²©ê³„",
            "ì‹¬í": "ì‹¬í",
            "ì†Œì•„": "ì†Œì•„",
            "ë…¸ì¸": "ë…¸ì¸",
            "ìŠ¤í¬ì¸ ": "ìŠ¤í¬ì¸ "
        }
        
        domain = "ê¸°íƒ€"
        for key, value in domain_mapping.items():
            if key in subject:
                domain = value
                break
        
        return {
            "difficulty": random.randint(4, 7),  # ì¤‘ê°„ ë‚œì´ë„
            "question_type": "ì‘ìš©",
            "domain": domain,
            "diagnostic_suitability": random.randint(6, 8),
            "discrimination_power": random.randint(5, 7),
            "reasoning": "ê¸°ë³¸ ë¶„ì„ê°’ ì ìš©"
        }
    
    async def select_diagnostic_questions(self, questions: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """ì§„ë‹¨í…ŒìŠ¤íŠ¸ìš© 30ë¬¸ì œ ì„ ë³„"""
        print(f"ğŸ¤– AI ë¶„ì„ ì‹œì‘: {len(questions)}ê°œ ë¬¸ì œ")
        
        # 1ë‹¨ê³„: ëª¨ë“  ë¬¸ì œ AI ë¶„ì„
        analyzed_questions = []
        
        for i, question in enumerate(questions, 1):
            print(f"ğŸ“Š ë¶„ì„ ì¤‘ ({i}/{len(questions)}): {question.get('unique_id')}")
            
            analysis = await self.analyze_question_difficulty(question)
            
            # ë¶„ì„ ê²°ê³¼ë¥¼ ë¬¸ì œì— ì¶”ê°€
            question['ai_analysis'] = analysis
            analyzed_questions.append(question)
            
            # ê³¼ë¶€í•˜ ë°©ì§€
            await asyncio.sleep(0.1)
        
        # 2ë‹¨ê³„: ì§„ë‹¨í…ŒìŠ¤íŠ¸ ì í•©ì„± ê¸°ì¤€ìœ¼ë¡œ í•„í„°ë§
        suitable_questions = [
            q for q in analyzed_questions 
            if q['ai_analysis'].get('diagnostic_suitability', 0) >= 6
        ]
        
        print(f"ğŸ¯ ì§„ë‹¨í…ŒìŠ¤íŠ¸ ì í•© ë¬¸ì œ: {len(suitable_questions)}ê°œ")
        
        # 3ë‹¨ê³„: ê· í˜• ì¡íŒ 30ë¬¸ì œ ì„ ë³„
        selected_questions = self._balance_selection(suitable_questions)
        
        print(f"âœ… ìµœì¢… ì„ ë³„ëœ ë¬¸ì œ: {len(selected_questions)}ê°œ")
        
        return selected_questions
    
    def _balance_selection(self, questions: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """ê· í˜• ì¡íŒ ë¬¸ì œ ì„ ë³„"""
        
        # ë‚œì´ë„ë³„ ë¶„í¬ ëª©í‘œ
        difficulty_targets = {
            "ì‰¬ì›€": 8,    # 1-4ì : ê¸°ë³¸ ê°œë… í™•ì¸
            "ë³´í†µ": 14,   # 5-7ì : í•µì‹¬ ì—­ëŸ‰ í‰ê°€  
            "ì–´ë ¤ì›€": 8   # 8-10ì : ê³ ê¸‰ ì‚¬ê³ ë ¥
        }
        
        # ë¶„ì•¼ë³„ ë¶„í¬ ëª©í‘œ
        domain_targets = {
            "ì‹ ê²½ê³„": 6,
            "ê·¼ê³¨ê²©ê³„": 8, 
            "ì‹¬í": 4,
            "ì†Œì•„": 2,
            "ë…¸ì¸": 2,
            "ìŠ¤í¬ì¸ ": 3,
            "ê¸°íƒ€": 5
        }
        
        # ë¬¸ì œ ìœ í˜•ë³„ ë¶„í¬ ëª©í‘œ
        type_targets = {
            "ê¸°ë³¸ê°œë…": 8,
            "ì‘ìš©": 12,
            "ì‹¤ë¬´": 6,
            "ì¢…í•©íŒë‹¨": 4
        }
        
        # ì„ ë³„ ì•Œê³ ë¦¬ì¦˜
        selected = []
        remaining = questions.copy()
        
        # 1ë‹¨ê³„: ë‚œì´ë„ë³„ ì„ ë³„
        for difficulty_range, target_count in [
            ((1, 4), difficulty_targets["ì‰¬ì›€"]),
            ((5, 7), difficulty_targets["ë³´í†µ"]), 
            ((8, 10), difficulty_targets["ì–´ë ¤ì›€"])
        ]:
            candidates = [
                q for q in remaining 
                if difficulty_range[0] <= q['ai_analysis'].get('difficulty', 5) <= difficulty_range[1]
            ]
            
            # ë³€ë³„ë ¥ ìˆœìœ¼ë¡œ ì •ë ¬í•´ì„œ ìƒìœ„ ì„ íƒ
            candidates.sort(key=lambda x: x['ai_analysis'].get('discrimination_power', 0), reverse=True)
            
            selected_count = min(target_count, len(candidates))
            selected.extend(candidates[:selected_count])
            
            # ì„ íƒëœ ë¬¸ì œë“¤ ì œê±°
            for q in candidates[:selected_count]:
                if q in remaining:
                    remaining.remove(q)
        
        # 2ë‹¨ê³„: ë¶€ì¡±í•œ ë¶„ì•¼ ë³´ì™„
        while len(selected) < self.target_count and remaining:
            # ê°€ì¥ ë¶€ì¡±í•œ ë¶„ì•¼ ì°¾ê¸°
            current_domains = {}
            for q in selected:
                domain = q['ai_analysis'].get('domain', 'ê¸°íƒ€')
                current_domains[domain] = current_domains.get(domain, 0) + 1
            
            most_needed_domain = None
            max_deficit = 0
            
            for domain, target in domain_targets.items():
                current = current_domains.get(domain, 0)
                deficit = target - current
                if deficit > max_deficit:
                    max_deficit = deficit
                    most_needed_domain = domain
            
            if most_needed_domain:
                # í•´ë‹¹ ë¶„ì•¼ì—ì„œ ë³€ë³„ë ¥ì´ ë†’ì€ ë¬¸ì œ ì„ íƒ
                candidates = [
                    q for q in remaining 
                    if q['ai_analysis'].get('domain') == most_needed_domain
                ]
                
                if candidates:
                    best_candidate = max(candidates, key=lambda x: x['ai_analysis'].get('discrimination_power', 0))
                    selected.append(best_candidate)
                    remaining.remove(best_candidate)
                else:
                    # í•´ë‹¹ ë¶„ì•¼ê°€ ì—†ìœ¼ë©´ ë³€ë³„ë ¥ ë†’ì€ ë¬¸ì œ ì„ íƒ
                    if remaining:
                        best_candidate = max(remaining, key=lambda x: x['ai_analysis'].get('discrimination_power', 0))
                        selected.append(best_candidate)
                        remaining.remove(best_candidate)
            else:
                break
        
        # ì •í™•íˆ 30ê°œê°€ ë˜ë„ë¡ ì¡°ì •
        if len(selected) > self.target_count:
            # ë³€ë³„ë ¥ ë‚®ì€ ìˆœìœ¼ë¡œ ì œê±°
            selected.sort(key=lambda x: x['ai_analysis'].get('discrimination_power', 0), reverse=True)
            selected = selected[:self.target_count]
        
        return selected
    
    async def create_diagnostic_test_json(self, selected_questions: List[Dict[str, Any]]) -> Dict[str, Any]:
        """ì§„ë‹¨í…ŒìŠ¤íŠ¸ JSON ìƒì„±"""
        
        # ì§„ë‹¨í…ŒìŠ¤íŠ¸ ë©”íƒ€ë°ì´í„°
        diagnostic_test = {
            "test_info": {
                "title": "ë¬¼ë¦¬ì¹˜ë£Œí•™ê³¼ ìˆ˜ì¤€ ì§„ë‹¨í…ŒìŠ¤íŠ¸",
                "description": "ë¬¼ë¦¬ì¹˜ë£Œì‚¬ êµ­ê°€ê³ ì‹œ ê¸°ì¶œë¬¸ì œ ê¸°ë°˜ í•™ìƒ ìˆ˜ì¤€ ì§„ë‹¨",
                "total_questions": len(selected_questions),
                "time_limit": 60,  # 60ë¶„
                "created_at": datetime.now().isoformat(),
                "version": "1.0",
                "source": "2021-2024ë…„ ë¬¼ë¦¬ì¹˜ë£Œì‚¬ êµ­ê°€ê³ ì‹œ ê¸°ì¶œ"
            },
            
            "scoring_criteria": {
                "total_score": 100,
                "score_per_question": round(100 / len(selected_questions), 1),
                "difficulty_weights": {
                    "ì‰¬ì›€": 1.0,    # ê¸°ë³¸ ì ìˆ˜
                    "ë³´í†µ": 1.2,    # 20% ê°€ì‚°
                    "ì–´ë ¤ì›€": 1.5   # 50% ê°€ì‚°
                },
                "level_classification": {
                    "ìƒê¸‰": {"min_score": 80, "description": "êµ­ê°€ê³ ì‹œ í•©ê²© ìˆ˜ì¤€"},
                    "ì¤‘ê¸‰": {"min_score": 65, "description": "ì¶”ê°€ í•™ìŠµ í•„ìš”"},
                    "í•˜ê¸‰": {"min_score": 50, "description": "ê¸°ì´ˆë¶€í„° ì²´ê³„ì  í•™ìŠµ í•„ìš”"},
                    "ë¯¸í¡": {"min_score": 0, "description": "ì „ë©´ì  ì¬í•™ìŠµ ê¶Œì¥"}
                }
            },
            
            "questions": []
        }
        
        # ë¬¸ì œ ë²ˆí˜¸ ì¬ì •ë ¬
        for i, question in enumerate(selected_questions, 1):
            diagnostic_question = {
                "question_id": f"DIAG_{i:03d}",
                "question_number": i,
                "content": question.get("content", ""),
                "options": question.get("options", {}),
                "correct_answer": question.get("correct_answer", ""),
                "subject": question.get("subject", "ë¬¼ë¦¬ì¹˜ë£Œí•™"),
                "area_name": question.get("area_name", ""),
                "year": question.get("year"),
                "original_question_number": question.get("question_number"),
                
                # AI ë¶„ì„ ê²°ê³¼
                "difficulty": question['ai_analysis'].get('difficulty'),
                "difficulty_level": self._categorize_difficulty(question['ai_analysis'].get('difficulty', 5)),
                "question_type": question['ai_analysis'].get('question_type'),
                "domain": question['ai_analysis'].get('domain'),
                "diagnostic_suitability": question['ai_analysis'].get('diagnostic_suitability'),
                "discrimination_power": question['ai_analysis'].get('discrimination_power'),
                
                # ì§„ë‹¨í…ŒìŠ¤íŠ¸ìš© ë©”íƒ€ë°ì´í„°
                "points": self._calculate_points(question['ai_analysis']),
                "source_info": {
                    "file": question.get("source_file"),
                    "unique_id": question.get("unique_id")
                }
            }
            
            diagnostic_test["questions"].append(diagnostic_question)
        
        # í†µê³„ ì •ë³´ ì¶”ê°€
        diagnostic_test["statistics"] = self._calculate_test_statistics(selected_questions)
        
        return diagnostic_test
    
    def _categorize_difficulty(self, difficulty_score: int) -> str:
        """ë‚œì´ë„ ì ìˆ˜ë¥¼ ë“±ê¸‰ìœ¼ë¡œ ë³€í™˜"""
        if difficulty_score <= 4:
            return "ì‰¬ì›€"
        elif difficulty_score <= 7:
            return "ë³´í†µ"
        else:
            return "ì–´ë ¤ì›€"
    
    def _calculate_points(self, analysis: Dict[str, Any]) -> float:
        """ë¬¸ì œë³„ ì ìˆ˜ ê³„ì‚° (ë‚œì´ë„ì™€ ë³€ë³„ë ¥ ê³ ë ¤)"""
        base_points = 100 / self.target_count  # ê¸°ë³¸ ì ìˆ˜
        difficulty = analysis.get('difficulty', 5)
        discrimination = analysis.get('discrimination_power', 5)
        
        # ë‚œì´ë„ ê°€ì¤‘ì¹˜
        if difficulty <= 4:
            weight = 1.0
        elif difficulty <= 7:
            weight = 1.2
        else:
            weight = 1.5
        
        # ë³€ë³„ë ¥ ë³´ì • (Â±10%)
        discrimination_factor = 0.9 + (discrimination / 50)
        
        return round(base_points * weight * discrimination_factor, 1)
    
    def _calculate_test_statistics(self, questions: List[Dict[str, Any]]) -> Dict[str, Any]:
        """í…ŒìŠ¤íŠ¸ í†µê³„ ê³„ì‚°"""
        
        # ë‚œì´ë„ ë¶„í¬
        difficulty_dist = {"ì‰¬ì›€": 0, "ë³´í†µ": 0, "ì–´ë ¤ì›€": 0}
        domain_dist = {}
        type_dist = {}
        
        avg_difficulty = 0
        avg_discrimination = 0
        
        for q in questions:
            analysis = q['ai_analysis']
            
            difficulty = analysis.get('difficulty', 5)
            avg_difficulty += difficulty
            avg_discrimination += analysis.get('discrimination_power', 5)
            
            # ë¶„í¬ ê³„ì‚°
            difficulty_level = self._categorize_difficulty(difficulty)
            difficulty_dist[difficulty_level] += 1
            
            domain = analysis.get('domain', 'ê¸°íƒ€')
            domain_dist[domain] = domain_dist.get(domain, 0) + 1
            
            question_type = analysis.get('question_type', 'ì‘ìš©')
            type_dist[question_type] = type_dist.get(question_type, 0) + 1
        
        return {
            "difficulty_distribution": difficulty_dist,
            "domain_distribution": domain_dist,
            "type_distribution": type_dist,
            "average_difficulty": round(avg_difficulty / len(questions), 1),
            "average_discrimination": round(avg_discrimination / len(questions), 1),
            "total_questions": len(questions)
        }
    
    async def run_creation_process(self) -> str:
        """ì „ì²´ ì§„ë‹¨í…ŒìŠ¤íŠ¸ ìƒì„± í”„ë¡œì„¸ìŠ¤"""
        print("ğŸš€ ë¬¼ë¦¬ì¹˜ë£Œí•™ê³¼ ì§„ë‹¨í…ŒìŠ¤íŠ¸ ìƒì„± ì‹œì‘!")
        
        # 1. 88ê°œ ë¬¸ì œ ë¡œë“œ
        all_questions = await self.load_all_questions()
        
        # 2. AI ë¶„ì„ì„ í†µí•œ 30ë¬¸ì œ ì„ ë³„
        selected_questions = await self.select_diagnostic_questions(all_questions)
        
        # 3. ì§„ë‹¨í…ŒìŠ¤íŠ¸ JSON ìƒì„±
        diagnostic_test = await self.create_diagnostic_test_json(selected_questions)
        
        # 4. íŒŒì¼ ì €ì¥
        output_file = Path("data/diagnostic_test_physics_therapy.json")
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(diagnostic_test, f, ensure_ascii=False, indent=2)
        
        print(f"ğŸ’¾ ì§„ë‹¨í…ŒìŠ¤íŠ¸ ì €ì¥ ì™„ë£Œ: {output_file}")
        
        # 5. ìš”ì•½ ì •ë³´ ì¶œë ¥
        self._print_summary(diagnostic_test)
        
        return str(output_file)
    
    def _print_summary(self, diagnostic_test: Dict[str, Any]):
        """ì§„ë‹¨í…ŒìŠ¤íŠ¸ ìš”ì•½ ì •ë³´ ì¶œë ¥"""
        print("\nğŸ“Š ì§„ë‹¨í…ŒìŠ¤íŠ¸ ìš”ì•½:")
        print(f"  ğŸ“ ì´ ë¬¸ì œ ìˆ˜: {diagnostic_test['test_info']['total_questions']}ë¬¸ì œ")
        print(f"  â±ï¸ ì œí•œ ì‹œê°„: {diagnostic_test['test_info']['time_limit']}ë¶„")
        
        stats = diagnostic_test['statistics']
        print(f"  ğŸ“ˆ í‰ê·  ë‚œì´ë„: {stats['average_difficulty']}/10")
        print(f"  ğŸ¯ í‰ê·  ë³€ë³„ë ¥: {stats['average_discrimination']}/10")
        
        print("\nğŸšï¸ ë‚œì´ë„ ë¶„í¬:")
        for level, count in stats['difficulty_distribution'].items():
            print(f"    {level}: {count}ë¬¸ì œ")
        
        print("\nğŸ¥ ë¶„ì•¼ë³„ ë¶„í¬:")
        for domain, count in stats['domain_distribution'].items():
            print(f"    {domain}: {count}ë¬¸ì œ")
        
        print("\nğŸ” ë¬¸ì œ ìœ í˜• ë¶„í¬:")
        for qtype, count in stats['type_distribution'].items():
            print(f"    {qtype}: {count}ë¬¸ì œ")

async def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    creator = DiagnosticTestCreator()
    output_file = await creator.run_creation_process()
    
    print(f"\nğŸ‰ ì§„ë‹¨í…ŒìŠ¤íŠ¸ ìƒì„± ì™„ë£Œ!")
    print(f"ğŸ“ íŒŒì¼ ìœ„ì¹˜: {output_file}")
    print("âœ… í•™ìƒ ìˆ˜ì¤€ ì§„ë‹¨ì„ ìœ„í•œ 30ë¬¸ì œê°€ ì¤€ë¹„ë˜ì—ˆìŠµë‹ˆë‹¤!")

if __name__ == "__main__":
    asyncio.run(main()) 